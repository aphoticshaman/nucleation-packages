#!/bin/bash
# =============================================================================
# ReLoRA Multi-Iteration Training Script
# =============================================================================
# Trains multiple LoRA iterations, merging after each
# Result: Pseudo full-rank training via accumulated low-rank updates
#
# Usage:
#   ./relora_train.sh --iterations 3 --rank 64 --base Qwen/Qwen2.5-72B-Instruct
#
# Requirements:
#   - axolotl installed
#   - peft installed (for merging)
#   - HF_TOKEN environment variable set
# =============================================================================

set -e

# Defaults
ITERATIONS=3
RANK=64
BASE_MODEL="Qwen/Qwen2.5-72B-Instruct"
OUTPUT_BASE="/workspace/output/latticeforge-relora"
HUB_USER="aphoticshaman"
CONFIG_TEMPLATE="config_relora.yaml"

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --iterations)
            ITERATIONS="$2"
            shift 2
            ;;
        --rank)
            RANK="$2"
            shift 2
            ;;
        --base)
            BASE_MODEL="$2"
            shift 2
            ;;
        --output)
            OUTPUT_BASE="$2"
            shift 2
            ;;
        --hub-user)
            HUB_USER="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            exit 1
            ;;
    esac
done

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ReLoRA Multi-Iteration Training"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Configuration:"
echo "  Iterations:  $ITERATIONS"
echo "  Rank:        $RANK (effective rank after all iterations: $((ITERATIONS * RANK)))"
echo "  Base Model:  $BASE_MODEL"
echo "  Output Base: $OUTPUT_BASE"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

CURRENT_BASE="$BASE_MODEL"

for i in $(seq 1 $ITERATIONS); do
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ITERATION $i / $ITERATIONS"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "Base model: $CURRENT_BASE"
    echo ""

    ITER_OUTPUT="${OUTPUT_BASE}-iter${i}"
    MERGED_OUTPUT="${OUTPUT_BASE}-iter${i}-merged"

    # Create temporary config for this iteration
    ITER_CONFIG="/tmp/relora_iter${i}.yaml"

    cat > "$ITER_CONFIG" << EOF
# ReLoRA Iteration $i / $ITERATIONS
# Auto-generated by relora_train.sh

base_model: $CURRENT_BASE
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true

load_in_4bit: true
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true

adapter: lora
lora_r: $RANK
lora_alpha: $((RANK * 2))
lora_dropout: 0.05
lora_use_rslora: true

lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

datasets:
  - path: aphoticshaman/latticeforge-briefing-data
    type: sharegpt
    conversation: chatml

output_dir: $ITER_OUTPUT

sequence_len: 4096
sample_packing: true
pad_to_sequence_len: true

gradient_accumulation_steps: 4
micro_batch_size: 2
num_epochs: 1

learning_rate: 1e-4
lr_scheduler: cosine
warmup_ratio: 0.1

optimizer: adamw_bnb_8bit
weight_decay: 0.01
max_grad_norm: 1.0

bf16: auto
tf32: true

deepspeed: deepspeed_configs/zero2.json

logging_steps: 10
eval_steps: 100
save_steps: 500
save_total_limit: 1

val_set_size: 0.02
eval_sample_packing: false

seed: $((42 + i))
gradient_checkpointing: true
flash_attention: true
EOF

    echo "Step 1: Training LoRA iteration $i..."
    accelerate launch --multi_gpu --num_processes 2 \
        -m axolotl.cli.train "$ITER_CONFIG"

    echo ""
    echo "Step 2: Merging LoRA into base model..."

    # Merge using Python script
    python3 << MERGE_EOF
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

print("Loading base model: $CURRENT_BASE")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

base_model = AutoModelForCausalLM.from_pretrained(
    "$CURRENT_BASE",
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)

print("Loading LoRA adapter: $ITER_OUTPUT")
model = PeftModel.from_pretrained(base_model, "$ITER_OUTPUT")

print("Merging adapter into base...")
merged_model = model.merge_and_unload()

print("Saving merged model: $MERGED_OUTPUT")
merged_model.save_pretrained("$MERGED_OUTPUT", safe_serialization=True)

tokenizer = AutoTokenizer.from_pretrained("$CURRENT_BASE", trust_remote_code=True)
tokenizer.save_pretrained("$MERGED_OUTPUT")

print("Merge complete!")
MERGE_EOF

    # Update base for next iteration
    CURRENT_BASE="$MERGED_OUTPUT"

    echo ""
    echo "Iteration $i complete. Merged model: $MERGED_OUTPUT"
    echo "Effective rank so far: $((i * RANK))"
done

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ReLoRA Training Complete!"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Final model: $CURRENT_BASE"
echo "Effective rank: $((ITERATIONS * RANK))"
echo ""

# Push final model to hub
if [[ -n "$HF_TOKEN" ]]; then
    echo "Pushing final model to HuggingFace Hub..."
    FINAL_HUB_ID="${HUB_USER}/latticeforge-elle-72b-relora-r$((ITERATIONS * RANK))"

    python3 << PUSH_EOF
from huggingface_hub import HfApi
api = HfApi()
api.upload_folder(
    folder_path="$CURRENT_BASE",
    repo_id="$FINAL_HUB_ID",
    repo_type="model",
)
print(f"Pushed to: https://huggingface.co/$FINAL_HUB_ID")
PUSH_EOF
fi

echo ""
echo "Done! ðŸŽ‰"
