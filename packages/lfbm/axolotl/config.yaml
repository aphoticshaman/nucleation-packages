# =============================================================================
# Axolotl Fine-Tuning Config for LatticeForge Briefing Model
# =============================================================================
# Base model: Qwen2.5-7B-Instruct (better reasoning than 3B, fits easily on H200)
# Method: Full fine-tune (bf16) - no adapter overhead at inference
# Training time: ~20-30 min on 2x H200, ~1 hour on 1x H100
# Cost: ~$3-5 on RunPod with H200
#
# For smaller GPUs (A100, A10), use config_qlora.yaml instead
# =============================================================================

base_model: Qwen/Qwen2.5-7B-Instruct
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true

# Full fine-tune (no quantization - we have the VRAM)
load_in_4bit: false
load_in_8bit: false

# Dataset - HuggingFace repo or local path
datasets:
  - path: aphoticshaman/latticeforge-briefing-data
    type: sharegpt
    conversation: chatml
    # Fallback to local if HF not available:
    # path: /workspace/training_data.jsonl

# Output - push to HuggingFace
output_dir: /workspace/output/latticeforge-briefing-7b
hub_model_id: aphoticshaman/latticeforge-briefing-7b
push_to_hub: true

# Training hyperparameters - optimized for H200
sequence_len: 2048
sample_packing: true
pad_to_sequence_len: true

# Larger batches possible with H200's 141GB HBM3e
gradient_accumulation_steps: 2
micro_batch_size: 8
num_epochs: 5  # More epochs for better JSON compliance
learning_rate: 1e-5  # Lower LR for full fine-tune
lr_scheduler: cosine
warmup_ratio: 0.1

optimizer: adamw_torch
weight_decay: 0.01
max_grad_norm: 1.0

# Precision - bf16 native on H200
bf16: true
tf32: true

# Logging
logging_steps: 10
eval_steps: 50
save_steps: 200
save_total_limit: 3

# Eval
val_set_size: 0.05
eval_sample_packing: false

# Performance
seed: 42
gradient_checkpointing: true
flash_attention: true

# DeepSpeed for multi-GPU (2x H200)
deepspeed: deepspeed_configs/zero2.json

# =============================================================================
# ALTERNATIVE: QLoRA config for smaller GPUs (A100/A10)
# =============================================================================
# Uncomment below and comment out the full fine-tune settings above
# to use QLoRA instead (fits on 24GB+ GPUs)
#
# load_in_4bit: true
# adapter: qlora
# lora_r: 64
# lora_alpha: 128
# lora_dropout: 0.05
# lora_target_modules:
#   - q_proj
#   - k_proj
#   - v_proj
#   - o_proj
#   - gate_proj
#   - up_proj
#   - down_proj
# micro_batch_size: 2
# optimizer: adamw_bnb_8bit
# =============================================================================
