# =============================================================================
# ULTIMATE ELLE TRAINING CONFIGURATION
# =============================================================================
# Model: Qwen/Qwen2.5-72B-Instruct â†’ Elle (Geopolitical + Economics Expert)
# Target: Post-PhD senior analyst with junior analyst motivation and novel insight
# Domains: Geopolitics, Macroeconomics, Microeconomics, Wall Street, Finance
# GPU: 1x H200 (141GB HBM3e) or 2x A100 (80GB)
# =============================================================================

# -----------------------------------------------------------------------------
# Base Model Configuration
# -----------------------------------------------------------------------------
base_model: Qwen/Qwen2.5-72B-Instruct
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

# Trust remote code for Qwen-specific implementations
trust_remote_code: true

# Load in 4-bit NF4 quantization
load_in_8bit: false
load_in_4bit: true

# Quantization configuration (BitsAndBytes)
bnb_config:
  load_in_4bit: true
  bnb_4bit_compute_dtype: bfloat16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true  # Reduces memory further

# -----------------------------------------------------------------------------
# LoRA Configuration (Advanced)
# -----------------------------------------------------------------------------
adapter: lora

# High rank for complex reasoning
lora_r: 128
lora_alpha: 256  # 2x rank for stronger adaptation
lora_dropout: 0.05

# Target all attention and MLP layers
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

lora_target_linear: true
lora_fan_in_fan_out: false

# -----------------------------------------------------------------------------
# Advanced LoRA Techniques
# -----------------------------------------------------------------------------

# DoRA: Weight-Decomposed Low-Rank Adaptation
# Decomposes weights into magnitude and direction for better generalization
lora_use_dora: true

# rsLoRA: Rank-Stabilized LoRA
# Stabilizes training across different ranks using sqrt(r) scaling
lora_use_rslora: true

# LoRA+: Differential Learning Rates
# Higher learning rate for B matrix (16x) speeds up learning
loraplus_lr_ratio: 16

# -----------------------------------------------------------------------------
# NEFTune: Noisy Embedding Fine-Tuning
# -----------------------------------------------------------------------------
# Adds random noise to embeddings during training
# Proven to improve generalization and instruction-following
neftune_noise_alpha: 5

# -----------------------------------------------------------------------------
# Data Configuration
# -----------------------------------------------------------------------------
datasets:
  # Primary geopolitical intelligence dataset
  - path: aphoticshaman/latticeforge-briefing-data
    type: chatml
    split: train

  # Math and reasoning augmentation
  - path: AI-MO/NuminaMath-CoT
    type: sharegpt
    conversation: chatml
    split: train[:10000]

  # Strategic analysis patterns
  - path: Open-Orca/OpenOrca
    type: sharegpt
    conversation: chatml
    split: train[:5000]

  # ==========================================================================
  # ECONOMICS & FINANCE DATASETS
  # ==========================================================================

  # Macroeconomics - Fed policy, monetary theory, fiscal policy
  - path: sujet-ai/Sujet-Finance-Instruct-177k
    type: sharegpt
    conversation: chatml
    split: train[:20000]

  # Wall Street - Trading, markets, financial instruments
  - path: virattt/financial-qa-10K
    type: sharegpt
    conversation: chatml
    split: train

  # Economic reasoning and analysis
  - path: TheFinAI/flare-finqa
    type: sharegpt
    conversation: chatml
    split: train

  # SEC filings, earnings calls, financial news
  - path: FinGPT/fingpt-sentiment-train
    type: alpaca
    split: train[:15000]

  # Economic theory and academic economics
  - path: winddude/econ-qa
    type: alpaca
    split: train

  # Market analysis and forecasting patterns
  - path: amphora/forex-news
    type: alpaca
    split: train[:10000]

# Data processing
dataset_prepared_path: /workspace/cache/prepared
shuffle_merged_datasets: true

# Chat template
chat_template: chatml
default_system_message: |
  You are Elle, a senior intelligence analyst with post-doctoral expertise spanning
  geopolitics, macroeconomics, microeconomics, and financial markets. Your background
  includes deep experience with Federal Reserve policy analysis, Wall Street trading
  dynamics, international monetary systems, and the intersection of economic power
  with strategic competition.

  You combine rigorous analytical methodology with the curiosity and energy of a
  junior analyst, always seeking novel insights and unconventional connections.
  You understand how capital flows, interest rate differentials, currency dynamics,
  and fiscal policy decisions cascade through global systems to create second and
  third-order geopolitical effects.

  Your analysis is precise, evidence-based, and considers multiple perspectives
  while maintaining intellectual humility. You can explain complex derivatives,
  analyze central bank decisions, forecast market movements based on geopolitical
  triggers, and identify the economic warfare dimensions of international conflicts.

# -----------------------------------------------------------------------------
# Training Configuration
# -----------------------------------------------------------------------------

# Sequence length (Qwen supports 32k, we use 8k for efficiency)
sequence_len: 8192
sample_packing: true
pad_to_sequence_len: true

# Batch configuration (optimized for H200)
micro_batch_size: 2
gradient_accumulation_steps: 8
# Effective batch size: 2 * 8 = 16

# Training duration
num_epochs: 3
max_steps: -1

# Optimizer configuration
optimizer: paged_adamw_8bit
lr_scheduler: cosine
learning_rate: 2.0e-5
weight_decay: 0.01
warmup_ratio: 0.03

# Gradient handling
max_grad_norm: 1.0
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

# -----------------------------------------------------------------------------
# Memory Optimization
# -----------------------------------------------------------------------------

# Flash Attention 2
flash_attention: true
eager_attention: false

# BF16 mixed precision (H200 native)
bf16: true
fp16: false
tf32: true

# xFormers memory efficient attention (fallback)
xformers_attention: false

# DeepSpeed (for multi-GPU)
deepspeed: null  # Use configs/deepspeed_zero2.json for multi-GPU

# -----------------------------------------------------------------------------
# Checkpointing & Logging
# -----------------------------------------------------------------------------
output_dir: /workspace/outputs/elle-ultimate
save_strategy: steps
save_steps: 500
save_total_limit: 3

# Evaluation
eval_strategy: steps
eval_steps: 250
evaluation_strategy: steps

# Logging
logging_steps: 10
report_to: wandb
wandb_project: lfbm-elle
wandb_entity: null  # Set via WANDB_ENTITY env var
wandb_run_name: elle-ultimate-v1

# -----------------------------------------------------------------------------
# Safety & Quality
# -----------------------------------------------------------------------------

# Early stopping
early_stopping_patience: 3
load_best_model_at_end: true
metric_for_best_model: eval_loss
greater_is_better: false

# Reproducibility
seed: 42
data_seed: 42

# -----------------------------------------------------------------------------
# Post-Training
# -----------------------------------------------------------------------------

# Merge LoRA weights after training
merge_lora: true
lora_merge_strategy: ties  # TIES merging for better quality

# Save formats
save_safetensors: true
push_to_hub: false  # Manual upload after validation
hub_model_id: aphoticshaman/elle-72b-ultimate

# GGUF conversion (for llama.cpp compatibility)
gguf_quantization_types:
  - Q4_K_M
  - Q5_K_M
  - Q8_0

# -----------------------------------------------------------------------------
# Validation Prompts
# -----------------------------------------------------------------------------
val_prompts:
  # Geopolitical
  - |
    Analyze the strategic implications of China's Belt and Road Initiative
    for European energy security over the next decade.
  - |
    What are the second and third-order effects of increased Arctic
    shipping routes on global trade patterns?
  - |
    Provide a multi-perspective analysis of the India-China border
    tensions and potential escalation scenarios.

  # Macroeconomics
  - |
    Analyze the likely Federal Reserve response to persistent 4% inflation
    combined with rising unemployment. What are the transmission mechanisms
    to emerging market currencies and sovereign debt markets?
  - |
    How does the unwinding of the yen carry trade affect global liquidity
    conditions and risk asset valuations?

  # Wall Street / Finance
  - |
    A major investment bank just reported significant losses in their prime
    brokerage division. Walk through the potential contagion pathways to
    other financial institutions and broader market stability.
  - |
    Explain how a sovereign wealth fund might use currency swaps and
    interest rate derivatives to hedge geopolitical exposure while
    maintaining strategic asset allocations.

  # Intersection
  - |
    The BRICS nations announce a new commodity-backed trade settlement
    system. Analyze the immediate and medium-term effects on dollar
    hegemony, Treasury yields, and global capital flows.

# -----------------------------------------------------------------------------
# Expected Resource Usage
# -----------------------------------------------------------------------------
# GPU Memory: ~120GB (with 4-bit quantization + gradient checkpointing)
# Training Time: ~12-18 hours for 3 epochs on H200
# Storage: ~100GB for checkpoints + ~15GB for final model
# =============================================================================
