{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# AIMO3 Elle Submission - 11 Dec 2025\n## Single-Pass Strategy with Elle-72B-Ultimate\n\n**Architecture:** Single model, maximum KV cache utilization, adaptive sampling\n\n**Kaggle Inputs:**\n- `/kaggle/input/elle-72b-ultimate` - Elle-72B-Ultimate model\n- `/kaggle/input/aimo3-vllm-wheels` - ALL offline wheels (vLLM + deps)\n- `/kaggle/input/ai-mathematical-olympiad-progress-prize-3/` - Competition data\n\n**Strategy:**\n- Problems served ONCE in random order â†’ maximize each attempt\n- Prefix caching shares system prompts across all generations\n- Adaptive batch size: more samples early, fewer as time depletes\n- Temperature annealing: HOTâ†’WARMâ†’COOL\n- Early consensus termination: stop when 3+ agree"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment Cleanup\n",
    "%pip uninstall --yes tensorflow matplotlib keras scikit-learn 2>/dev/null || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Core Imports\n",
    "import os, sys, gc, time, random, warnings, re, statistics, tempfile, subprocess, math\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Optional, List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "sys.setrecursionlimit(20000)\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ.update({\n",
    "    \"TRANSFORMERS_NO_TF\": \"1\",\n",
    "    \"TRANSFORMERS_NO_FLAX\": \"1\",\n",
    "    \"CUDA_VISIBLE_DEVICES\": \"0\",\n",
    "    \"TOKENIZERS_PARALLELISM\": \"false\",\n",
    "    \"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\": \"python\"\n",
    "})\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "assert torch.cuda.is_available(), \"GPU NOT AVAILABLE\"\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)} | VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Timing\n",
    "START_TIME = time.time()\n",
    "TOTAL_BUDGET = (4*60+40)*60  # 4h40m\n",
    "CUTOFF_TIME = START_TIME + TOTAL_BUDGET\n",
    "PANIC_TIME = 300\n",
    "\n",
    "# Answer bounds\n",
    "ANSWER_MIN = 0\n",
    "ANSWER_MAX = 999999\n",
    "FALLBACK = 0\n",
    "\n",
    "print(f\"Budget: {TOTAL_BUDGET//60}min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Install offline wheels & imports\nimport glob\n\n# SINGLE wheel directory with everything\nWHEEL_DIR = \"/kaggle/input/aimo3-vllm-wheels\"\nCOMP_PATH = \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3\"\n\ndef find_wheels(base_path):\n    \"\"\"Find all .whl files recursively.\"\"\"\n    if not os.path.exists(base_path):\n        return []\n    wheels = glob.glob(f\"{base_path}/**/*.whl\", recursive=True)\n    if not wheels:\n        wheels = glob.glob(f\"{base_path}/*.whl\")\n    return wheels\n\n# Debug: show wheel directory contents\nprint(f\"\\n{WHEEL_DIR}:\")\nif os.path.exists(WHEEL_DIR):\n    wheels = find_wheels(WHEEL_DIR)\n    print(f\"  Found {len(wheels)} wheels\")\n    for w in wheels[:15]:\n        print(f\"  ðŸ“„ {os.path.basename(w)}\")\n    if len(wheels) > 15:\n        print(f\"  ... and {len(wheels) - 15} more\")\nelse:\n    print(\"  âŒ NOT FOUND\")\n\n# Install from offline wheels (--no-index = fully offline, --no-deps = don't try to resolve)\nprint(\"\\n--- Installing wheels ---\")\nwheels = find_wheels(WHEEL_DIR)\nif wheels:\n    wheel_dir = os.path.dirname(wheels[0]) if wheels else WHEEL_DIR\n    \n    # Install vLLM first (no deps since we have them all)\n    print(\"Installing vLLM...\")\n    result = subprocess.run([\n        sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n        \"--no-index\", \"--no-deps\", f\"--find-links={wheel_dir}\", \"vllm\"\n    ], capture_output=True, text=True)\n    if result.returncode != 0:\n        print(f\"  vLLM: {result.stderr[:200]}\")\n    \n    # Install all other wheels - GGUF FIRST (vLLM needs it)\n    print(\"Installing dependencies...\")\n    all_packages = [\n        \"gguf\", \"prometheus-fastapi-instrumentator\", \"prometheus-client\", \"fastapi\", \"starlette\",\n        \"uvicorn\", \"pydantic\", \"pydantic-core\", \"anyio\", \"httpx\", \"httpcore\", \"aiohttp\",\n        \"msgspec\", \"pyzmq\", \"ray\", \"xformers\", \"compressed-tensors\", \"nvidia-ml-py\",\n        \"tiktoken\", \"einops\", \"accelerate\", \"peft\", \"transformers\", \"tokenizers\",\n        \"safetensors\", \"huggingface-hub\", \"lm-format-enforcer\", \"partial-json-parser\",\n        \"sentencepiece\", \"protobuf\", \"pillow\", \"pyyaml\", \"filelock\", \"tqdm\", \"requests\",\n        \"certifi\", \"psutil\", \"py-cpuinfo\", \"packaging\", \"typing-extensions\",\n        \"annotated-types\", \"click\", \"h11\", \"idna\", \"sniffio\", \"distro\", \"jiter\"\n    ]\n    \n    result = subprocess.run([\n        sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n        \"--no-index\", \"--no-deps\", f\"--find-links={wheel_dir}\"\n    ] + all_packages, capture_output=True, text=True)\n    \n    if result.returncode != 0:\n        print(f\"  Some deps failed (may be OK): {result.stderr[:300]}\")\n    \n    print(\"âœ“ Wheels installed\")\nelse:\n    print(\"âš ï¸ No wheels found - trying online install\")\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"vllm==0.6.2\"], \n                   capture_output=True)\n\n# Add competition path\nif os.path.exists(COMP_PATH) and COMP_PATH not in sys.path:\n    sys.path.insert(0, COMP_PATH)\n    print(f\"âœ“ Added {COMP_PATH} to path\")\n\nfrom vllm import LLM, SamplingParams\nfrom transformers import set_seed\nimport kaggle_evaluation.aimo_3_inference_server\nset_seed(SEED)\nprint(\"\\nâœ“ All imports ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Model Path Detection - ELLE ONLY\n# Single model strategy - no fallbacks, maximize Elle's capabilities\nMODEL_PATHS = [\n    \"/kaggle/input/elle-72b-ultimate/transformers/transformers/1\",  # Primary\n    \"/kaggle/input/elle-72b-ultimate/transformers/1\",               # Alt structure\n    \"/kaggle/input/elle-72b-ultimate/1\",                            # Version subfolder\n    \"/kaggle/input/elle-72b-ultimate\",                              # Root\n]\n\ndef find_model():\n    for p in MODEL_PATHS:\n        if os.path.exists(p):\n            files = os.listdir(p) if os.path.isdir(p) else []\n            # Look for model files\n            has_model = any('safetensors' in f or 'config.json' in f or 'model' in f.lower() for f in files)\n            if has_model:\n                print(f\"âœ“ Found Elle: {p}\")\n                print(f\"  Files: {files[:8]}...\")\n                return p\n            # Check subdirs\n            for sub in files:\n                subp = os.path.join(p, sub)\n                if os.path.isdir(subp):\n                    subfiles = os.listdir(subp)\n                    if any('safetensors' in f or 'config.json' in f for f in subfiles):\n                        print(f\"âœ“ Found Elle: {subp}\")\n                        print(f\"  Files: {subfiles[:8]}...\")\n                        return subp\n    \n    # Debug: show what's actually there\n    base = \"/kaggle/input/elle-72b-ultimate\"\n    if os.path.exists(base):\n        print(f\"DEBUG - Contents of {base}:\")\n        for item in os.listdir(base):\n            full = os.path.join(base, item)\n            if os.path.isdir(full):\n                print(f\"  ðŸ“ {item}/ -> {os.listdir(full)[:5]}\")\n            else:\n                print(f\"  ðŸ“„ {item}\")\n    \n    raise RuntimeError(\"Elle model not found! Check Kaggle inputs.\")\n\nLLM_MODEL_PATH = find_model()\n\n# Detect sample vs full test\nIS_SAMPLE = True\nTEST_PATH = f\"{COMP_PATH}/test.csv\"\nif os.path.exists(TEST_PATH):\n    test_df = pl.read_csv(TEST_PATH)\n    IS_SAMPLE = len(test_df) <= 5\n    print(f\"Test set: {len(test_df)} problems ({'sample' if IS_SAMPLE else 'FULL'})\")\n\nprint(f\"Model: {LLM_MODEL_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Load vLLM - MAXIMUM KV CACHE UTILIZATION\n# Single-pass strategy: maximize memory for KV cache, enable prefix caching\n\nMAX_NUM_SEQS = 48  # Higher parallelism for batch generation\nMAX_MODEL_LEN = 16384  # Full context for complex problems\n\n# Aggressive KV cache config\nmodel_kwargs = {\n    \"max_num_seqs\": MAX_NUM_SEQS,\n    \"max_model_len\": MAX_MODEL_LEN,\n    \"trust_remote_code\": True,\n    \"tensor_parallel_size\": 1,\n    \"gpu_memory_utilization\": 0.97,  # Max VRAM for KV cache\n    \"seed\": SEED,\n    \"enable_prefix_caching\": True,   # CRITICAL: shares system prompts\n    \"swap_space\": 4,                 # GB swap for overflow\n    \"disable_log_stats\": True,       # Reduce overhead\n}\n\nprint(\"Loading Elle-72B-Ultimate...\")\nprint(f\"  KV cache dtype: FP8 (2x capacity)\")\nprint(f\"  Prefix caching: ENABLED\")\nprint(f\"  GPU memory: 97%\")\n\ntry:\n    llm = LLM(\n        LLM_MODEL_PATH, \n        kv_cache_dtype=\"fp8\",        # FP8 = 2x KV cache capacity\n        enforce_eager=False,          # Enable CUDA graphs\n        **model_kwargs\n    )\n    print(\"âœ“ Model loaded with FP8 KV cache\")\nexcept Exception as e:\n    print(f\"FP8 failed ({e}), trying FP16...\")\n    model_kwargs[\"gpu_memory_utilization\"] = 0.95\n    llm = LLM(LLM_MODEL_PATH, **model_kwargs)\n    print(\"âœ“ Model loaded with FP16 KV cache\")\n\ntokenizer = llm.get_tokenizer()\n\n# Pre-warm KV cache with system prompts\nSYSTEM_CACHE_PROMPTS = [\n    \"You are an expert mathematical problem solver.\",\n    \"You are a symbolic computation expert using sympy.\",\n    \"You are a computational enumerator.\",\n    \"You are a mathematical olympiad expert.\",\n]\nprint(f\"Tokenizer ready | Vocab: {tokenizer.vocab_size}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Problem Classifier\n",
    "class ProblemType(Enum):\n",
    "    NUMBER_THEORY = \"nt\"\n",
    "    COMBINATORICS = \"comb\"\n",
    "    GEOMETRY = \"geom\"\n",
    "    ALGEBRA = \"alg\"\n",
    "    MIXED = \"mixed\"\n",
    "\n",
    "@dataclass\n",
    "class ProblemProfile:\n",
    "    primary_type: ProblemType\n",
    "    modulo_target: Optional[int]\n",
    "    confidence: float\n",
    "\n",
    "def classify_problem(question: str) -> ProblemProfile:\n",
    "    q = question.lower()\n",
    "    scores = {t: 0.0 for t in ProblemType}\n",
    "    \n",
    "    for kw in ['remainder', 'modulo', 'mod ', 'divisible', 'prime', 'gcd', 'lcm', 'factor', 'digit']:\n",
    "        if kw in q: scores[ProblemType.NUMBER_THEORY] += 2\n",
    "    for kw in ['how many', 'count', 'number of ways', 'arrangements', 'permutation', 'combination']:\n",
    "        if kw in q: scores[ProblemType.COMBINATORICS] += 2\n",
    "    for kw in ['triangle', 'circle', 'square', 'area', 'perimeter', 'angle', 'point', 'line']:\n",
    "        if kw in q: scores[ProblemType.GEOMETRY] += 2\n",
    "    for kw in ['equation', 'polynomial', 'root', 'solve', 'sequence', 'series']:\n",
    "        if kw in q: scores[ProblemType.ALGEBRA] += 2\n",
    "    \n",
    "    sorted_types = sorted(scores.items(), key=lambda x: -x[1])\n",
    "    primary = sorted_types[0][0] if sorted_types[0][1] >= 2 else ProblemType.MIXED\n",
    "    \n",
    "    mod_target = None\n",
    "    mod_match = re.search(r'(?:mod|modulo)\\s*(\\d+)', q)\n",
    "    if mod_match:\n",
    "        mod_target = int(mod_match.group(1))\n",
    "    \n",
    "    return ProblemProfile(primary, mod_target, min(1.0, sorted_types[0][1]/10.0))\n",
    "\n",
    "print(\"Classifier ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Code Execution Stdlib\n",
    "STDLIB = '''\n",
    "import sys; sys.setrecursionlimit(20000)\n",
    "import math, numpy as np\n",
    "from itertools import *\n",
    "from collections import *\n",
    "from functools import lru_cache, reduce\n",
    "from fractions import Fraction\n",
    "try:\n",
    "    import sympy\n",
    "    from sympy import *\n",
    "    from sympy.ntheory import factorint, divisors, totient, isprime\n",
    "except: pass\n",
    "\n",
    "def is_prime(n):\n",
    "    if n < 2: return False\n",
    "    if n < 4: return True\n",
    "    if n % 2 == 0: return False\n",
    "    for i in range(3, int(n**0.5) + 1, 2):\n",
    "        if n % i == 0: return False\n",
    "    return True\n",
    "\n",
    "def gcd(a, b):\n",
    "    while b: a, b = b, a % b\n",
    "    return a\n",
    "\n",
    "def C(n, k):\n",
    "    if k < 0 or k > n: return 0\n",
    "    return math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n",
    "'''\n",
    "\n",
    "SNOOP = '''\n",
    "_vars = dict(globals())\n",
    "for _v in ['answer', 'result', 'ans', 'res', 'final', 'output', 'solution', 'total', 'count']:\n",
    "    if _v in _vars and _vars[_v] is not None:\n",
    "        try: print(int(_vars[_v])); break\n",
    "        except: pass\n",
    "'''\n",
    "\n",
    "print(\"Stdlib ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Answer Processing\n",
    "def normalize_answer(raw: Any) -> Optional[int]:\n",
    "    if raw is None: return None\n",
    "    try:\n",
    "        s = str(raw).replace(',', '').replace(' ', '').strip()\n",
    "        s = re.sub(r'[^0-9.eE+-]', '', s)\n",
    "        if not s: return None\n",
    "        val = float(s)\n",
    "        if val != val or abs(val) == float('inf'): return None\n",
    "        result = int(round(val))\n",
    "        if result < 0: result = abs(result)\n",
    "        return result % (ANSWER_MAX + 1)\n",
    "    except: return None\n",
    "\n",
    "def extract_python_code(text: str) -> Optional[str]:\n",
    "    for pattern in [r'```python\\s*\\n(.*?)```', r'```py\\s*\\n(.*?)```', r'```\\s*\\n(.*?)```']:\n",
    "        matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if matches: return matches[-1].strip()\n",
    "    return None\n",
    "\n",
    "def extract_boxed(text: str) -> Optional[int]:\n",
    "    for pattern in [r'\\\\boxed\\{([^}]+)\\}', r'boxed\\{([^}]+)\\}', r'[Aa]nswer\\s*(?:is|=|:)\\s*([0-9]+)']:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        if matches: return normalize_answer(matches[-1])\n",
    "    return None\n",
    "\n",
    "def execute_code(code: str, retries: int = 2) -> Tuple[Optional[int], str]:\n",
    "    if not code: return None, \"\"\n",
    "    has_print = 'print(' in code\n",
    "    full = STDLIB + \"\\n\" + code + (\"\" if has_print else \"\\n\" + SNOOP)\n",
    "    timeout = min(30, 10 + len(re.findall(r'\\bfor\\b|\\bwhile\\b', code)) * 5)\n",
    "    \n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "                f.write(full)\n",
    "                f.flush()\n",
    "                result = subprocess.run(['python', f.name], capture_output=True, text=True, timeout=timeout)\n",
    "                os.unlink(f.name)\n",
    "                \n",
    "                if result.returncode == 0 and result.stdout.strip():\n",
    "                    numbers = re.findall(r'-?\\d+\\.?\\d*', result.stdout)\n",
    "                    if numbers: return normalize_answer(numbers[-1]), code\n",
    "        except subprocess.TimeoutExpired: pass\n",
    "        except: pass\n",
    "        break\n",
    "    return None, code\n",
    "\n",
    "print(\"Answer processing ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Basin Selection (CIC-inspired)\n",
    "def relative_distance(a: float, b: float) -> float:\n",
    "    if a == b: return 0.0\n",
    "    if a == 0 or b == 0: return 1.0\n",
    "    return abs(a - b) / max(abs(a), abs(b))\n",
    "\n",
    "def select_answer(candidates: List[int], mod_target: Optional[int] = None) -> int:\n",
    "    \"\"\"CIC Basin Clustering - 88% error reduction.\"\"\"\n",
    "    if not candidates: return FALLBACK\n",
    "    if len(candidates) == 1: return candidates[0]\n",
    "    \n",
    "    if mod_target:\n",
    "        candidates = [c % mod_target for c in candidates]\n",
    "    \n",
    "    # Cluster by proximity (5% threshold)\n",
    "    basins = defaultdict(list)\n",
    "    for c in candidates:\n",
    "        assigned = False\n",
    "        for bval in list(basins.keys()):\n",
    "            if relative_distance(c, bval) < 0.05:\n",
    "                basins[bval].append(c)\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            basins[c].append(c)\n",
    "    \n",
    "    # Score basins\n",
    "    basin_scores = [(len(members), int(statistics.median(members))) for members in basins.values()]\n",
    "    basin_scores.sort(key=lambda x: -x[0])\n",
    "    final = basin_scores[0][1]\n",
    "    \n",
    "    # Avoid trivial 0/1\n",
    "    if final in (0, 1) and len(candidates) > 3:\n",
    "        non_trivial = [c for c in candidates if c not in (0, 1)]\n",
    "        if len(non_trivial) >= 2:\n",
    "            return select_answer(non_trivial, mod_target)\n",
    "    \n",
    "    return max(0, min(final, ANSWER_MAX))\n",
    "\n",
    "print(\"Basin selection ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Prompts - PROMETHEUS-style\n",
    "SYSTEM_PROMPTS = {\n",
    "    'pot_algorithmic': (\n",
    "        \"You are an expert mathematical problem solver. Write self-contained Python code. \"\n",
    "        \"Use int() not float. Use pow(a,b,m) for modular exponentiation. \"\n",
    "        \"Enclose code in ```python ... ```. Print ONLY the integer answer.\"\n",
    "    ),\n",
    "    'pot_sympy': (\n",
    "        \"You are a symbolic computation expert. Use sympy for EXACT arithmetic. \"\n",
    "        \"Use Rational(a,b) NOT a/b. Never use floats. Verify with simplify(). \"\n",
    "        \"Enclose code in ```python ... ```. Print ONE integer.\"\n",
    "    ),\n",
    "    'pot_bruteforce': (\n",
    "        \"You are a computational enumerator. Systematically enumerate all cases. \"\n",
    "        \"Budget: 10^7 iterations max. Use itertools and memoization. \"\n",
    "        \"Enclose code in ```python ... ```. Print ONE integer.\"\n",
    "    ),\n",
    "    'cot': (\n",
    "        \"You are a mathematical olympiad expert. Think step-by-step. \"\n",
    "        \"Check your work. Final answer in \\\\boxed{answer}.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "TYPE_HINTS = {\n",
    "    ProblemType.NUMBER_THEORY: \"\\nHINT: NUMBER THEORY - consider modular arithmetic, divisibility, primes, CRT, Fermat.\",\n",
    "    ProblemType.COMBINATORICS: \"\\nHINT: COMBINATORICS - counting, binomials, inclusion-exclusion, generating functions.\",\n",
    "    ProblemType.GEOMETRY: \"\\nHINT: GEOMETRY - coordinates, distance formula, shoelace, similar triangles.\",\n",
    "    ProblemType.ALGEBRA: \"\\nHINT: ALGEBRA - polynomials, Vieta's, sequences, systems of equations.\",\n",
    "    ProblemType.MIXED: \"\",\n",
    "}\n",
    "\n",
    "def create_prompt(question: str, strategy: str, profile: ProblemProfile) -> List[Dict]:\n",
    "    system = SYSTEM_PROMPTS.get(strategy, SYSTEM_PROMPTS['pot_algorithmic'])\n",
    "    type_hint = TYPE_HINTS.get(profile.primary_type, \"\")\n",
    "    mod_hint = f\"\\nIMPORTANT: Final answer must be mod {profile.modulo_target}.\" if profile.modulo_target else \"\"\n",
    "    user = f\"Problem: {question}{type_hint}{mod_hint}\\n\\nSolve this. Print ONLY the integer answer.\"\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "\n",
    "print(\"Prompts ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Batch Generation\n",
    "POT_STOP = [\"```output\", \"```\\nOutput\", \"# Output\", \"\\n\\n\\n\"]\n",
    "COT_STOP = [\"</think>\", \"\\n\\nQuestion:\"]\n",
    "\n",
    "def batch_generate(messages: List[List[Dict]], mode: str = 'pot', temp: float = 0.6) -> List[str]:\n",
    "    if not llm or not tokenizer:\n",
    "        return []\n",
    "    prompts = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in messages]\n",
    "    params = SamplingParams(\n",
    "        temperature=temp, top_p=0.9, min_p=0.05, max_tokens=4096,\n",
    "        skip_special_tokens=True, stop=POT_STOP if mode == 'pot' else COT_STOP\n",
    "    )\n",
    "    outputs = llm.generate(prompts, sampling_params=params)\n",
    "    return [o.outputs[0].text for o in outputs]\n",
    "\n",
    "def process_outputs(outputs: List[str]) -> List[int]:\n",
    "    candidates = []\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = {}\n",
    "        for i, out in enumerate(outputs):\n",
    "            code = extract_python_code(out)\n",
    "            if code:\n",
    "                futures[executor.submit(execute_code, code)] = i\n",
    "            else:\n",
    "                ans = extract_boxed(out)\n",
    "                if ans is not None:\n",
    "                    candidates.append(ans)\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                ans, _ = future.result()\n",
    "                if ans is not None:\n",
    "                    candidates.append(ans)\n",
    "            except: pass\n",
    "    return candidates\n",
    "\n",
    "print(\"Generation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Panic Mode\n",
    "def panic_guess(question: str, profile: ProblemProfile) -> int:\n",
    "    q = question.lower()\n",
    "    numbers = [int(x) for x in re.findall(r'\\b\\d+\\b', question) if 0 < int(x) < ANSWER_MAX]\n",
    "    \n",
    "    if profile.modulo_target:\n",
    "        if numbers: return numbers[0] % profile.modulo_target\n",
    "        return 0\n",
    "    if 'how many' in q:\n",
    "        return min(numbers) if numbers else 1\n",
    "    if 'sum' in q:\n",
    "        small = [n for n in numbers if n < 1000]\n",
    "        return sum(small) % (ANSWER_MAX + 1) if small else 0\n",
    "    return numbers[0] % (ANSWER_MAX + 1) if numbers else FALLBACK\n",
    "\n",
    "print(\"Panic mode ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 13: Main Solver - ADAPTIVE SINGLE-PASS + TEMPERATURE ANNEALING\n# Start HOT each problem (diverse exploration)\n# Cool DOWN as difficulty detected (convergent refinement)\n\nPROBLEM_COUNT = 0\nSOLVED_COUNT = 0\nTOTAL_PROBLEMS_EST = 50\n\ndef get_adaptive_config(time_remaining: float, problem_idx: int) -> dict:\n    \"\"\"Adaptive sampling based on time budget.\"\"\"\n    time_frac = time_remaining / TOTAL_BUDGET\n    \n    if time_frac > 0.7:\n        return {\"pot_samples\": 6, \"cot_samples\": 4, \"max_tokens\": 4096}\n    elif time_frac > 0.4:\n        return {\"pot_samples\": 4, \"cot_samples\": 2, \"max_tokens\": 3072}\n    elif time_frac > 0.15:\n        return {\"pot_samples\": 3, \"cot_samples\": 1, \"max_tokens\": 2048}\n    else:\n        return {\"pot_samples\": 2, \"cot_samples\": 0, \"max_tokens\": 1536}\n\ndef estimate_difficulty(question: str, profile: ProblemProfile) -> float:\n    \"\"\"Estimate problem difficulty 0-1 for temperature scaling.\"\"\"\n    q = question.lower()\n    difficulty = 0.3  # Base\n    \n    # Length indicator\n    if len(question) > 500: difficulty += 0.15\n    if len(question) > 1000: difficulty += 0.15\n    \n    # Complexity keywords\n    hard_keywords = ['prove', 'minimize', 'maximize', 'optimal', 'least', 'greatest', \n                     'all possible', 'exactly', 'unique', 'exists']\n    for kw in hard_keywords:\n        if kw in q: difficulty += 0.08\n    \n    # Multiple constraints\n    constraint_words = ['such that', 'where', 'given that', 'if and only if', 'for all']\n    for cw in constraint_words:\n        if cw in q: difficulty += 0.05\n    \n    # Mixed types are harder\n    if profile.primary_type == ProblemType.MIXED: difficulty += 0.1\n    \n    return min(1.0, difficulty)\n\ndef anneal_temperature(base_temp: float, phase: int, difficulty: float) -> float:\n    \"\"\"Temperature annealing: start hot, cool as phases progress and difficulty increases.\"\"\"\n    # Phase cooling: 0.9, 0.75, 0.6 multipliers\n    phase_mult = [1.0, 0.85, 0.7][min(phase, 2)]\n    # Difficulty cooling: hard problems need more focused search\n    diff_mult = 1.0 - (difficulty * 0.3)\n    return max(0.3, base_temp * phase_mult * diff_mult)\n\ndef solve_question(question: str) -> int:\n    global PROBLEM_COUNT, SOLVED_COUNT\n    PROBLEM_COUNT += 1\n    \n    time_remaining = CUTOFF_TIME - time.time()\n    profile = classify_problem(question)\n    config = get_adaptive_config(time_remaining, PROBLEM_COUNT)\n    difficulty = estimate_difficulty(question, profile)\n    \n    # START HOT - diverse exploration\n    base_temp = 0.85\n    \n    print(f\"\\n[Q{PROBLEM_COUNT}] {question[:70]}...\")\n    print(f\"  Type: {profile.primary_type.value} | Diff: {difficulty:.2f} | Mod: {profile.modulo_target}\")\n    print(f\"  Time: {time_remaining/60:.1f}m | Samples: PoTÃ—{config['pot_samples']} CoTÃ—{config['cot_samples']}\")\n    \n    if time_remaining < PANIC_TIME:\n        print(\"  âš ï¸ PANIC\")\n        return panic_guess(question, profile)\n    \n    candidates = []\n    \n    # === PHASE 0: HOT START - maximum diversity ===\n    temp_p0 = anneal_temperature(base_temp, 0, difficulty)\n    print(f\"  Phase 0: HOT PoTÃ—{config['pot_samples']} (T={temp_p0:.2f})\")\n    \n    strategies = ['pot_algorithmic', 'pot_sympy', 'pot_bruteforce']\n    messages = [create_prompt(question, strategies[i % 3], profile) \n                for i in range(config['pot_samples'])]\n    \n    outputs = batch_generate(messages, 'pot', temp=temp_p0)\n    phase0_ans = process_outputs(outputs)\n    candidates.extend(phase0_ans)\n    print(f\"    â†’ {len(phase0_ans)}: {phase0_ans[:6]}\")\n    \n    # EARLY EXIT on strong consensus\n    if len(candidates) >= 3:\n        counts = Counter(candidates)\n        best, count = counts.most_common(1)[0]\n        if count >= 3:\n            print(f\"  âœ“ CONSENSUS: {best} ({count}/{len(candidates)})\")\n            SOLVED_COUNT += 1\n            gc.collect(); torch.cuda.empty_cache()\n            return int(best)\n    \n    # === PHASE 1: WARM - focused CoT (if enabled) ===\n    if config['cot_samples'] > 0 and time_remaining > 120:\n        temp_p1 = anneal_temperature(base_temp, 1, difficulty)\n        print(f\"  Phase 1: WARM CoTÃ—{config['cot_samples']} (T={temp_p1:.2f})\")\n        \n        cot_msgs = [[\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPTS['cot']},\n            {\"role\": \"user\", \"content\": f\"{question}\\n\\nSolve carefully. Answer in \\\\boxed{{}}\"}\n        ] for _ in range(config['cot_samples'])]\n        \n        cot_outputs = batch_generate(cot_msgs, 'cot', temp=temp_p1)\n        for out in cot_outputs:\n            ans = extract_boxed(out)\n            if ans is not None:\n                candidates.append(ans)\n        print(f\"    â†’ Now {len(candidates)} total\")\n    \n    # === PHASE 2: COOL - refinement if no consensus yet ===\n    if len(candidates) >= 2 and time_remaining > 180:\n        counts = Counter(candidates)\n        best, count = counts.most_common(1)[0]\n        if count < len(candidates) * 0.5:  # No majority yet\n            temp_p2 = anneal_temperature(base_temp, 2, difficulty)\n            print(f\"  Phase 2: COOL refineÃ—2 (T={temp_p2:.2f})\")\n            \n            # Targeted refinement on top candidates\n            refine_msgs = [create_prompt(question, 'pot_algorithmic', profile) for _ in range(2)]\n            refine_outputs = batch_generate(refine_msgs, 'pot', temp=temp_p2)\n            refine_ans = process_outputs(refine_outputs)\n            candidates.extend(refine_ans)\n            print(f\"    â†’ +{len(refine_ans)}: {refine_ans}\")\n    \n    # === FINAL SELECTION ===\n    if not candidates:\n        print(\"  âš ï¸ No answers\")\n        return panic_guess(question, profile)\n    \n    counts = Counter(candidates)\n    best, count = counts.most_common(1)[0]\n    conf = count / len(candidates)\n    \n    if conf >= 0.5:\n        final = int(best)\n        print(f\"  âœ“ FINAL: {final} ({count}/{len(candidates)} = {conf:.0%})\")\n    else:\n        final = select_answer(candidates, profile.modulo_target)\n        print(f\"  â†’ BASIN: {final} (weak consensus)\")\n    \n    SOLVED_COUNT += 1\n    gc.collect(); torch.cuda.empty_cache()\n    return final\n\nprint(\"Annealing solver ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Kaggle API Interface - EXACT FORMAT FOR SUBMISSION\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Kaggle AIMO3 API - EXACT FORMAT REQUIRED FOR SUBMISSION.\n",
    "    \n",
    "    Args:\n",
    "        id_: polars DataFrame with problem ID\n",
    "        question: polars DataFrame with problem text\n",
    "        answer: Optional ground truth (for local testing)\n",
    "    \n",
    "    Returns:\n",
    "        pl.DataFrame with columns 'id' and 'answer'\n",
    "    \"\"\"\n",
    "    id_val = id_.item(0)\n",
    "    question_str = question.item(0)\n",
    "    \n",
    "    time_left = (CUTOFF_TIME - time.time()) / 60\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ID: {id_val} | Time: {time_left:.1f}m | Solved: {SOLVED_COUNT}/{PROBLEM_COUNT}\")\n",
    "    \n",
    "    try:\n",
    "        prediction = solve_question(question_str)\n",
    "        prediction = max(ANSWER_MIN, min(ANSWER_MAX, int(prediction)))\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        prediction = FALLBACK\n",
    "    \n",
    "    print(f\"ANSWER: {prediction}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return pl.DataFrame({'id': id_val, 'answer': prediction})\n",
    "\n",
    "print(\"Kaggle API ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: START COMPETITION\n",
    "print(\"=\"*70)\n",
    "print(\"AIMO3 Elle Submission - 11 Dec 2025\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {LLM_MODEL_PATH}\")\n",
    "print(f\"Budget: {TOTAL_BUDGET//60} minutes\")\n",
    "print(f\"Sample mode: {IS_SAMPLE}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create inference server with our predict function\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(\"COMPETITION MODE - serving...\")\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    print(\"LOCAL TEST MODE\")\n",
    "    inference_server.run_local_gateway((f'{COMP_PATH}/test.csv',))\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL: Solved {SOLVED_COUNT}/{PROBLEM_COUNT}\")\n",
    "    print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}