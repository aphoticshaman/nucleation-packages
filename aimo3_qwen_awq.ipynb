{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIMO3 Submission - Qwen2.5-72B-Instruct-AWQ\n",
    "# Quantized model that fits on H100 80GB\n",
    "# Uses vLLM with AWQ quantization\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# Free up memory\n",
    "for pkg in ['tensorflow', 'matplotlib', 'keras', 'sklearn', 'scikit-learn']:\n",
    "    try:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], \n",
    "                      capture_output=True, timeout=60)\n",
    "    except: pass\n",
    "gc.collect()\n",
    "\n",
    "# GPU info\n",
    "gpu_info = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], \n",
    "                          capture_output=True, text=True)\n",
    "print(f\"GPU: {gpu_info.stdout.strip()}\")\n",
    "print(f\"Budget: 280min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM wheels - skip packages that conflict with Kaggle\n",
    "WHEEL_DIR = Path('/kaggle/input/aimo3-vllm-wheels')\n",
    "\n",
    "# Packages to skip - Kaggle provides these and mixing versions breaks things\n",
    "SKIP_PACKAGES = {\n",
    "    # CRITICAL - PyTorch ecosystem must stay intact\n",
    "    'torch', 'xformers', 'triton',  # Cannot mix PyTorch versions\n",
    "    'transformers', 'tokenizers', 'numpy',\n",
    "    # RISKY - may cause import errors  \n",
    "    'nvidia', 'accelerate', 'huggingface', 'safetensors', 'pillow', 'protobuf',\n",
    "    'pyyaml', 'regex', 'packaging', 'tqdm', 'fsspec', 'certifi', 'charset',\n",
    "    'idna', 'urllib3', 'jinja2', 'markupsafe', 'sympy', 'mpmath', 'networkx',\n",
    "    'six', 'python-dateutil', 'attrs', 'referencing', 'jsonschema', 'rpds',\n",
    "    # FLASHINFER - not in our wheels anyway\n",
    "    'flashinfer',\n",
    "}\n",
    "\n",
    "def should_skip(wheel_name):\n",
    "    name_lower = wheel_name.lower()\n",
    "    for skip in SKIP_PACKAGES:\n",
    "        if skip.lower().replace('-', '_') in name_lower.replace('-', '_'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "if WHEEL_DIR.exists():\n",
    "    wheels = list(WHEEL_DIR.glob('*.whl'))\n",
    "    print(f\"\\n{WHEEL_DIR}:\")\n",
    "    print(f\"  Found {len(wheels)} wheels\")\n",
    "    for w in wheels[:10]:\n",
    "        print(f\"  {w.name}\")\n",
    "    if len(wheels) > 10:\n",
    "        print(f\"  ... and {len(wheels)-10} more\")\n",
    "    \n",
    "    print(f\"\\n--- Installing wheels (skipping Kaggle-provided packages) ---\")\n",
    "    installed, skipped = 0, 0\n",
    "    for wheel in sorted(wheels):\n",
    "        if should_skip(wheel.name):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, '-m', 'pip', 'install', '--no-deps', '--quiet', str(wheel)],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            installed += 1\n",
    "    print(f\"Installed {installed} wheels (skipped {skipped} conflicting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import vLLM and verify\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "print(f\"\\nAll imports ready\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path - Qwen2.5-72B-Instruct-AWQ\n",
    "MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/72b-instruct-awq/1\"\n",
    "\n",
    "# Verify model exists\n",
    "model_path = Path(MODEL_PATH)\n",
    "if model_path.exists():\n",
    "    files = list(model_path.glob('*'))\n",
    "    print(f\"Found Qwen2.5-72B-AWQ: {MODEL_PATH}\")\n",
    "    safetensors = [f for f in files if f.suffix == '.safetensors']\n",
    "    print(f\"  Safetensor shards: {len(safetensors)}\")\n",
    "    total_size = sum(f.stat().st_size for f in safetensors) / 1e9\n",
    "    print(f\"  Total size: {total_size:.1f} GB\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model not found at {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with AWQ quantization\n",
    "print(\"Loading Qwen2.5-72B-Instruct-AWQ...\")\n",
    "print(\"  Using AWQ INT4 quantization (~36GB VRAM)\")\n",
    "\n",
    "llm = LLM(\n",
    "    MODEL_PATH,\n",
    "    quantization=\"awq\",\n",
    "    dtype=\"float16\",\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.92,\n",
    "    max_model_len=4096,\n",
    "    max_num_seqs=16,\n",
    "    enforce_eager=True,  # More stable\n",
    "    enable_prefix_caching=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Max context: 4096 tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test problems\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "\n",
    "# Get sample problems for local testing\n",
    "try:\n",
    "    import pandas as pd\n",
    "    sample_df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/sample_submission.csv')\n",
    "    test_df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv')\n",
    "    print(f\"Test set: {len(test_df)} problems\")\n",
    "except:\n",
    "    print(\"Running in submission mode - problems provided by server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math problem solving configuration\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert mathematical problem solver specializing in competition mathematics.\n",
    "\n",
    "When solving problems:\n",
    "1. Read the problem carefully and identify what is being asked\n",
    "2. Break down the problem into manageable steps\n",
    "3. Show your work clearly with mathematical reasoning\n",
    "4. Double-check your calculations\n",
    "5. State your final answer clearly\n",
    "\n",
    "For numerical answers, provide ONLY the integer value as your final answer.\n",
    "If the answer should be a remainder or modular result, compute it explicitly.\"\"\"\n",
    "\n",
    "def format_prompt(problem: str) -> str:\n",
    "    \"\"\"Format problem for Qwen chat template.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Solve this problem step by step:\\n\\n{problem}\\n\\nShow your complete solution, then state your final numerical answer.\"}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Sampling parameters for math\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    max_tokens=2048,\n",
    "    stop=[\"<|endoftext|>\", \"<|im_end|>\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer(response: str) -> int:\n",
    "    \"\"\"Extract numerical answer from model response.\"\"\"\n",
    "    # Look for common answer patterns\n",
    "    patterns = [\n",
    "        r'(?:final answer|answer is|therefore|thus|hence)[:\\s]*(?:\\$)?\\s*(\\d+)',\n",
    "        r'(?:=|equals?)\\s*(\\d+)\\s*$',\n",
    "        r'\\\\boxed\\{(\\d+)\\}',\n",
    "        r'\\*\\*(\\d+)\\*\\*\\s*$',\n",
    "    ]\n",
    "    \n",
    "    response_lower = response.lower()\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, response_lower, re.IGNORECASE | re.MULTILINE)\n",
    "        if matches:\n",
    "            try:\n",
    "                return int(matches[-1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    # Fallback: find last number in response\n",
    "    numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "    if numbers:\n",
    "        return int(numbers[-1])\n",
    "    \n",
    "    return 0  # Default if no number found\n",
    "\n",
    "def solve_problem(problem: str, num_samples: int = 3) -> int:\n",
    "    \"\"\"Solve a math problem using majority voting.\"\"\"\n",
    "    prompt = format_prompt(problem)\n",
    "    \n",
    "    # Generate multiple solutions\n",
    "    outputs = llm.generate([prompt] * num_samples, sampling_params)\n",
    "    \n",
    "    answers = []\n",
    "    for output in outputs:\n",
    "        response = output.outputs[0].text\n",
    "        answer = extract_answer(response)\n",
    "        answers.append(answer)\n",
    "    \n",
    "    # Majority vote\n",
    "    from collections import Counter\n",
    "    if answers:\n",
    "        most_common = Counter(answers).most_common(1)[0][0]\n",
    "        return most_common\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample problem\n",
    "test_problem = \"\"\"Find the sum of all positive integers n such that n^2 + n + 1 divides n^4 + n^2 + 1.\"\"\"\n",
    "\n",
    "print(\"Testing with sample problem...\")\n",
    "print(f\"Problem: {test_problem[:100]}...\")\n",
    "\n",
    "prompt = format_prompt(test_problem)\n",
    "output = llm.generate([prompt], sampling_params)[0]\n",
    "response = output.outputs[0].text\n",
    "\n",
    "print(f\"\\nResponse preview: {response[:500]}...\")\n",
    "print(f\"\\nExtracted answer: {extract_answer(response)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main prediction function for Kaggle submission\n",
    "def predict(id_: str, question: str) -> int:\n",
    "    \"\"\"Main prediction function called by Kaggle evaluation server.\"\"\"\n",
    "    try:\n",
    "        answer = solve_problem(question, num_samples=3)\n",
    "        print(f\"Problem {id_}: answer = {answer}\")\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error on problem {id_}: {e}\")\n",
    "        return 0\n",
    "\n",
    "print(\"Prediction function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start inference server for Kaggle submission\n",
    "print(\"Starting Kaggle inference server...\")\n",
    "kaggle_evaluation.aimo_2_inference_server.serve(predict)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10851160,
     "sourceId": 87890,
     "sourceType": "competition"
    },
    {
     "sourceId": 215356942,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 291039,
     "modelInstanceId": 270825,
     "sourceId": 321405,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
