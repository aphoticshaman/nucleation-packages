{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5099c7ad",
   "metadata": {
    "papermill": {
     "duration": 0.001746,
     "end_time": "2025-11-10T20:13:42.000669",
     "exception": false,
     "start_time": "2025-11-10T20:13:41.998923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Thanks for the work [boristown](https://www.kaggle.com/code/boristown/agi-compressarc)\n",
    "\n",
    "### In this version, redundant parameters have been removed, the code structure has been changed and improved, and execution has been significantly accelerated. A more compact and faster pipeline allows for more iterations within 12 hours, which increases the likelihood of finding the right solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2195e33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T20:13:42.004314Z",
     "iopub.status.busy": "2025-11-10T20:13:42.004094Z",
     "iopub.status.idle": "2025-11-10T20:19:40.645426Z",
     "shell.execute_reply": "2025-11-10T20:19:40.644704Z"
    },
    "papermill": {
     "duration": 358.645054,
     "end_time": "2025-11-10T20:19:40.647220",
     "exception": false,
     "start_time": "2025-11-10T20:13:42.002166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import importlib\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "# Add external module path\n",
    "sys.path.append('/kaggle/input/arc-inp')\n",
    "\n",
    "# Dynamically import preprocessing module\n",
    "path = \"/kaggle/input/arc-inp/preprocessing.py\"\n",
    "preprocessing_spec = importlib.util.spec_from_file_location(\"preprocessing\", path)\n",
    "preprocessing = importlib.util.module_from_spec(preprocessing_spec)\n",
    "sys.modules[\"preprocessing\"] = preprocessing\n",
    "preprocessing_spec.loader.exec_module(preprocessing)\n",
    "\n",
    "# Import project modules\n",
    "import train\n",
    "import layers\n",
    "import solve_task\n",
    "import initializers\n",
    "import arc_compressor\n",
    "import solution_selection\n",
    "import multitensor_systems\n",
    "\n",
    "fake_mode = not os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n",
    "\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_device('cuda')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    end = start + 5*3600\n",
    "\n",
    "    n_cpus = multiprocessing.cpu_count()\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    \n",
    "    split = \"evaluation\" if fake_mode else \"test\"\n",
    "    with open(f'../input/arc-prize-2025/arc-agi_{split}_challenges.json', 'r') as f:\n",
    "        problems = json.load(f)\n",
    "    \n",
    "    task_names = list(problems.keys())\n",
    "    \n",
    "    del problems\n",
    "    \n",
    "    n_tasks = len(task_names)\n",
    "\n",
    "def parallelize_runs(gpu_quotas, task_usages, n_iterations):\n",
    "    gpu_quotas = gpu_quotas[:]\n",
    "    t = time.time()\n",
    "    \n",
    "    tasks_started = [False for i in range(n_tasks)]\n",
    "    tasks_finished = [False for i in range(n_tasks)]\n",
    "    processes = [None for i in range(n_tasks)]\n",
    "    process_gpu_ids = [None for i in range(n_tasks)]\n",
    "    \n",
    "    with multiprocessing.Manager() as manager:\n",
    "        memory_dict = manager.dict()\n",
    "        solutions_dict = manager.dict()\n",
    "        error_queue = manager.Queue()\n",
    "        while not all(tasks_finished):\n",
    "            if not error_queue.empty():\n",
    "                raise ValueError(error_queue.get())\n",
    "            for i in range(n_tasks):\n",
    "                if tasks_started[i] and not tasks_finished[i]:\n",
    "                    processes[i].join(timeout=0)\n",
    "                    if not processes[i].is_alive():\n",
    "                        tasks_finished[i] = True\n",
    "                        gpu_quotas[process_gpu_ids[i]] += task_usages[i]\n",
    "            \n",
    "            for gpu_id in range(n_gpus):\n",
    "                for i in range(n_tasks):\n",
    "                    enough_quota = gpu_quotas[gpu_id] > task_usages[i]\n",
    "                    enough_cpus = sum(map(int, tasks_started)) - sum(map(int, tasks_finished)) < n_cpus\n",
    "                    if not tasks_started[i] and enough_quota and enough_cpus:\n",
    "                        gpu_quotas[gpu_id] -= task_usages[i]\n",
    "                        args = (task_names[i], split, end, n_iterations, gpu_id, memory_dict, solutions_dict, error_queue)\n",
    "                        p = multiprocessing.Process(target=solve_task.solve_task, args=args)\n",
    "                        p.start()\n",
    "                        processes[i] = p\n",
    "                        tasks_started[i] = True\n",
    "                        process_gpu_ids[i] = gpu_id\n",
    "                        \n",
    "            # time.sleep(0.8) smaller -> faster\n",
    "        if not error_queue.empty():\n",
    "            raise ValueError(error_queue.get())\n",
    "        \n",
    "        memory_dict = dict(memory_dict)\n",
    "        solutions_dict = dict(solutions_dict)\n",
    "    \n",
    "    time_taken = time.time() - t\n",
    "    \n",
    "    return memory_dict, solutions_dict, time_taken\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gpu_memory_quotas = [torch.cuda.mem_get_info(i)[0] for i in range(n_gpus)]\n",
    "\n",
    "    gpu_task_quotas = [int(gpu_memory_quota // (4 * 1024**3)) for gpu_memory_quota in gpu_memory_quotas]\n",
    "    task_usages = [1 for i in range(n_tasks)]\n",
    "    memory_dict, _, _ = parallelize_runs(gpu_task_quotas, task_usages, 2)\n",
    "    \n",
    "    tasks = sorted(memory_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    task_names, task_memory_usages = zip(*tasks)\n",
    "    \n",
    "    test_steps = 5 if fake_mode else 20\n",
    "    mq = [memory_quota - 6 * 1024**3 for memory_quota in gpu_memory_quotas]\n",
    "    _, _, time_taken = parallelize_runs(mq, task_memory_usages, test_steps)\n",
    "    \n",
    "    time_per_step = time_taken / test_steps\n",
    "    time_left = end - time.time()\n",
    "    n_steps = 5 if fake_mode else int(time_left // time_per_step)\n",
    "    _, solutions_dict, time_taken = parallelize_runs(mq, task_memory_usages, n_steps)\n",
    "    \n",
    "    with open('submission.json', 'w') as f:\n",
    "        json.dump(solutions_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 8340309,
     "sourceId": 13523815,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 362.10529,
   "end_time": "2025-11-10T20:19:41.366485",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-10T20:13:39.261195",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
