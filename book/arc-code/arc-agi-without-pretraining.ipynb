{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a2c6bf",
   "metadata": {
    "papermill": {
     "duration": 0.003255,
     "end_time": "2025-04-09T01:58:17.067756",
     "exception": false,
     "start_time": "2025-04-09T01:58:17.064501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ARC-AGI Without Pretraining - Official Competition Template Version\n",
    "This file interfaces between the kaggle competition website and the rest of the solution code, which is included in the input files.\n",
    "\n",
    "The main differences between this notebook and the method in the ARC-AGI Without Pretraining blog post aim to parallelize the solving of many puzzles at once using all the CPUs and GPUs that are offered in this competition. In the blog post, we solved puzzles in series, vastly underutilized one RTX 4070 GPU, and blew past the time budget. Instead, what we do in this notebook is:\n",
    "- We run 2 steps of every puzzle to determine how much memory each puzzle uses.\n",
    "- We run 10 steps of every puzzle at optimal puzzle parallelization under memory constraint to determine how much time per step we need to solve the puzzles in bulk.\n",
    "- We run as many steps as we can at optimal puzzle parallelization under memory constraint to fit a 12 hour budget.\n",
    "- We have changed layers.direction_share() to make it run faster, and got something like a 5-10% speedup.\n",
    "\n",
    "If the dataset size is 120 puzzles, we should expect this to get ~2300 steps in per puzzle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b26df5",
   "metadata": {
    "papermill": {
     "duration": 0.002406,
     "end_time": "2025-04-09T01:58:17.073091",
     "exception": false,
     "start_time": "2025-04-09T01:58:17.070685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5434b45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T01:58:17.079123Z",
     "iopub.status.busy": "2025-04-09T01:58:17.078827Z",
     "iopub.status.idle": "2025-04-09T01:58:20.546831Z",
     "shell.execute_reply": "2025-04-09T01:58:20.546153Z"
    },
    "papermill": {
     "duration": 3.472762,
     "end_time": "2025-04-09T01:58:20.548390",
     "exception": false,
     "start_time": "2025-04-09T01:58:17.075628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import importlib\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append('/kaggle/input/compressarc')\n",
    "\n",
    "# This little block of code does \"import preprocessing\" but avoids a name collision with another module\n",
    "module_path = \"/kaggle/input/compressarc/preprocessing.py\"\n",
    "module_name = \"preprocessing\"\n",
    "spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
    "preprocessing = importlib.util.module_from_spec(spec)\n",
    "sys.modules[module_name] = preprocessing\n",
    "spec.loader.exec_module(preprocessing)\n",
    "import train\n",
    "import arc_compressor\n",
    "import initializers\n",
    "import multitensor_systems\n",
    "import layers\n",
    "import solution_selection\n",
    "import visualization\n",
    "import solve_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc057e",
   "metadata": {
    "papermill": {
     "duration": 0.002511,
     "end_time": "2025-04-09T01:58:20.553859",
     "exception": false,
     "start_time": "2025-04-09T01:58:20.551348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Getting all the task names, setting defaults and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090e3020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T01:58:20.559802Z",
     "iopub.status.busy": "2025-04-09T01:58:20.559479Z",
     "iopub.status.idle": "2025-04-09T01:58:20.716693Z",
     "shell.execute_reply": "2025-04-09T01:58:20.716023Z"
    },
    "papermill": {
     "duration": 0.161676,
     "end_time": "2025-04-09T01:58:20.718113",
     "exception": false,
     "start_time": "2025-04-09T01:58:20.556437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_device('cuda')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    start_time = time.time()\n",
    "    end_time = start_time + 12*3600 - 300\n",
    "\n",
    "    n_cpus = multiprocessing.cpu_count()\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Find all the puzzle names\n",
    "    split = \"test\"\n",
    "    with open(f'../input/arc-prize-2025/arc-agi_{split}_challenges.json', 'r') as f:\n",
    "        problems = json.load(f)\n",
    "    task_names = list(problems.keys())\n",
    "    del problems\n",
    "    n_tasks = len(task_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1760db3",
   "metadata": {
    "papermill": {
     "duration": 0.0024,
     "end_time": "2025-04-09T01:58:20.723439",
     "exception": false,
     "start_time": "2025-04-09T01:58:20.721039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Function that can spawn processes and schedule them on GPUs to take up each GPUs quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc01f51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T01:58:20.729312Z",
     "iopub.status.busy": "2025-04-09T01:58:20.729079Z",
     "iopub.status.idle": "2025-04-09T01:58:20.737075Z",
     "shell.execute_reply": "2025-04-09T01:58:20.736490Z"
    },
    "papermill": {
     "duration": 0.012017,
     "end_time": "2025-04-09T01:58:20.738059",
     "exception": false,
     "start_time": "2025-04-09T01:58:20.726042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallelize_runs(gpu_quotas, task_usages, n_iterations, verbose=False):\n",
    "    gpu_quotas = gpu_quotas[:]\n",
    "    # Schedule the tasks greedily to max out memory usage\n",
    "    t = time.time()\n",
    "    tasks_started = [False for i in range(n_tasks)]\n",
    "    tasks_finished = [False for i in range(n_tasks)]\n",
    "    processes = [None for i in range(n_tasks)]\n",
    "    process_gpu_ids = [None for i in range(n_tasks)]\n",
    "    with multiprocessing.Manager() as manager:\n",
    "        memory_dict = manager.dict()\n",
    "        solutions_dict = manager.dict()\n",
    "        error_queue = manager.Queue()\n",
    "        while not all(tasks_finished):\n",
    "            if not error_queue.empty():\n",
    "                raise ValueError(error_queue.get())\n",
    "            for i in range(n_tasks):\n",
    "                if tasks_started[i] and not tasks_finished[i]:\n",
    "                    processes[i].join(timeout=0)\n",
    "                    if not processes[i].is_alive():\n",
    "                        tasks_finished[i] = True\n",
    "                        gpu_quotas[process_gpu_ids[i]] += task_usages[i]\n",
    "                        if verbose:\n",
    "                            print(task_names[i], 'finished on gpu', process_gpu_ids[i],\n",
    "                                  'New quota is', gpu_quotas[process_gpu_ids[i]])\n",
    "            for gpu_id in range(n_gpus):\n",
    "                for i in range(n_tasks):\n",
    "                    enough_quota = gpu_quotas[gpu_id] > task_usages[i]\n",
    "                    enough_cpus = sum(map(int, tasks_started)) - sum(map(int, tasks_finished)) < n_cpus\n",
    "                    if not tasks_started[i] and enough_quota and enough_cpus:\n",
    "                        gpu_quotas[gpu_id] -= task_usages[i]\n",
    "                        args = (task_names[i], split, end_time, n_iterations, gpu_id, memory_dict, solutions_dict, error_queue)\n",
    "                        p = multiprocessing.Process(target=solve_task.solve_task, args=args)\n",
    "                        p.start()\n",
    "                        processes[i] = p\n",
    "                        tasks_started[i] = True\n",
    "                        process_gpu_ids[i] = gpu_id\n",
    "                        if verbose:\n",
    "                            print(task_names[i], 'started on gpu', process_gpu_ids[i],\n",
    "                                  'New quota is', gpu_quotas[process_gpu_ids[i]])\n",
    "            time.sleep(1)\n",
    "        if not error_queue.empty():\n",
    "            raise ValueError(error_queue.get())\n",
    "        memory_dict = dict(memory_dict)\n",
    "        solutions_dict = dict(solutions_dict)\n",
    "    time_taken = time.time() - t\n",
    "    if verbose:\n",
    "        print('All jobs finished in', time_taken, 'seconds.')\n",
    "    return memory_dict, solutions_dict, time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16e5df",
   "metadata": {
    "papermill": {
     "duration": 0.00235,
     "end_time": "2025-04-09T01:58:20.742901",
     "exception": false,
     "start_time": "2025-04-09T01:58:20.740551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Measuring the amount of memory used for every task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3115db96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T01:58:20.748530Z",
     "iopub.status.busy": "2025-04-09T01:58:20.748324Z",
     "iopub.status.idle": "2025-04-09T02:01:17.670154Z",
     "shell.execute_reply": "2025-04-09T02:01:17.669446Z"
    },
    "papermill": {
     "duration": 176.926344,
     "end_time": "2025-04-09T02:01:17.671765",
     "exception": false,
     "start_time": "2025-04-09T01:58:20.745421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    gpu_memory_quotas = [torch.cuda.mem_get_info(i)[0] for i in range(n_gpus)]\n",
    "\n",
    "    gpu_task_quotas = [int(gpu_memory_quota // (4 * 1024**3)) for gpu_memory_quota in gpu_memory_quotas]\n",
    "    task_usages = [1 for i in range(n_tasks)]\n",
    "    memory_dict, _, _ = parallelize_runs(gpu_task_quotas, task_usages, 2, verbose=False)\n",
    "    \n",
    "    # Sort the tasks by decreasing memory usage\n",
    "    tasks = sorted(memory_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    task_names, task_memory_usages = zip(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b1571",
   "metadata": {
    "papermill": {
     "duration": 0.002481,
     "end_time": "2025-04-09T02:01:17.677368",
     "exception": false,
     "start_time": "2025-04-09T02:01:17.674887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Computing the time taken, while saturating memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83928dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T02:01:17.683497Z",
     "iopub.status.busy": "2025-04-09T02:01:17.683223Z",
     "iopub.status.idle": "2025-04-09T02:04:31.926787Z",
     "shell.execute_reply": "2025-04-09T02:04:31.924835Z"
    },
    "papermill": {
     "duration": 194.262404,
     "end_time": "2025-04-09T02:04:31.942302",
     "exception": true,
     "start_time": "2025-04-09T02:01:17.679898",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/kaggle/input/compressarc/solve_task.py\", line 49, in solve_task\n    train.take_step(task, model, optimizer, train_step, train_history_logger)\n  File \"/kaggle/input/compressarc/train.py\", line 107, in take_step\n    loss.backward()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 572, in backward\n    return handle_torch_function(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/overrides.py\", line 1717, in handle_torch_function\n    result = mode.__torch_function__(public_api, types, args, kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 106, in __torch_function__\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.28 GiB of which 18.44 MiB is free. Process 4142 has 184.00 MiB memory in use. Process 9065 has 2.28 GiB memory in use. Process 9069 has 1.81 GiB memory in use. Process 10042 has 276.00 MiB memory in use. Process 10105 has 930.00 MiB memory in use. Process 10106 has 928.00 MiB memory in use. Process 10104 has 946.00 MiB memory in use. Process 10107 has 916.00 MiB memory in use. Process 10127 has 920.00 MiB memory in use. Process 10234 has 908.00 MiB memory in use. Process 10245 has 900.00 MiB memory in use. Process 10246 has 894.00 MiB memory in use. Process 10247 has 892.00 MiB memory in use. Process 10260 has 876.00 MiB memory in use. Process 10271 has 862.00 MiB memory in use. Process 10283 has 736.00 MiB memory in use. Process 10282 has 846.00 MiB memory in use. Process 10281 has 856.00 MiB memory in use. Process 10389 has 768.00 MiB memory in use. Process 10387 has 766.00 MiB memory in use. Process 10390 has 742.00 MiB memory in use. Process 10388 has 760.00 MiB memory in use. Process 10577 has 714.00 MiB memory in use. Process 10686 has 714.00 MiB memory in use. Process 11024 has 510.00 MiB memory in use. Process 11023 has 694.00 MiB memory in use. Of the allocated memory 266.87 MiB is allocated by PyTorch, and 11.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2f4e46558080>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtest_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msafe_gpu_memory_quotas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmemory_quota\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmemory_quota\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgpu_memory_quotas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallelize_runs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_gpu_memory_quotas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_memory_usages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-4ef21d5781ed>\u001b[0m in \u001b[0;36mparallelize_runs\u001b[0;34m(gpu_quotas, task_usages, n_iterations, verbose)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0merror_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtasks_started\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtasks_finished\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/kaggle/input/compressarc/solve_task.py\", line 49, in solve_task\n    train.take_step(task, model, optimizer, train_step, train_history_logger)\n  File \"/kaggle/input/compressarc/train.py\", line 107, in take_step\n    loss.backward()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 572, in backward\n    return handle_torch_function(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/overrides.py\", line 1717, in handle_torch_function\n    result = mode.__torch_function__(public_api, types, args, kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 106, in __torch_function__\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.28 GiB of which 18.44 MiB is free. Process 4142 has 184.00 MiB memory in use. Process 9065 has 2.28 GiB memory in use. Process 9069 has 1.81 GiB memory in use. Process 10042 has 276.00 MiB memory in use. Process 10105 has 930.00 MiB memory in use. Process 10106 has 928.00 MiB memory in use. Process 10104 has 946.00 MiB memory in use. Process 10107 has 916.00 MiB memory in use. Process 10127 has 920.00 MiB memory in use. Process 10234 has 908.00 MiB memory in use. Process 10245 has 900.00 MiB memory in use. Process 10246 has 894.00 MiB memory in use. Process 10247 has 892.00 MiB memory in use. Process 10260 has 876.00 MiB memory in use. Process 10271 has 862.00 MiB memory in use. Process 10283 has 736.00 MiB memory in use. Process 10282 has 846.00 MiB memory in use. Process 10281 has 856.00 MiB memory in use. Process 10389 has 768.00 MiB memory in use. Process 10387 has 766.00 MiB memory in use. Process 10390 has 742.00 MiB memory in use. Process 10388 has 760.00 MiB memory in use. Process 10577 has 714.00 MiB memory in use. Process 10686 has 714.00 MiB memory in use. Process 11024 has 510.00 MiB memory in use. Process 11023 has 694.00 MiB memory in use. Of the allocated memory 266.87 MiB is allocated by PyTorch, and 11.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_steps = 20\n",
    "    safe_gpu_memory_quotas = [memory_quota - 6 * 1024**3 for memory_quota in gpu_memory_quotas]\n",
    "    _, _, time_taken = parallelize_runs(safe_gpu_memory_quotas, task_memory_usages, test_steps, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1872682",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Computing the solution for every task, while saturating memory and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dd4b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T04:10:47.619864Z",
     "iopub.status.busy": "2025-03-30T04:10:47.619663Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    time_per_step = time_taken / test_steps\n",
    "    time_left = end_time - time.time()\n",
    "    n_steps = int(time_left // time_per_step)\n",
    "    _, solutions_dict, time_taken = parallelize_runs(safe_gpu_memory_quotas, task_memory_usages, n_steps, verbose=True)\n",
    "    \n",
    "    # Format the solutions and put into submission file\n",
    "    with open('submission.json', 'w') as f:\n",
    "        json.dump(solutions_dict, f, indent=4)\n",
    "        \n",
    "    print(n_tasks, 'tasks solved.')\n",
    "    print(n_steps, 'steps taken.')\n",
    "    print(time_taken, 'seconds taken.')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11483707,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 7001095,
     "sourceId": 11212096,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30920,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 381.144382,
   "end_time": "2025-04-09T02:04:35.483710",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-09T01:58:14.339328",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
