# LatticeForge Research References Library

**Rolling bibliography of peer-reviewed papers, preprints, and technical reports informing LatticeForge development.**

Last updated: 2025-12-08

---

## Human-AI Collaboration & Synergy

### Riedl, C. & Weidmann, B. (2025). Quantifying Human-AI Synergy.
*Under review (NeurIPS 2025)*
- **Key finding**: Theory of Mind predicts AI collaboration performance (ρs=0.17, p<0.001) but NOT solo performance (ρs=0.06, p=0.13)
- **Implication**: Collaborative ability (κ) is mathematically separable from individual ability (θ)
- **URL**: Preprint under double-blind review

### Weidmann, B. & Deming, D.J. (2021). Team players: How social skills improve team performance.
*Econometrica, 89(6), 2637-2657*
- **DOI**: https://doi.org/10.3982/ECTA18461
- **Key finding**: ToM predicts team performance; individual intelligence does not
- **Foundational** for human-AI synergy framework

### Woolley, A.W., Chabris, C.F., Pentland, A., Hashmi, N., & Malone, T.W. (2010). Evidence for a collective intelligence factor in the performance of human groups.
*Science, 330(6004), 686-688*
- **DOI**: https://doi.org/10.1126/science.1193147
- **Key finding**: Collective intelligence (c-factor) exists and is measurable
- **Seminal** paper establishing CI as a construct

### Riedl, C., Kim, Y.J., Gupta, P., Malone, T.W., & Woolley, A.W. (2021). Quantifying collective intelligence in human groups.
*PNAS, 118(21), e2005737118*
- **DOI**: https://doi.org/10.1073/pnas.2005737118
- **Key finding**: Bayesian framework for measuring collective intelligence
- **Methodological foundation** for IRT approach

### Westby, S. & Riedl, C. (2023). Collective intelligence in human-ai teams: A bayesian theory of mind approach.
*Proceedings of AAAI, 37(5), 6119-6127*
- **DOI**: https://doi.org/10.1609/aaai.v37i5.25765
- **Key finding**: Bayesian ToM models human-AI collaboration
- **Direct precursor** to Riedl & Weidmann (2025)

---

## LLM Theory of Mind & Benchmarking

### Chang, S., Anderson, A., & Hofman, J.M. (2025). Chatbench: From static benchmarks to human-ai evaluation.
*arXiv:2504.07114*
- **URL**: https://arxiv.org/abs/2504.07114
- **Key finding**: Human-AI benchmarks reveal synergy invisible to static benchmarks
- **Data source** for Riedl & Weidmann (2025)

### Prakash, N., Shapira, N., Sharma, A.S., Riedl, C., Belinkov, Y., Shaham, T.R., Bau, D., & Geiger, A. (2025). Language models use lookbacks to track beliefs.
*arXiv:2505.14685*
- **URL**: https://arxiv.org/abs/2505.14685
- **Key finding**: Mechanistic analysis of ToM circuits in LLMs
- **Implication**: ToM is not emergent magic—it's identifiable computation

### Kim, H., Sclar, M., Zhou, X., Le Bras, R., Kim, G., Choi, Y., & Sap, M. (2023). FANToM: A benchmark for stress-testing machine theory of mind in interactions.
*arXiv:2310.15421*
- **URL**: https://arxiv.org/abs/2310.15421
- **Key finding**: LLMs struggle with interactive ToM vs. static ToM tasks

### Shapira, N., Levy, M., Alavi, S.H., Zhou, X., Choi, Y., Goldberg, Y., Sap, M., & Shwartz, V. (2024). Clever hans or neural theory of mind? Stress testing social reasoning in large language models.
*EACL 2024*
- **URL**: https://aclanthology.org/2024.eacl-long.138
- **Key finding**: LLMs may be pattern-matching rather than reasoning about mental states

### Strachan, J.W.A., et al. (2024). Testing theory of mind in large language models and humans.
*Nature Human Behaviour, 8(7), 1285-1295*
- **DOI**: https://doi.org/10.1038/s41562-024-01882-z
- **Key finding**: GPT-4 matches human ToM performance on classic tasks

---

## AI Impact on Work & Productivity

### Dell'Acqua, F., et al. (2023). Navigating the jagged technological frontier: Field experimental evidence of the effects of AI on knowledge worker productivity and quality.
*Harvard Business School Working Paper 24-013*
- **URL**: https://www.hbs.edu/ris/Publication%20Files/24-013_d9b45b68-9e74-42d6-a1c6-c72fb70c7282.pdf
- **Key finding**: AI helps on some tasks, hurts on others (the "jagged frontier")
- **Implication**: Task-specific deployment matters

### Brynjolfsson, E., Li, D., & Raymond, L. (2025). Generative AI at work.
*Quarterly Journal of Economics, qjae044*
- **DOI**: https://doi.org/10.1093/qje/qjae044
- **Key finding**: AI most helps lower-performing workers
- **Equalizing effect** confirmed

### Noy, S. & Zhang, W. (2023). Experimental evidence on the productivity effects of generative artificial intelligence.
*Science, 381(6654), 187-192*
- **DOI**: https://doi.org/10.1126/science.adh2586
- **Key finding**: ChatGPT increases writing productivity 40%, reduces inequality

### Haupt, A. & Brynjolfsson, E. (2025). AI should not be an imitation game: Centaur evaluations.
- **URL**: https://www.andyhaupt.com/assets/papers/Centaur_Evaluations.pdf
- **Key finding**: AI benchmarks should measure human-AI teams, not AI alone

---

## Item Response Theory & Measurement

### Baker, F.B. & Kim, S.H. (2004). Item Response Theory: Parameter estimation techniques.
*CRC Press, Boca Raton, FL*
- **ISBN**: 978-0824758257
- **Foundational text** for IRT methodology

### Lalor, J.P., Wu, H., & Yu, H. (2016). Building an evaluation scale using item response theory.
*EMNLP 2016, 648-657*
- **DOI**: https://doi.org/10.18653/v1/D16-1062
- **Key finding**: First application of IRT to NLP evaluation
- **Methodological precedent** for AI benchmarking

### Gelman, A. & Hill, J. (2007). Data Analysis using Regression and Multilevel/Hierarchical Models.
*Cambridge University Press*
- **ISBN**: 978-0521686891
- **Standard reference** for hierarchical Bayesian modeling

---

## Cognitive Science Foundations

### Premack, D. & Woodruff, G. (1978). Does the chimpanzee have a theory of mind?
*Behavioral and Brain Sciences, 1(4), 515-526*
- **DOI**: https://doi.org/10.1017/S0140525X00076512
- **Origin** of Theory of Mind concept

### Frith, C.D. & Frith, U. (2006). The neural basis of mentalizing.
*Neuron, 50(4), 531-534*
- **DOI**: https://doi.org/10.1016/j.neuron.2006.05.001
- **Neuroscience** of ToM

### Clark, H.H. (1996). Using Language.
*Cambridge University Press*
- **ISBN**: 978-0521567459
- **Foundational** for common ground and grounding in communication

### Tomasello, M. (2010). Origins of Human Communication.
*MIT Press*
- **ISBN**: 978-0262515207
- **Key concept**: Shared intentionality underlies human collaboration

---

## Complex Systems & Risk

### *[Placeholder for CIC/SDPM references - add as formalized]*

---

## LLM Behavior & Alignment

### Perez, E., et al. (2023). Discovering language model behaviors with model-written evaluations.
*Findings of ACL 2023, 13387-13434*
- **URL**: https://aclanthology.org/2023.findings-acl.847
- **Key finding**: LLMs can evaluate themselves but with biases

### Bansal, G., et al. (2024). Challenges in human-agent communication.
*arXiv:2412.10380*
- **URL**: https://arxiv.org/abs/2412.10380
- **Key finding**: Sycophancy and communication breakdowns in human-AI interaction

### Shojaee, P., et al. (2025). The illusion of thinking: Understanding the strengths and limitations of reasoning models via the lens of problem complexity.
*Apple ML Research*
- **URL**: https://ml-site.cdn-apple.com/papers/the-illusion-of-thinking.pdf
- **Key finding**: LLMs may not be reasoning as deeply as benchmarks suggest

---

## Meta-References

### Eloundou, T., et al. (2024). First-person fairness in chatbots.
*arXiv:2410.19803*
- **URL**: https://arxiv.org/abs/2410.19803
- **Key finding**: LMRA (LLM as Research Assistant) methodology validation
- **Used in** Riedl & Weidmann (2025) for ToM assessment

### Li, D., et al. (2025). From generation to judgment: Opportunities and challenges of LLM-as-a-judge.
*EMNLP 2025, 2757-2791*
- **Methodology** for validating LLM-as-judge approaches

---

## How to Add References

When adding a new reference:
1. Include full citation in standard format
2. **DOI** if published (required if available)
3. **URL** if preprint/working paper (required if no DOI)
4. **Key finding**: 1-2 sentences on main contribution
5. **Implication/Application** for LatticeForge if relevant

Format:
```
### Author(s) (Year). Title.
*Journal/Venue, Volume(Issue), Pages*
- **DOI**: https://doi.org/...
- **URL**: https://... (if no DOI)
- **Key finding**: ...
- **Application**: ...
```

---

*This is a living document. Add references as they inform development.*
