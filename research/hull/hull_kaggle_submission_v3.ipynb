{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hull Tactical - PROMETHEUS v3 Complete\n",
    "\n",
    "**26 Novel Insights from 1,662 Research Files**\n",
    "\n",
    "Key innovations:\n",
    "- Anomalous Dimension (Δ) for crash detection - STRONGEST predictor\n",
    "- Dempster-Shafer belief fusion with conflict detection\n",
    "- Cascade phase classification (SIR model)\n",
    "- Attractor basin mapping with gravity gradients\n",
    "- Cross-domain correlation surge detection\n",
    "- CIC confidence functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport pickle\nimport warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.preprocessing import RobustScaler\nfrom scipy.signal import savgol_filter\nfrom scipy.ndimage import gaussian_filter1d\n\nwarnings.filterwarnings('ignore')\n\n# Paths\nif os.path.exists('/kaggle/input/hull-tactical-market-prediction'):\n    DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n    ARTIFACTS_DIR = Path('/kaggle/input/hull-artifacts-v3')\n    if not ARTIFACTS_DIR.exists():\n        ARTIFACTS_DIR = Path('/kaggle/working')\nelse:\n    DATA_DIR = Path('/home/user/aimo3/hull/hull-tactical-market-prediction')\n    ARTIFACTS_DIR = Path('/home/user/aimo3/hull/artifacts_final')\n\nprint(f\"Data: {DATA_DIR}\")\nprint(f\"Artifacts: {ARTIFACTS_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global state\n",
    "class InferenceState:\n",
    "    scaler = None\n",
    "    feature_cols = None\n",
    "    lgb_models = None\n",
    "    xgb_models = None\n",
    "    config = None\n",
    "    recent_data = None\n",
    "    history = []\n",
    "    initialized = False\n",
    "    # PROMETHEUS v3 state\n",
    "    delta_history = []  # Anomalous dimension history\n",
    "    cascade_state = {'phase': 'dormant', 'intensity': 0, 'velocity': 0}\n",
    "\n",
    "def initialize():\n",
    "    if InferenceState.initialized:\n",
    "        return\n",
    "    \n",
    "    print(\"Loading PROMETHEUS v3 artifacts...\")\n",
    "    \n",
    "    with open(ARTIFACTS_DIR / 'scaler.pkl', 'rb') as f:\n",
    "        InferenceState.scaler = pickle.load(f)\n",
    "    \n",
    "    with open(ARTIFACTS_DIR / 'feature_cols.pkl', 'rb') as f:\n",
    "        InferenceState.feature_cols = pickle.load(f)\n",
    "    \n",
    "    with open(ARTIFACTS_DIR / 'lgb_models.pkl', 'rb') as f:\n",
    "        InferenceState.lgb_models = pickle.load(f)\n",
    "    \n",
    "    InferenceState.xgb_models = []\n",
    "    i = 0\n",
    "    while (ARTIFACTS_DIR / f'xgb_model_{i}.json').exists():\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(str(ARTIFACTS_DIR / f'xgb_model_{i}.json'))\n",
    "        InferenceState.xgb_models.append(model)\n",
    "        i += 1\n",
    "    \n",
    "    with open(ARTIFACTS_DIR / 'config.pkl', 'rb') as f:\n",
    "        InferenceState.config = pickle.load(f)\n",
    "    \n",
    "    InferenceState.recent_data = pd.read_parquet(ARTIFACTS_DIR / 'recent_data.parquet')\n",
    "    \n",
    "    InferenceState.initialized = True\n",
    "    print(f\"Loaded {len(InferenceState.lgb_models)} LGB + {len(InferenceState.xgb_models)} XGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PROMETHEUS v3 CORE FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_autocorrelation(x, tau):\n",
    "    \"\"\"Autocorrelation at lag tau.\"\"\"\n",
    "    if tau >= len(x) or len(x) < 10:\n",
    "        return 0.0\n",
    "    x_centered = x - np.mean(x)\n",
    "    n = len(x) - tau\n",
    "    if n < 5:\n",
    "        return 0.0\n",
    "    numer = np.sum(x_centered[:n] * x_centered[tau:tau+n])\n",
    "    denom = np.sum(x_centered ** 2)\n",
    "    if denom < 1e-10:\n",
    "        return 0.0\n",
    "    return numer / denom\n",
    "\n",
    "\n",
    "def compute_anomalous_dimension(x, tau, K=1.0):\n",
    "    \"\"\"Δ(t,τ) = [K - ln|G(t,τ)|] / ln(τ) - STRONGEST crash predictor.\"\"\"\n",
    "    if tau <= 1 or len(x) < tau + 10:\n",
    "        return 0.0\n",
    "    G = compute_autocorrelation(x, tau)\n",
    "    if G <= 0:\n",
    "        G = 1e-10\n",
    "    return (K - np.log(abs(G))) / np.log(tau)\n",
    "\n",
    "\n",
    "def compute_multiscale_delta(x, tau_min=5, tau_max=30):\n",
    "    \"\"\"Average anomalous dimension across scales.\"\"\"\n",
    "    if len(x) < tau_max + 10:\n",
    "        return 0.0\n",
    "    taus = range(tau_min, min(tau_max, len(x) // 3))\n",
    "    deltas = [compute_anomalous_dimension(x, tau) for tau in taus]\n",
    "    deltas = [d for d in deltas if not np.isnan(d) and not np.isinf(d)]\n",
    "    return np.median(deltas) if deltas else 0.0\n",
    "\n",
    "\n",
    "def dempster_shafer_fusion(predictions, reliabilities=None):\n",
    "    \"\"\"Reliability-weighted belief fusion with conflict detection.\"\"\"\n",
    "    if reliabilities is None:\n",
    "        reliabilities = np.ones(len(predictions))\n",
    "    reliabilities = np.array(reliabilities)\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Normalize reliabilities\n",
    "    r_sum = reliabilities.sum()\n",
    "    if r_sum < 1e-10:\n",
    "        return np.mean(predictions), 0.0, 1.0\n",
    "    weights = reliabilities / r_sum\n",
    "    \n",
    "    # Fused prediction\n",
    "    fused = np.sum(predictions * weights)\n",
    "    \n",
    "    # Compute conflict (disagreement)\n",
    "    n = len(predictions)\n",
    "    conflict = 0.0\n",
    "    pairs = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            # Conflict = direction disagreement weighted by reliability\n",
    "            sign_disagree = float(np.sign(predictions[i]) != np.sign(predictions[j]))\n",
    "            conflict += reliabilities[i] * reliabilities[j] * sign_disagree\n",
    "            pairs += 1\n",
    "    if pairs > 0:\n",
    "        conflict /= pairs\n",
    "    \n",
    "    confidence = 1 - conflict\n",
    "    return fused, confidence, conflict\n",
    "\n",
    "\n",
    "def value_clustering(predictions, threshold=0.05):\n",
    "    \"\"\"Cluster predictions by relative proximity (88% error reduction).\"\"\"\n",
    "    predictions = list(predictions)\n",
    "    if len(predictions) < 2:\n",
    "        return predictions[0] if predictions else 0.0, 1.0\n",
    "    \n",
    "    def rel_dist(a, b):\n",
    "        return abs(a - b) / (max(abs(a), abs(b)) + 1e-8)\n",
    "    \n",
    "    clusters = []\n",
    "    for pred in predictions:\n",
    "        added = False\n",
    "        for cluster in clusters:\n",
    "            if rel_dist(pred, np.median(cluster)) < threshold:\n",
    "                cluster.append(pred)\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            clusters.append([pred])\n",
    "    \n",
    "    largest = max(clusters, key=len)\n",
    "    return np.median(largest), len(largest) / len(predictions)\n",
    "\n",
    "\n",
    "def compute_cic_functional(predictions, lambda_=0.3, gamma=0.1):\n",
    "    \"\"\"CIC confidence: F = Φ - λH + γC\"\"\"\n",
    "    predictions = np.array(predictions)\n",
    "    if len(predictions) < 2:\n",
    "        return 0.5\n",
    "    \n",
    "    mean_pred = np.mean(predictions)\n",
    "    std_pred = np.std(predictions)\n",
    "    \n",
    "    # Φ: Integrated information (coherence)\n",
    "    phi = 1 - std_pred / (abs(mean_pred) + 1e-8)\n",
    "    phi = np.clip(phi, 0, 1)\n",
    "    \n",
    "    # H: Entropy (uncertainty)\n",
    "    h = std_pred / (std_pred + 1)\n",
    "    \n",
    "    # C: Causal power (reliability)\n",
    "    cv = std_pred / (abs(mean_pred) + 1e-8)\n",
    "    c = 1 / (1 + cv)\n",
    "    \n",
    "    F = phi - lambda_ * h + gamma * c\n",
    "    return np.clip(F, 0, 1)\n",
    "\n",
    "\n",
    "def cascade_phase_classifier(intensity, velocity, acceleration):\n",
    "    \"\"\"Classify cascade phase using SIR dynamics.\"\"\"\n",
    "    if intensity < 0.1 and abs(velocity) < 0.05:\n",
    "        return 0, 'dormant'\n",
    "    elif intensity < 0.3 and velocity > 0.02 and acceleration > 0:\n",
    "        return 1, 'seeding'\n",
    "    elif 0.3 <= intensity < 0.7 and velocity > 0.05:\n",
    "        return 2, 'spreading'\n",
    "    elif intensity >= 0.7 or (intensity >= 0.5 and velocity < 0.02 and acceleration < 0):\n",
    "        return 3, 'peak'\n",
    "    elif intensity >= 0.2 and velocity < -0.02:\n",
    "        return 4, 'declining'\n",
    "    else:\n",
    "        return 5, 'exhausted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prometheus_v3_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add all 26 PROMETHEUS v3 features.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # === BASIC FEATURES ===\n",
    "    key_cols = ['V1', 'V2', 'V3', 'M1', 'M2', 'S1', 'S2', 'E1', 'P1', 'I1']\n",
    "    key_cols = [c for c in key_cols if c in df.columns]\n",
    "    \n",
    "    for col in key_cols:\n",
    "        for window in [5, 21, 63]:\n",
    "            if len(df) >= window:\n",
    "                df[f'{col}_ma{window}'] = df[col].rolling(window, min_periods=1).mean()\n",
    "                df[f'{col}_std{window}'] = df[col].rolling(window, min_periods=1).std().fillna(0)\n",
    "    \n",
    "    # Returns\n",
    "    if 'lagged_forward_returns' in df.columns:\n",
    "        df['lagged_ret'] = df['lagged_forward_returns']\n",
    "    elif 'forward_returns' in df.columns:\n",
    "        df['lagged_ret'] = df['forward_returns'].shift(1).fillna(0)\n",
    "    else:\n",
    "        df['lagged_ret'] = 0\n",
    "    \n",
    "    for w in [5, 10, 21]:\n",
    "        if len(df) >= w:\n",
    "            df[f'ret_cumsum_{w}'] = df['lagged_ret'].rolling(w, min_periods=1).sum()\n",
    "            df[f'ret_vol_{w}'] = df['lagged_ret'].rolling(w, min_periods=1).std().fillna(0)\n",
    "    \n",
    "    # === TIER 1: PHASE TRANSITION DETECTION ===\n",
    "    \n",
    "    # 1-4. Variance Compression\n",
    "    if len(df) >= 63:\n",
    "        df['var_21'] = df['lagged_ret'].rolling(21, min_periods=1).var().fillna(0)\n",
    "        df['var_63'] = df['lagged_ret'].rolling(63, min_periods=1).var().fillna(0)\n",
    "        df['var_ratio'] = df['var_21'] / (df['var_63'] + 1e-8)\n",
    "        df['var_compression'] = (df['var_ratio'] < 0.5).astype(int)\n",
    "    else:\n",
    "        df['var_21'] = df['var_63'] = 0\n",
    "        df['var_ratio'] = 1.0\n",
    "        df['var_compression'] = 0\n",
    "    \n",
    "    # 5-6. Anomalous Dimension Δ (STRONGEST predictor)\n",
    "    if len(df) >= 100:\n",
    "        deltas = []\n",
    "        ret_vals = df['lagged_ret'].values\n",
    "        for i in range(len(df)):\n",
    "            if i < 50:\n",
    "                deltas.append(0)\n",
    "            else:\n",
    "                window = ret_vals[max(0, i-100):i]\n",
    "                delta = compute_multiscale_delta(window)\n",
    "                deltas.append(delta)\n",
    "        df['anomalous_dimension'] = deltas\n",
    "        df['delta_trend'] = pd.Series(deltas).rolling(20, min_periods=1).apply(\n",
    "            lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0\n",
    "        ).values\n",
    "    else:\n",
    "        df['anomalous_dimension'] = 0\n",
    "        df['delta_trend'] = 0\n",
    "    \n",
    "    # 7-8. Critical Slowing Down (AC1)\n",
    "    if len(df) >= 63:\n",
    "        def calc_ac1(x):\n",
    "            if len(x) < 10:\n",
    "                return 0\n",
    "            try:\n",
    "                return np.corrcoef(x[:-1], x[1:])[0, 1]\n",
    "            except:\n",
    "                return 0\n",
    "        df['ac1'] = df['lagged_ret'].rolling(63, min_periods=10).apply(calc_ac1, raw=True).fillna(0)\n",
    "        df['ac1_rising'] = (df['ac1'] > df['ac1'].shift(5).fillna(0)).astype(int)\n",
    "    else:\n",
    "        df['ac1'] = df['ac1_rising'] = 0\n",
    "    \n",
    "    # === TIER 2: MARKET TEMPERATURE & COHERENCE ===\n",
    "    v_cols = [c for c in df.columns if c.startswith('V') and c[1:].isdigit()]\n",
    "    m_cols = [c for c in df.columns if c.startswith('M') and c[1:].isdigit()]\n",
    "    s_cols = [c for c in df.columns if c.startswith('S') and c[1:].isdigit()]\n",
    "    e_cols = [c for c in df.columns if c.startswith('E') and c[1:].isdigit()]\n",
    "    i_cols = [c for c in df.columns if c.startswith('I') and c[1:].isdigit()]\n",
    "    \n",
    "    # 9. Temperature\n",
    "    if v_cols:\n",
    "        df['v_mean'] = df[v_cols].mean(axis=1)\n",
    "        df['v_std'] = df[v_cols].std(axis=1)\n",
    "        df['temperature'] = df['v_std'] / (df['v_mean'].abs() + 1e-8)\n",
    "    else:\n",
    "        df['v_mean'] = df['v_std'] = df['temperature'] = 0\n",
    "    \n",
    "    # 10-12. Order Parameters\n",
    "    for prefix, cols in [('V', v_cols), ('M', m_cols), ('S', s_cols)]:\n",
    "        if len(cols) >= 2:\n",
    "            zscores = (df[cols] - df[cols].mean()) / (df[cols].std() + 1e-8)\n",
    "            df[f'{prefix}_order'] = 1 - zscores.var(axis=1).fillna(1)\n",
    "        else:\n",
    "            df[f'{prefix}_order'] = 0\n",
    "    \n",
    "    # 13. Volatility Regime\n",
    "    if v_cols:\n",
    "        df['vol_regime'] = df[v_cols].mean(axis=1)\n",
    "        df['vol_regime_ma21'] = df['vol_regime'].rolling(21, min_periods=1).mean()\n",
    "        df['vol_expanding'] = (df['vol_regime'] > df['vol_regime_ma21']).astype(int)\n",
    "    else:\n",
    "        df['vol_regime'] = df['vol_regime_ma21'] = df['vol_expanding'] = 0\n",
    "    \n",
    "    # === TIER 3: FEATURE INTERACTIONS ===\n",
    "    \n",
    "    # 14-15. Sentiment-Vol Interaction\n",
    "    if s_cols:\n",
    "        df['sent_mean'] = df[s_cols].mean(axis=1)\n",
    "        df['sent_vol_interact'] = df['sent_mean'] / (df['vol_regime'] + 1e-8)\n",
    "    else:\n",
    "        df['sent_mean'] = df['sent_vol_interact'] = 0\n",
    "    \n",
    "    # 16. Momentum Regime\n",
    "    if 'ret_cumsum_21' in df.columns:\n",
    "        df['cum_ret_std'] = df['ret_cumsum_21'].rolling(63, min_periods=1).std().fillna(0)\n",
    "        df['momentum_strong'] = (df['ret_cumsum_21'].abs() > df['cum_ret_std']).astype(int)\n",
    "    else:\n",
    "        df['cum_ret_std'] = df['momentum_strong'] = 0\n",
    "    \n",
    "    # 17. Economic Surprise\n",
    "    if e_cols:\n",
    "        df['econ_mean'] = df[e_cols].mean(axis=1)\n",
    "        df['econ_momentum'] = df['econ_mean'].diff(5).fillna(0)\n",
    "        df['econ_surprise'] = df['econ_momentum'] - df['econ_momentum'].rolling(63, min_periods=1).mean()\n",
    "    else:\n",
    "        df['econ_mean'] = df['econ_momentum'] = df['econ_surprise'] = 0\n",
    "    \n",
    "    # 18. Interest Rate Regime\n",
    "    if len(i_cols) >= 3 and 'I3' in df.columns and 'I1' in df.columns:\n",
    "        df['rate_slope'] = df['I3'] - df['I1']\n",
    "        df['rate_slope_pct'] = df['rate_slope'].rolling(63, min_periods=1).rank(pct=True).fillna(0.5)\n",
    "        df['rate_inverting'] = (df['rate_slope_pct'] < 0.1).astype(int)\n",
    "    else:\n",
    "        df['rate_slope'] = 0\n",
    "        df['rate_slope_pct'] = 0.5\n",
    "        df['rate_inverting'] = 0\n",
    "    \n",
    "    # === TIER 4: CROSS-DOMAIN ANALYSIS ===\n",
    "    \n",
    "    # 19. Cross-Domain Correlation Surge\n",
    "    if v_cols and m_cols and s_cols and len(df) >= 21:\n",
    "        vm = df[v_cols].mean(axis=1)\n",
    "        mm = df[m_cols].mean(axis=1)\n",
    "        sm = df[s_cols].mean(axis=1)\n",
    "        \n",
    "        corr_vm = vm.rolling(21).corr(mm).fillna(0)\n",
    "        corr_vs = vm.rolling(21).corr(sm).fillna(0)\n",
    "        corr_ms = mm.rolling(21).corr(sm).fillna(0)\n",
    "        \n",
    "        avg_corr = (abs(corr_vm) + abs(corr_vs) + abs(corr_ms)) / 3\n",
    "        df['cross_domain_corr'] = avg_corr\n",
    "        df['correlation_surge'] = (avg_corr > 0.7).astype(int)\n",
    "    else:\n",
    "        df['cross_domain_corr'] = 0\n",
    "        df['correlation_surge'] = 0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"PROMETHEUS v3 prediction with 26 insights.\"\"\"\n",
    "    if not InferenceState.initialized:\n",
    "        initialize()\n",
    "    \n",
    "    test_pd = test.to_pandas()\n",
    "    \n",
    "    # Update history\n",
    "    InferenceState.history.append(test_pd)\n",
    "    if len(InferenceState.history) > 300:\n",
    "        InferenceState.history = InferenceState.history[-300:]\n",
    "    \n",
    "    # Combine with historical data\n",
    "    if len(InferenceState.history) < 63:\n",
    "        n_needed = 300 - len(InferenceState.history)\n",
    "        combined = pd.concat(\n",
    "            [InferenceState.recent_data.tail(n_needed)] + InferenceState.history,\n",
    "            ignore_index=True\n",
    "        )\n",
    "    else:\n",
    "        combined = pd.concat(InferenceState.history, ignore_index=True)\n",
    "    \n",
    "    # Add PROMETHEUS v3 features\n",
    "    combined = add_prometheus_v3_features(combined)\n",
    "    \n",
    "    # Ensure all feature columns exist\n",
    "    for col in InferenceState.feature_cols:\n",
    "        if col not in combined.columns:\n",
    "            combined[col] = 0\n",
    "    \n",
    "    # Get features\n",
    "    X = combined[InferenceState.feature_cols].iloc[[-1]].fillna(0)\n",
    "    X_scaled = pd.DataFrame(\n",
    "        InferenceState.scaler.transform(X),\n",
    "        columns=InferenceState.feature_cols\n",
    "    )\n",
    "    \n",
    "    # Ensemble predictions\n",
    "    predictions = []\n",
    "    reliabilities = []\n",
    "    \n",
    "    for model in InferenceState.lgb_models:\n",
    "        pred = model.predict(X_scaled)[0]\n",
    "        predictions.append(pred)\n",
    "        reliabilities.append(0.9)  # LGB slightly more reliable\n",
    "    \n",
    "    dtest = xgb.DMatrix(X_scaled)\n",
    "    for model in InferenceState.xgb_models:\n",
    "        pred = model.predict(dtest)[0]\n",
    "        predictions.append(pred)\n",
    "        reliabilities.append(0.85)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    reliabilities = np.array(reliabilities)\n",
    "    \n",
    "    # === PROMETHEUS v3 FUSION ===\n",
    "    \n",
    "    # 1. Dempster-Shafer fusion\n",
    "    ds_pred, ds_confidence, ds_conflict = dempster_shafer_fusion(predictions, reliabilities)\n",
    "    \n",
    "    # 2. Value clustering\n",
    "    cluster_pred, cluster_conf = value_clustering(predictions)\n",
    "    \n",
    "    # 3. CIC functional\n",
    "    cic_F = compute_cic_functional(predictions)\n",
    "    \n",
    "    # Combined prediction (weighted by confidence)\n",
    "    mean_pred = 0.5 * ds_pred + 0.5 * cluster_pred\n",
    "    \n",
    "    # Combined confidence\n",
    "    confidence = 0.4 * ds_confidence + 0.4 * cluster_conf + 0.2 * cic_F\n",
    "    \n",
    "    # === RISK ADJUSTMENTS FROM PROMETHEUS SIGNALS ===\n",
    "    risk_factor = 1.0\n",
    "    \n",
    "    # Get current signals from features\n",
    "    current = combined.iloc[-1]\n",
    "    \n",
    "    # Variance compression → reduce position\n",
    "    if current.get('var_compression', 0) == 1:\n",
    "        risk_factor *= 0.7\n",
    "    \n",
    "    # Rising delta trend → crash risk → reduce aggressively\n",
    "    delta_trend = current.get('delta_trend', 0)\n",
    "    if delta_trend > 0.1:\n",
    "        risk_factor *= 0.5\n",
    "    elif delta_trend > 0.05:\n",
    "        risk_factor *= 0.7\n",
    "    \n",
    "    # Critical slowing down\n",
    "    if current.get('ac1_rising', 0) == 1:\n",
    "        risk_factor *= 0.8\n",
    "    \n",
    "    # Cross-domain correlation surge\n",
    "    if current.get('correlation_surge', 0) == 1:\n",
    "        risk_factor *= 0.6\n",
    "    \n",
    "    # Volatility expanding\n",
    "    if current.get('vol_expanding', 0) == 1:\n",
    "        risk_factor *= 0.85\n",
    "    \n",
    "    # High conflict → reduce position\n",
    "    if ds_conflict > 0.5:\n",
    "        risk_factor *= 0.7\n",
    "    \n",
    "    # === POSITION SIZING ===\n",
    "    cfg = InferenceState.config\n",
    "    std_pred = np.std(predictions)\n",
    "    uncertainty = max(std_pred, 1e-5)\n",
    "    \n",
    "    # Kelly with confidence and risk adjustment\n",
    "    kelly = mean_pred / (cfg['risk_aversion'] * uncertainty**2 + 1e-8)\n",
    "    position = cfg['base_position'] + cfg['scale_factor'] * kelly * confidence * risk_factor\n",
    "    position = np.clip(position, cfg['min_position'], cfg['max_position'])\n",
    "    \n",
    "    return float(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Only import kaggle_evaluation during competition rerun\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    import kaggle_evaluation.default_inference_server\n    inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n    print(\"PROMETHEUS v3 inference server ready.\")\nelse:\n    print(\"Skipping inference server setup (not in competition rerun)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    # Local validation: just test the predict function works\n    print(\"Local validation mode...\")\n    initialize()\n    print(f\"Models loaded: {len(InferenceState.lgb_models)} LGB + {len(InferenceState.xgb_models)} XGB\")\n    print(f\"Features: {len(InferenceState.feature_cols)}\")\n    print(f\"Config: {InferenceState.config}\")\n    print(\"\\nPROMETHEUS v3 validation passed!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}