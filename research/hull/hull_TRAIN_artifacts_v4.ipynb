{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Hull Tactical - Training Notebook v4\n",
    "\n",
    "**CRITICAL**: This notebook actually TRAINS models. Previous versions just loaded frozen artifacts.\n",
    "\n",
    "Changes from v3:\n",
    "- Position sizing: More aggressive (scale_factor 120 vs 80, bounds [0.0, 2.0] vs [0.2, 1.8])\n",
    "- Risk aversion: Lower (35 vs 50) - take bigger bets when confident\n",
    "- Feature engineering: PROMETHEUS features included in training\n",
    "- Model hyperparameters: Deeper trees, more iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths\n",
    "if os.path.exists('/kaggle/input/hull-tactical-market-prediction'):\n",
    "    DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "    ARTIFACTS_DIR = Path('/kaggle/working/artifacts_v4')\n",
    "else:\n",
    "    DATA_DIR = Path('/home/user/aimo3/hull/hull-tactical-market-prediction')\n",
    "    ARTIFACTS_DIR = Path('/home/user/aimo3/hull/artifacts_v4')\n",
    "\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Artifacts: {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# V4 CONFIG - MORE AGGRESSIVE THAN V3\n",
    "# ============================================================================\n",
    "CONFIG = {\n",
    "    # Position sizing - MORE AGGRESSIVE\n",
    "    'base_position': 1.0,\n",
    "    'risk_aversion': 35.0,      # Was 50 - lower = bigger bets\n",
    "    'scale_factor': 120.0,      # Was 80 - higher = more responsive\n",
    "    'min_position': 0.0,        # Was 0.2 - allow going flat\n",
    "    'max_position': 2.0,        # Was 1.8 - allow more leverage\n",
    "    \n",
    "    # Model params - DEEPER TREES\n",
    "    'lgb_params': {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 63,       # Was 31\n",
    "        'max_depth': 8,         # Added constraint\n",
    "        'learning_rate': 0.02,  # Was 0.01\n",
    "        'feature_fraction': 0.7,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 5,\n",
    "        'min_child_samples': 50,\n",
    "        'lambda_l1': 0.1,\n",
    "        'lambda_l2': 0.1,\n",
    "        'verbose': -1,\n",
    "        'seed': 42,\n",
    "        'n_jobs': -1\n",
    "    },\n",
    "    'xgb_params': {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.02,  # Was 0.01\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'min_child_weight': 50,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'seed': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    },\n",
    "    'n_models': 5,\n",
    "    'num_boost_round': 1500,    # Was 1000\n",
    "    'early_stopping': 100       # Was 50\n",
    "}\n",
    "\n",
    "print(\"V4 Config loaded:\")\n",
    "print(f\"  Position range: [{CONFIG['min_position']}, {CONFIG['max_position']}]\")\n",
    "print(f\"  Risk aversion: {CONFIG['risk_aversion']}\")\n",
    "print(f\"  Scale factor: {CONFIG['scale_factor']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(DATA_DIR / 'train.csv')\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Date range: {train_df['date_id'].min()} - {train_df['date_id'].max()}\")\n",
    "print(f\"\\nTarget (market_forward_excess_returns):\")\n",
    "print(train_df['market_forward_excess_returns'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PROMETHEUS FEATURE ENGINEERING - NOW USED IN TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def compute_autocorrelation(x, tau):\n",
    "    \"\"\"Autocorrelation at lag tau.\"\"\"\n",
    "    if tau >= len(x) or len(x) < 10:\n",
    "        return 0.0\n",
    "    x_centered = x - np.mean(x)\n",
    "    n = len(x) - tau\n",
    "    if n < 5:\n",
    "        return 0.0\n",
    "    numer = np.sum(x_centered[:n] * x_centered[tau:tau+n])\n",
    "    denom = np.sum(x_centered ** 2)\n",
    "    if denom < 1e-10:\n",
    "        return 0.0\n",
    "    return numer / denom\n",
    "\n",
    "\n",
    "def add_prometheus_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add PROMETHEUS features - these will be TRAINED into models.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # === BASIC ROLLING FEATURES ===\n",
    "    key_cols = ['V1', 'V2', 'V3', 'M1', 'M2', 'S1', 'S2', 'E1', 'P1', 'I1']\n",
    "    key_cols = [c for c in key_cols if c in df.columns]\n",
    "    \n",
    "    for col in key_cols:\n",
    "        for window in [5, 21, 63]:\n",
    "            if len(df) >= window:\n",
    "                df[f'{col}_ma{window}'] = df[col].rolling(window, min_periods=1).mean()\n",
    "                df[f'{col}_std{window}'] = df[col].rolling(window, min_periods=1).std().fillna(0)\n",
    "    \n",
    "    # === LAGGED RETURNS ===\n",
    "    if 'lagged_forward_returns' in df.columns:\n",
    "        df['lagged_ret'] = df['lagged_forward_returns']\n",
    "    elif 'forward_returns' in df.columns:\n",
    "        df['lagged_ret'] = df['forward_returns'].shift(1).fillna(0)\n",
    "    else:\n",
    "        df['lagged_ret'] = 0\n",
    "    \n",
    "    for w in [5, 10, 21, 63]:\n",
    "        if len(df) >= w:\n",
    "            df[f'ret_cumsum_{w}'] = df['lagged_ret'].rolling(w, min_periods=1).sum()\n",
    "            df[f'ret_vol_{w}'] = df['lagged_ret'].rolling(w, min_periods=1).std().fillna(0)\n",
    "            # Sharpe-like ratio\n",
    "            df[f'sharpe_{w}'] = df[f'ret_cumsum_{w}'] / (df[f'ret_vol_{w}'] * np.sqrt(w) + 1e-8)\n",
    "    \n",
    "    # === TIER 1: PHASE TRANSITION DETECTION ===\n",
    "    \n",
    "    # 1. Variance Compression\n",
    "    if len(df) >= 63:\n",
    "        df['var_21'] = df['lagged_ret'].rolling(21, min_periods=5).var().fillna(0)\n",
    "        df['var_63'] = df['lagged_ret'].rolling(63, min_periods=10).var().fillna(0)\n",
    "        df['var_ratio'] = df['var_21'] / (df['var_63'] + 1e-8)\n",
    "        df['var_compression'] = (df['var_ratio'] < 0.5).astype(float)\n",
    "    else:\n",
    "        df['var_21'] = df['var_63'] = df['var_ratio'] = 0\n",
    "        df['var_compression'] = 0\n",
    "    \n",
    "    # 2. Critical Slowing Down (AC1)\n",
    "    if len(df) >= 63:\n",
    "        def calc_ac1(x):\n",
    "            if len(x) < 10:\n",
    "                return 0\n",
    "            try:\n",
    "                return np.corrcoef(x[:-1], x[1:])[0, 1]\n",
    "            except:\n",
    "                return 0\n",
    "        df['ac1'] = df['lagged_ret'].rolling(63, min_periods=10).apply(calc_ac1, raw=True).fillna(0)\n",
    "        df['ac1_ma21'] = df['ac1'].rolling(21, min_periods=1).mean()\n",
    "        df['ac1_rising'] = (df['ac1'] > df['ac1_ma21']).astype(float)\n",
    "    else:\n",
    "        df['ac1'] = df['ac1_ma21'] = df['ac1_rising'] = 0\n",
    "    \n",
    "    # === TIER 2: MARKET TEMPERATURE & COHERENCE ===\n",
    "    v_cols = [c for c in df.columns if c.startswith('V') and c[1:].isdigit()]\n",
    "    m_cols = [c for c in df.columns if c.startswith('M') and c[1:].isdigit()]\n",
    "    s_cols = [c for c in df.columns if c.startswith('S') and c[1:].isdigit()]\n",
    "    e_cols = [c for c in df.columns if c.startswith('E') and c[1:].isdigit()]\n",
    "    i_cols = [c for c in df.columns if c.startswith('I') and c[1:].isdigit()]\n",
    "    \n",
    "    # Temperature\n",
    "    if v_cols:\n",
    "        df['v_mean'] = df[v_cols].mean(axis=1)\n",
    "        df['v_std'] = df[v_cols].std(axis=1)\n",
    "        df['temperature'] = df['v_std'] / (df['v_mean'].abs() + 1e-8)\n",
    "    else:\n",
    "        df['v_mean'] = df['v_std'] = df['temperature'] = 0\n",
    "    \n",
    "    # Order Parameters\n",
    "    for prefix, cols in [('V', v_cols), ('M', m_cols), ('S', s_cols)]:\n",
    "        if len(cols) >= 2:\n",
    "            zscores = (df[cols] - df[cols].mean()) / (df[cols].std() + 1e-8)\n",
    "            df[f'{prefix}_order'] = 1 - zscores.var(axis=1).fillna(1)\n",
    "        else:\n",
    "            df[f'{prefix}_order'] = 0\n",
    "    \n",
    "    # Volatility Regime\n",
    "    if v_cols:\n",
    "        df['vol_regime'] = df[v_cols].mean(axis=1)\n",
    "        df['vol_regime_ma21'] = df['vol_regime'].rolling(21, min_periods=1).mean()\n",
    "        df['vol_expanding'] = (df['vol_regime'] > df['vol_regime_ma21']).astype(float)\n",
    "    else:\n",
    "        df['vol_regime'] = df['vol_regime_ma21'] = df['vol_expanding'] = 0\n",
    "    \n",
    "    # === TIER 3: FEATURE INTERACTIONS ===\n",
    "    \n",
    "    # Sentiment mean\n",
    "    if s_cols:\n",
    "        df['sent_mean'] = df[s_cols].mean(axis=1)\n",
    "        df['sent_vol_interact'] = df['sent_mean'] / (df['vol_regime'] + 1e-8)\n",
    "    else:\n",
    "        df['sent_mean'] = df['sent_vol_interact'] = 0\n",
    "    \n",
    "    # Momentum strength\n",
    "    if 'ret_cumsum_21' in df.columns:\n",
    "        df['cum_ret_std'] = df['ret_cumsum_21'].rolling(63, min_periods=1).std().fillna(0)\n",
    "        df['momentum_strong'] = (df['ret_cumsum_21'].abs() > df['cum_ret_std']).astype(float)\n",
    "    else:\n",
    "        df['cum_ret_std'] = df['momentum_strong'] = 0\n",
    "    \n",
    "    # Economic surprise\n",
    "    if e_cols:\n",
    "        df['econ_mean'] = df[e_cols].mean(axis=1)\n",
    "        df['econ_momentum'] = df['econ_mean'].diff(5).fillna(0)\n",
    "        df['econ_surprise'] = df['econ_momentum'] - df['econ_momentum'].rolling(63, min_periods=1).mean()\n",
    "    else:\n",
    "        df['econ_mean'] = df['econ_momentum'] = df['econ_surprise'] = 0\n",
    "    \n",
    "    # Interest rate regime\n",
    "    if len(i_cols) >= 3 and 'I3' in df.columns and 'I1' in df.columns:\n",
    "        df['rate_slope'] = df['I3'] - df['I1']\n",
    "        df['rate_slope_pct'] = df['rate_slope'].rolling(63, min_periods=1).rank(pct=True).fillna(0.5)\n",
    "        df['rate_inverting'] = (df['rate_slope_pct'] < 0.1).astype(float)\n",
    "    else:\n",
    "        df['rate_slope'] = 0\n",
    "        df['rate_slope_pct'] = 0.5\n",
    "        df['rate_inverting'] = 0\n",
    "    \n",
    "    # === TIER 4: CROSS-DOMAIN CORRELATION ===\n",
    "    if v_cols and m_cols and s_cols and len(df) >= 21:\n",
    "        vm = df[v_cols].mean(axis=1)\n",
    "        mm = df[m_cols].mean(axis=1)\n",
    "        sm = df[s_cols].mean(axis=1)\n",
    "        \n",
    "        corr_vm = vm.rolling(21, min_periods=5).corr(mm).fillna(0)\n",
    "        corr_vs = vm.rolling(21, min_periods=5).corr(sm).fillna(0)\n",
    "        corr_ms = mm.rolling(21, min_periods=5).corr(sm).fillna(0)\n",
    "        \n",
    "        avg_corr = (abs(corr_vm) + abs(corr_vs) + abs(corr_ms)) / 3\n",
    "        df['cross_domain_corr'] = avg_corr\n",
    "        df['correlation_surge'] = (avg_corr > 0.7).astype(float)\n",
    "    else:\n",
    "        df['cross_domain_corr'] = 0\n",
    "        df['correlation_surge'] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"PROMETHEUS feature engineering defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "print(\"Applying feature engineering...\")\n",
    "df = train_df.copy()\n",
    "df = df.sort_values('date_id').reset_index(drop=True)\n",
    "\n",
    "# Apply PROMETHEUS features\n",
    "df = add_prometheus_features(df)\n",
    "\n",
    "# Get feature columns (exclude target and metadata)\n",
    "exclude_cols = ['date_id', 'forward_returns', 'risk_free_rate', \n",
    "                'market_forward_excess_returns', 'is_scored',\n",
    "                'lagged_forward_returns', 'lagged_risk_free_rate',\n",
    "                'lagged_market_forward_excess_returns', 'lagged_ret']\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols \n",
    "                and df[c].dtype in ['float64', 'int64', 'float32', 'int32']]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"\\nNew PROMETHEUS features:\")\n",
    "prometheus_feats = [c for c in feature_cols if any(x in c for x in \n",
    "    ['var_', 'ac1', 'temperature', 'order', 'vol_regime', 'sent_', \n",
    "     'momentum_', 'econ_', 'rate_', 'cross_domain', 'correlation_', 'sharpe_'])]\n",
    "print(f\"  Count: {len(prometheus_feats)}\")\n",
    "print(f\"  Examples: {prometheus_feats[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final data\n",
    "df_valid = df[df['market_forward_excess_returns'].notna()].copy()\n",
    "print(f\"Valid samples: {len(df_valid)}\")\n",
    "\n",
    "# Fill NaN in features\n",
    "X = df_valid[feature_cols].fillna(0)\n",
    "y = df_valid['market_forward_excess_returns']\n",
    "\n",
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=feature_cols, index=X.index)\n",
    "\n",
    "print(f\"X shape: {X_scaled.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN MODELS WITH TIME SERIES CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "# Use last 20% for final validation, train on first 80%\n",
    "val_size = int(len(X_scaled) * 0.2)\n",
    "X_train = X_scaled.iloc[:-val_size]\n",
    "y_train = y.iloc[:-val_size]\n",
    "X_val = X_scaled.iloc[-val_size:]\n",
    "y_val = y.iloc[-val_size:]\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}\")\n",
    "\n",
    "# Train multiple models with different seeds\n",
    "lgb_models = []\n",
    "xgb_models = []\n",
    "\n",
    "for seed in range(CONFIG['n_models']):\n",
    "    print(f\"\\nTraining model {seed+1}/{CONFIG['n_models']}...\")\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_params = CONFIG['lgb_params'].copy()\n",
    "    lgb_params['seed'] = 42 + seed\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    lgb_model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round=CONFIG['num_boost_round'],\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(CONFIG['early_stopping'], verbose=False)]\n",
    "    )\n",
    "    lgb_models.append(lgb_model)\n",
    "    print(f\"  LGB best iter: {lgb_model.best_iteration}\")\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_params = CONFIG['xgb_params'].copy()\n",
    "    xgb_params['seed'] = 42 + seed\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    xgb_model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=CONFIG['num_boost_round'],\n",
    "        evals=[(dval, 'val')],\n",
    "        early_stopping_rounds=CONFIG['early_stopping'],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    xgb_models.append(xgb_model)\n",
    "    print(f\"  XGB best iter: {xgb_model.best_iteration}\")\n",
    "\n",
    "print(f\"\\nTrained {len(lgb_models)} LGB + {len(xgb_models)} XGB models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate ensemble on held-out data\n",
    "print(\"Validating ensemble...\")\n",
    "\n",
    "predictions = []\n",
    "for model in lgb_models:\n",
    "    pred = model.predict(X_val)\n",
    "    predictions.append(pred)\n",
    "\n",
    "dval = xgb.DMatrix(X_val)\n",
    "for model in xgb_models:\n",
    "    pred = model.predict(dval)\n",
    "    predictions.append(pred)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "mean_pred = predictions.mean(axis=0)\n",
    "std_pred = predictions.std(axis=0)\n",
    "\n",
    "# Position sizing\n",
    "cfg = CONFIG\n",
    "positions = []\n",
    "for i in range(len(mean_pred)):\n",
    "    uncertainty = max(std_pred[i], 1e-5)\n",
    "    kelly = mean_pred[i] / (cfg['risk_aversion'] * uncertainty**2 + 1e-8)\n",
    "    pos = cfg['base_position'] + cfg['scale_factor'] * kelly\n",
    "    pos = np.clip(pos, cfg['min_position'], cfg['max_position'])\n",
    "    positions.append(pos)\n",
    "positions = np.array(positions)\n",
    "\n",
    "# Calculate metrics\n",
    "strategy_returns = positions * y_val.values\n",
    "market_returns = y_val.values\n",
    "\n",
    "strategy_mean = strategy_returns.mean() * 252\n",
    "strategy_vol = strategy_returns.std() * np.sqrt(252)\n",
    "market_vol = market_returns.std() * np.sqrt(252)\n",
    "sharpe = strategy_mean / (strategy_vol + 1e-8)\n",
    "\n",
    "print(f\"\\n=== VALIDATION METRICS ===\")\n",
    "print(f\"Strategy Annual Return: {strategy_mean:.4f}\")\n",
    "print(f\"Strategy Annual Vol: {strategy_vol:.4f}\")\n",
    "print(f\"Market Annual Vol: {market_vol:.4f}\")\n",
    "print(f\"Sharpe Ratio: {sharpe:.4f}\")\n",
    "print(f\"Position Mean: {positions.mean():.3f}\")\n",
    "print(f\"Position Std: {positions.std():.3f}\")\n",
    "print(f\"Position Min/Max: {positions.min():.3f} / {positions.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE ARTIFACTS\n",
    "# ============================================================================\n",
    "print(f\"\\nSaving artifacts to {ARTIFACTS_DIR}...\")\n",
    "\n",
    "# Save scaler\n",
    "with open(ARTIFACTS_DIR / 'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save feature columns\n",
    "with open(ARTIFACTS_DIR / 'feature_cols.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_cols, f)\n",
    "\n",
    "# Save LGB models\n",
    "with open(ARTIFACTS_DIR / 'lgb_models.pkl', 'wb') as f:\n",
    "    pickle.dump(lgb_models, f)\n",
    "\n",
    "# Save XGB models\n",
    "for i, model in enumerate(xgb_models):\n",
    "    model.save_model(str(ARTIFACTS_DIR / f'xgb_model_{i}.json'))\n",
    "\n",
    "# Save config with position sizing params\n",
    "config_to_save = {\n",
    "    'base_position': cfg['base_position'],\n",
    "    'risk_aversion': cfg['risk_aversion'],\n",
    "    'scale_factor': cfg['scale_factor'],\n",
    "    'min_position': cfg['min_position'],\n",
    "    'max_position': cfg['max_position']\n",
    "}\n",
    "with open(ARTIFACTS_DIR / 'config.pkl', 'wb') as f:\n",
    "    pickle.dump(config_to_save, f)\n",
    "\n",
    "# Save recent data for inference\n",
    "recent_data = df_valid.tail(300).copy()\n",
    "recent_data.to_parquet(ARTIFACTS_DIR / 'recent_data.parquet')\n",
    "\n",
    "print(f\"\\nArtifacts saved:\")\n",
    "for f in sorted(ARTIFACTS_DIR.glob('*')):\n",
    "    print(f\"  {f.name}: {f.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify config changes\n",
    "print(\"\\n=== V4 CONFIG COMPARISON ===\")\n",
    "print(\"\\nOld (v3) config:\")\n",
    "print(\"  base_position: 1.0\")\n",
    "print(\"  risk_aversion: 50.0\")\n",
    "print(\"  scale_factor: 80.0\")\n",
    "print(\"  min_position: 0.2\")\n",
    "print(\"  max_position: 1.8\")\n",
    "\n",
    "print(\"\\nNew (v4) config:\")\n",
    "for k, v in config_to_save.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n=== FEATURE COMPARISON ===\")\n",
    "print(f\"Old feature count: ~187\")\n",
    "print(f\"New feature count: {len(feature_cols)}\")\n",
    "\n",
    "print(\"\\nTraining complete! Use artifacts_v4 in your submission notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
