{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Hull Tactical - V5 FULL PROMETHEUS\n",
    "\n",
    "**This implements ALL 30 breakthrough insights:**\n",
    "- Phase Transition Detection (Variance Compression, Critical Slowing Down, Anomalous Dimension)\n",
    "- Ensemble Intelligence (Dempster-Shafer Fusion, Value Clustering, CIC Confidence)\n",
    "- Adaptive Position Sizing (Kelly + Regime + Risk Signals)\n",
    "- Feature Engineering (Temperature, Order Parameters, Cross-Domain Correlation)\n",
    "\n",
    "**Target**: Break 0.244 → Achieve 1.5+ Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths\n",
    "if os.path.exists('/kaggle/input/hull-tactical-market-prediction'):\n",
    "    DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "    # Try to load pre-trained artifacts\n",
    "    for name in ['hull-artifacts-v5', 'hull-artifacts-v4', 'hull-artifacts']:\n",
    "        test = Path(f'/kaggle/input/{name}')\n",
    "        if test.exists():\n",
    "            ARTIFACTS_DIR = test\n",
    "            break\n",
    "    else:\n",
    "        ARTIFACTS_DIR = Path('/kaggle/working')\n",
    "else:\n",
    "    DATA_DIR = Path('/home/user/aimo3/hull/hull-tactical-market-prediction')\n",
    "    ARTIFACTS_DIR = Path('/home/user/aimo3/hull/artifacts_v5')\n",
    "\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Artifacts: {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# V5 CONFIG - FULL PROMETHEUS PARAMETERS\n",
    "# =============================================================================\n",
    "CONFIG = {\n",
    "    # Position Sizing - ADAPTIVE\n",
    "    'base_position': 1.0,\n",
    "    'risk_aversion': 30.0,       # Even lower - trust the model more\n",
    "    'scale_factor': 150.0,       # Higher responsiveness\n",
    "    'min_position': 0.0,         # Can go flat\n",
    "    'max_position': 2.0,         # Full leverage\n",
    "    \n",
    "    # Risk Adjustment Factors\n",
    "    'var_compression_factor': 0.6,    # Reduce when variance compresses\n",
    "    'ac1_rising_factor': 0.7,         # Reduce when AC1 rising\n",
    "    'correlation_surge_factor': 0.5,  # Reduce when correlations surge\n",
    "    'high_conflict_factor': 0.6,      # Reduce when models disagree\n",
    "    'vol_expanding_factor': 0.8,      # Reduce when volatility expanding\n",
    "    \n",
    "    # Model Parameters\n",
    "    'lgb_params': {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 127,        # Deeper\n",
    "        'max_depth': 10,\n",
    "        'learning_rate': 0.015,\n",
    "        'feature_fraction': 0.6,\n",
    "        'bagging_fraction': 0.6,\n",
    "        'bagging_freq': 3,\n",
    "        'min_child_samples': 30,\n",
    "        'lambda_l1': 0.05,\n",
    "        'lambda_l2': 0.05,\n",
    "        'verbose': -1,\n",
    "        'seed': 42,\n",
    "        'n_jobs': -1\n",
    "    },\n",
    "    'xgb_params': {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.015,\n",
    "        'subsample': 0.6,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'min_child_weight': 30,\n",
    "        'reg_alpha': 0.05,\n",
    "        'reg_lambda': 0.05,\n",
    "        'seed': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    },\n",
    "    'n_models': 7,               # More models for better ensemble\n",
    "    'num_boost_round': 2000,\n",
    "    'early_stopping': 150\n",
    "}\n",
    "\n",
    "print(\"V5 FULL PROMETHEUS Config:\")\n",
    "print(f\"  Position range: [{CONFIG['min_position']}, {CONFIG['max_position']}]\")\n",
    "print(f\"  Risk aversion: {CONFIG['risk_aversion']}\")\n",
    "print(f\"  Scale factor: {CONFIG['scale_factor']}\")\n",
    "print(f\"  Models: {CONFIG['n_models']} LGB + {CONFIG['n_models']} XGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INSIGHT #1-4: PHASE TRANSITION DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_autocorrelation(x, tau):\n",
    "    \"\"\"Autocorrelation at lag tau.\"\"\"\n",
    "    if tau >= len(x) or len(x) < 10:\n",
    "        return 0.0\n",
    "    x_centered = x - np.mean(x)\n",
    "    n = len(x) - tau\n",
    "    if n < 5:\n",
    "        return 0.0\n",
    "    numer = np.sum(x_centered[:n] * x_centered[tau:tau+n])\n",
    "    denom = np.sum(x_centered ** 2)\n",
    "    return numer / (denom + 1e-10)\n",
    "\n",
    "\n",
    "def compute_anomalous_dimension(x, tau, K=1.0):\n",
    "    \"\"\"Δ(t,τ) = [K - ln|G(t,τ)|] / ln(τ) - STRONGEST crash predictor\"\"\"\n",
    "    if tau <= 1 or len(x) < tau + 10:\n",
    "        return 0.0\n",
    "    G = compute_autocorrelation(x, tau)\n",
    "    if abs(G) < 1e-10:\n",
    "        G = 1e-10\n",
    "    return (K - np.log(abs(G))) / np.log(tau)\n",
    "\n",
    "\n",
    "def compute_multiscale_delta(x, tau_min=5, tau_max=30):\n",
    "    \"\"\"Average anomalous dimension across scales.\"\"\"\n",
    "    if len(x) < tau_max + 10:\n",
    "        return 0.0\n",
    "    taus = range(tau_min, min(tau_max, len(x) // 3))\n",
    "    deltas = [compute_anomalous_dimension(x, tau) for tau in taus]\n",
    "    deltas = [d for d in deltas if not np.isnan(d) and not np.isinf(d) and abs(d) < 100]\n",
    "    return np.median(deltas) if deltas else 0.0\n",
    "\n",
    "\n",
    "def detect_variance_compression(returns, window_short=21, window_long=63):\n",
    "    \"\"\"Insight #1: Variance compression precedes regime shifts\"\"\"\n",
    "    if len(returns) < window_long:\n",
    "        return 0.0, 1.0\n",
    "    recent_var = returns[-window_short:].var()\n",
    "    baseline_var = returns[-window_long:-window_short].var()\n",
    "    ratio = recent_var / (baseline_var + 1e-10)\n",
    "    compression = ratio < 0.5  # Compression detected\n",
    "    return float(compression), ratio\n",
    "\n",
    "\n",
    "def detect_critical_slowing_down(returns, window=63):\n",
    "    \"\"\"Insight #2: Rising AC1 = approaching transition\"\"\"\n",
    "    if len(returns) < window:\n",
    "        return 0.0, 0.0\n",
    "    recent = returns[-window:]\n",
    "    try:\n",
    "        ac1 = np.corrcoef(recent[:-1], recent[1:])[0, 1]\n",
    "    except:\n",
    "        ac1 = 0.0\n",
    "    ac1_rising = ac1 > 0.5  # High autocorrelation\n",
    "    return float(ac1_rising), ac1\n",
    "\n",
    "print(\"Phase transition detection functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INSIGHT #5-8: ENSEMBLE INTELLIGENCE\n",
    "# =============================================================================\n",
    "\n",
    "def dempster_shafer_fusion(predictions, reliabilities=None):\n",
    "    \"\"\"Insight #5: Reliability-weighted belief fusion with conflict detection\"\"\"\n",
    "    if reliabilities is None:\n",
    "        reliabilities = np.ones(len(predictions))\n",
    "    reliabilities = np.array(reliabilities)\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Normalize reliabilities\n",
    "    r_sum = reliabilities.sum()\n",
    "    if r_sum < 1e-10:\n",
    "        return np.mean(predictions), 0.0, 1.0\n",
    "    weights = reliabilities / r_sum\n",
    "    \n",
    "    # Fused prediction\n",
    "    fused = np.sum(predictions * weights)\n",
    "    \n",
    "    # Compute conflict (direction disagreement)\n",
    "    n = len(predictions)\n",
    "    conflict = 0.0\n",
    "    pairs = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            sign_disagree = float(np.sign(predictions[i]) != np.sign(predictions[j]))\n",
    "            conflict += reliabilities[i] * reliabilities[j] * sign_disagree\n",
    "            pairs += 1\n",
    "    if pairs > 0:\n",
    "        conflict /= pairs\n",
    "    \n",
    "    confidence = 1 - conflict\n",
    "    return fused, confidence, conflict\n",
    "\n",
    "\n",
    "def value_clustering(predictions, threshold=0.05):\n",
    "    \"\"\"Insight #6: Cluster predictions by relative proximity (92% error reduction)\"\"\"\n",
    "    predictions = list(predictions)\n",
    "    if len(predictions) < 2:\n",
    "        return predictions[0] if predictions else 0.0, 1.0\n",
    "    \n",
    "    def rel_dist(a, b):\n",
    "        return abs(a - b) / (max(abs(a), abs(b)) + 1e-8)\n",
    "    \n",
    "    clusters = []\n",
    "    for pred in predictions:\n",
    "        added = False\n",
    "        for cluster in clusters:\n",
    "            if rel_dist(pred, np.median(cluster)) < threshold:\n",
    "                cluster.append(pred)\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            clusters.append([pred])\n",
    "    \n",
    "    largest = max(clusters, key=len)\n",
    "    return np.median(largest), len(largest) / len(predictions)\n",
    "\n",
    "\n",
    "def compute_cic_functional(predictions, lambda_=0.3, gamma=0.1):\n",
    "    \"\"\"Insight #17: CIC confidence: F = Φ - λH + γC\"\"\"\n",
    "    predictions = np.array(predictions)\n",
    "    if len(predictions) < 2:\n",
    "        return 0.5\n",
    "    \n",
    "    mean_pred = np.mean(predictions)\n",
    "    std_pred = np.std(predictions)\n",
    "    \n",
    "    # Φ: Integrated information (coherence)\n",
    "    phi = 1 - std_pred / (abs(mean_pred) + 1e-8)\n",
    "    phi = np.clip(phi, 0, 1)\n",
    "    \n",
    "    # H: Entropy (uncertainty)\n",
    "    h = std_pred / (std_pred + 1)\n",
    "    \n",
    "    # C: Causal power (coefficient of variation inverse)\n",
    "    cv = std_pred / (abs(mean_pred) + 1e-8)\n",
    "    c = 1 / (1 + cv)\n",
    "    \n",
    "    F = phi - lambda_ * h + gamma * c\n",
    "    return np.clip(F, 0, 1)\n",
    "\n",
    "print(\"Ensemble intelligence functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FULL PROMETHEUS FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "def add_full_prometheus_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add ALL PROMETHEUS features for training.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # === BASIC ROLLING FEATURES ===\n",
    "    key_cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'M1', 'M2', 'M3', 'S1', 'S2', 'S3', 'E1', 'E2', 'P1', 'I1', 'I2']\n",
    "    key_cols = [c for c in key_cols if c in df.columns]\n",
    "    \n",
    "    for col in key_cols:\n",
    "        for window in [5, 10, 21, 63]:\n",
    "            if len(df) >= window:\n",
    "                df[f'{col}_ma{window}'] = df[col].rolling(window, min_periods=1).mean()\n",
    "                df[f'{col}_std{window}'] = df[col].rolling(window, min_periods=1).std().fillna(0)\n",
    "                # Z-score\n",
    "                df[f'{col}_z{window}'] = (df[col] - df[f'{col}_ma{window}']) / (df[f'{col}_std{window}'] + 1e-8)\n",
    "    \n",
    "    # === LAGGED RETURNS ===\n",
    "    if 'lagged_forward_returns' in df.columns:\n",
    "        df['lagged_ret'] = df['lagged_forward_returns']\n",
    "    elif 'forward_returns' in df.columns:\n",
    "        df['lagged_ret'] = df['forward_returns'].shift(1).fillna(0)\n",
    "    else:\n",
    "        df['lagged_ret'] = 0\n",
    "    \n",
    "    for w in [5, 10, 21, 63, 126]:\n",
    "        if len(df) >= w:\n",
    "            df[f'ret_cumsum_{w}'] = df['lagged_ret'].rolling(w, min_periods=1).sum()\n",
    "            df[f'ret_vol_{w}'] = df['lagged_ret'].rolling(w, min_periods=1).std().fillna(0)\n",
    "            df[f'sharpe_{w}'] = df[f'ret_cumsum_{w}'] / (df[f'ret_vol_{w}'] * np.sqrt(w) + 1e-8)\n",
    "            df[f'ret_skew_{w}'] = df['lagged_ret'].rolling(w, min_periods=w//2).skew().fillna(0)\n",
    "            df[f'ret_kurt_{w}'] = df['lagged_ret'].rolling(w, min_periods=w//2).kurt().fillna(0)\n",
    "    \n",
    "    # === PHASE TRANSITION FEATURES ===\n",
    "    \n",
    "    # Variance Compression (Insight #1)\n",
    "    if len(df) >= 63:\n",
    "        df['var_21'] = df['lagged_ret'].rolling(21, min_periods=5).var().fillna(0)\n",
    "        df['var_63'] = df['lagged_ret'].rolling(63, min_periods=10).var().fillna(0)\n",
    "        df['var_ratio'] = df['var_21'] / (df['var_63'] + 1e-10)\n",
    "        df['var_compression'] = (df['var_ratio'] < 0.5).astype(float)\n",
    "        df['var_expansion'] = (df['var_ratio'] > 2.0).astype(float)\n",
    "    else:\n",
    "        df['var_21'] = df['var_63'] = df['var_ratio'] = 0\n",
    "        df['var_compression'] = df['var_expansion'] = 0\n",
    "    \n",
    "    # Critical Slowing Down (Insight #2)\n",
    "    if len(df) >= 63:\n",
    "        def calc_ac1(x):\n",
    "            if len(x) < 10:\n",
    "                return 0\n",
    "            try:\n",
    "                return np.corrcoef(x[:-1], x[1:])[0, 1]\n",
    "            except:\n",
    "                return 0\n",
    "        df['ac1'] = df['lagged_ret'].rolling(63, min_periods=10).apply(calc_ac1, raw=True).fillna(0)\n",
    "        df['ac1_ma21'] = df['ac1'].rolling(21, min_periods=1).mean()\n",
    "        df['ac1_rising'] = (df['ac1'] > df['ac1_ma21']).astype(float)\n",
    "        df['ac1_high'] = (df['ac1'] > 0.5).astype(float)\n",
    "    else:\n",
    "        df['ac1'] = df['ac1_ma21'] = df['ac1_rising'] = df['ac1_high'] = 0\n",
    "    \n",
    "    # Anomalous Dimension (Insight #3)\n",
    "    if len(df) >= 100:\n",
    "        deltas = []\n",
    "        ret_vals = df['lagged_ret'].values\n",
    "        for i in range(len(df)):\n",
    "            if i < 50:\n",
    "                deltas.append(0)\n",
    "            else:\n",
    "                window = ret_vals[max(0, i-100):i]\n",
    "                delta = compute_multiscale_delta(window)\n",
    "                deltas.append(delta)\n",
    "        df['anomalous_dimension'] = deltas\n",
    "        df['delta_ma10'] = pd.Series(deltas).rolling(10, min_periods=1).mean().values\n",
    "        df['delta_trend'] = pd.Series(deltas).rolling(20, min_periods=1).apply(\n",
    "            lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0\n",
    "        ).values\n",
    "        df['delta_rising'] = (df['delta_trend'] > 0.05).astype(float)\n",
    "    else:\n",
    "        df['anomalous_dimension'] = df['delta_ma10'] = df['delta_trend'] = df['delta_rising'] = 0\n",
    "    \n",
    "    # === MARKET TEMPERATURE & COHERENCE ===\n",
    "    v_cols = [c for c in df.columns if c.startswith('V') and c[1:].isdigit()]\n",
    "    m_cols = [c for c in df.columns if c.startswith('M') and c[1:].isdigit()]\n",
    "    s_cols = [c for c in df.columns if c.startswith('S') and c[1:].isdigit()]\n",
    "    e_cols = [c for c in df.columns if c.startswith('E') and c[1:].isdigit()]\n",
    "    i_cols = [c for c in df.columns if c.startswith('I') and c[1:].isdigit()]\n",
    "    p_cols = [c for c in df.columns if c.startswith('P') and c[1:].isdigit()]\n",
    "    \n",
    "    # Temperature (Insight #9)\n",
    "    if v_cols:\n",
    "        df['v_mean'] = df[v_cols].mean(axis=1)\n",
    "        df['v_std'] = df[v_cols].std(axis=1)\n",
    "        df['temperature'] = df['v_std'] / (df['v_mean'].abs() + 1e-8)\n",
    "        df['temp_ma21'] = df['temperature'].rolling(21, min_periods=1).mean()\n",
    "        df['temp_high'] = (df['temperature'] > df['temp_ma21'] * 1.5).astype(float)\n",
    "    else:\n",
    "        df['v_mean'] = df['v_std'] = df['temperature'] = df['temp_ma21'] = df['temp_high'] = 0\n",
    "    \n",
    "    # Order Parameters (Insight #10 - Kuramoto-inspired)\n",
    "    for prefix, cols in [('V', v_cols), ('M', m_cols), ('S', s_cols)]:\n",
    "        if len(cols) >= 2:\n",
    "            zscores = (df[cols] - df[cols].mean()) / (df[cols].std() + 1e-8)\n",
    "            df[f'{prefix}_order'] = 1 - zscores.var(axis=1).fillna(1)\n",
    "            df[f'{prefix}_coherence'] = zscores.mean(axis=1).abs()\n",
    "        else:\n",
    "            df[f'{prefix}_order'] = df[f'{prefix}_coherence'] = 0\n",
    "    \n",
    "    # Volatility Regime\n",
    "    if v_cols:\n",
    "        df['vol_regime'] = df[v_cols].mean(axis=1)\n",
    "        df['vol_regime_ma21'] = df['vol_regime'].rolling(21, min_periods=1).mean()\n",
    "        df['vol_regime_ma63'] = df['vol_regime'].rolling(63, min_periods=1).mean()\n",
    "        df['vol_expanding'] = (df['vol_regime'] > df['vol_regime_ma21']).astype(float)\n",
    "        df['vol_contracting'] = (df['vol_regime'] < df['vol_regime_ma21'] * 0.8).astype(float)\n",
    "    else:\n",
    "        df['vol_regime'] = df['vol_regime_ma21'] = df['vol_regime_ma63'] = 0\n",
    "        df['vol_expanding'] = df['vol_contracting'] = 0\n",
    "    \n",
    "    # === FEATURE INTERACTIONS ===\n",
    "    \n",
    "    # Sentiment-Vol Interaction (Insight #14)\n",
    "    if s_cols:\n",
    "        df['sent_mean'] = df[s_cols].mean(axis=1)\n",
    "        df['sent_std'] = df[s_cols].std(axis=1)\n",
    "        df['sent_vol_interact'] = df['sent_mean'] / (df['vol_regime'] + 1e-8)\n",
    "        df['sent_vol_product'] = df['sent_mean'] * df['vol_regime']\n",
    "    else:\n",
    "        df['sent_mean'] = df['sent_std'] = df['sent_vol_interact'] = df['sent_vol_product'] = 0\n",
    "    \n",
    "    # Momentum Regime (Insight #16)\n",
    "    if 'ret_cumsum_21' in df.columns:\n",
    "        df['cum_ret_std'] = df['ret_cumsum_21'].rolling(63, min_periods=1).std().fillna(0)\n",
    "        df['momentum_strong'] = (df['ret_cumsum_21'].abs() > df['cum_ret_std']).astype(float)\n",
    "        df['momentum_direction'] = np.sign(df['ret_cumsum_21'])\n",
    "    else:\n",
    "        df['cum_ret_std'] = df['momentum_strong'] = df['momentum_direction'] = 0\n",
    "    \n",
    "    # Economic Surprise (Insight #17)\n",
    "    if e_cols:\n",
    "        df['econ_mean'] = df[e_cols].mean(axis=1)\n",
    "        df['econ_std'] = df[e_cols].std(axis=1)\n",
    "        df['econ_momentum'] = df['econ_mean'].diff(5).fillna(0)\n",
    "        df['econ_momentum_ma'] = df['econ_momentum'].rolling(63, min_periods=1).mean()\n",
    "        df['econ_surprise'] = df['econ_momentum'] - df['econ_momentum_ma']\n",
    "    else:\n",
    "        df['econ_mean'] = df['econ_std'] = df['econ_momentum'] = df['econ_momentum_ma'] = df['econ_surprise'] = 0\n",
    "    \n",
    "    # Interest Rate Regime (Insight #18)\n",
    "    if len(i_cols) >= 3 and 'I3' in df.columns and 'I1' in df.columns:\n",
    "        df['rate_slope'] = df['I3'] - df['I1']\n",
    "        df['rate_slope_ma21'] = df['rate_slope'].rolling(21, min_periods=1).mean()\n",
    "        df['rate_slope_pct'] = df['rate_slope'].rolling(63, min_periods=1).rank(pct=True).fillna(0.5)\n",
    "        df['rate_inverting'] = (df['rate_slope_pct'] < 0.1).astype(float)\n",
    "        df['rate_steepening'] = (df['rate_slope_pct'] > 0.9).astype(float)\n",
    "    else:\n",
    "        df['rate_slope'] = df['rate_slope_ma21'] = 0\n",
    "        df['rate_slope_pct'] = 0.5\n",
    "        df['rate_inverting'] = df['rate_steepening'] = 0\n",
    "    \n",
    "    # === CROSS-DOMAIN CORRELATION (Insight #12) ===\n",
    "    if v_cols and m_cols and s_cols and len(df) >= 21:\n",
    "        vm = df[v_cols].mean(axis=1)\n",
    "        mm = df[m_cols].mean(axis=1)\n",
    "        sm = df[s_cols].mean(axis=1)\n",
    "        \n",
    "        corr_vm = vm.rolling(21, min_periods=5).corr(mm).fillna(0)\n",
    "        corr_vs = vm.rolling(21, min_periods=5).corr(sm).fillna(0)\n",
    "        corr_ms = mm.rolling(21, min_periods=5).corr(sm).fillna(0)\n",
    "        \n",
    "        df['corr_vm'] = corr_vm\n",
    "        df['corr_vs'] = corr_vs\n",
    "        df['corr_ms'] = corr_ms\n",
    "        \n",
    "        avg_corr = (abs(corr_vm) + abs(corr_vs) + abs(corr_ms)) / 3\n",
    "        df['cross_domain_corr'] = avg_corr\n",
    "        df['correlation_surge'] = (avg_corr > 0.7).astype(float)\n",
    "        df['correlation_collapse'] = (avg_corr < 0.2).astype(float)\n",
    "    else:\n",
    "        df['corr_vm'] = df['corr_vs'] = df['corr_ms'] = 0\n",
    "        df['cross_domain_corr'] = 0\n",
    "        df['correlation_surge'] = df['correlation_collapse'] = 0\n",
    "    \n",
    "    # === RISK SIGNAL COMPOSITE ===\n",
    "    df['risk_signal'] = (\n",
    "        df['var_compression'] * 0.3 +\n",
    "        df['ac1_rising'] * 0.2 +\n",
    "        df['delta_rising'] * 0.2 +\n",
    "        df['correlation_surge'] * 0.2 +\n",
    "        df['vol_expanding'] * 0.1\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Full PROMETHEUS feature engineering defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process training data\n",
    "print(\"Loading training data...\")\n",
    "train_df = pd.read_csv(DATA_DIR / 'train.csv')\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "\n",
    "print(\"\\nApplying PROMETHEUS feature engineering...\")\n",
    "df = train_df.copy()\n",
    "df = df.sort_values('date_id').reset_index(drop=True)\n",
    "df = add_full_prometheus_features(df)\n",
    "\n",
    "# Get feature columns\n",
    "exclude_cols = ['date_id', 'forward_returns', 'risk_free_rate', \n",
    "                'market_forward_excess_returns', 'is_scored',\n",
    "                'lagged_forward_returns', 'lagged_risk_free_rate',\n",
    "                'lagged_market_forward_excess_returns', 'lagged_ret']\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols \n",
    "                and df[c].dtype in ['float64', 'int64', 'float32', 'int32']]\n",
    "\n",
    "print(f\"\\nTotal features: {len(feature_cols)}\")\n",
    "print(f\"\\nPROMETHEUS features:\")\n",
    "prometheus_feats = [c for c in feature_cols if any(x in c for x in \n",
    "    ['var_', 'ac1', 'delta', 'temperature', 'order', 'coherence', 'vol_regime', \n",
    "     'sent_', 'momentum_', 'econ_', 'rate_', 'corr_', 'cross_domain', \n",
    "     'correlation_', 'sharpe_', 'risk_signal', '_z'])]\n",
    "print(f\"  Count: {len(prometheus_feats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "df_valid = df[df['market_forward_excess_returns'].notna()].copy()\n",
    "print(f\"Valid samples: {len(df_valid)}\")\n",
    "\n",
    "X = df_valid[feature_cols].fillna(0)\n",
    "y = df_valid['market_forward_excess_returns']\n",
    "\n",
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=feature_cols, index=X.index)\n",
    "\n",
    "# Time series split\n",
    "val_size = int(len(X_scaled) * 0.2)\n",
    "X_train = X_scaled.iloc[:-val_size]\n",
    "y_train = y.iloc[:-val_size]\n",
    "X_val = X_scaled.iloc[-val_size:]\n",
    "y_val = y.iloc[-val_size:]\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}\")\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble models\n",
    "print(\"Training V5 FULL PROMETHEUS ensemble...\")\n",
    "\n",
    "lgb_models = []\n",
    "xgb_models = []\n",
    "\n",
    "for seed in range(CONFIG['n_models']):\n",
    "    print(f\"\\nModel {seed+1}/{CONFIG['n_models']}...\")\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_params = CONFIG['lgb_params'].copy()\n",
    "    lgb_params['seed'] = 42 + seed\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    lgb_model = lgb.train(\n",
    "        lgb_params,\n",
    "        train_data,\n",
    "        num_boost_round=CONFIG['num_boost_round'],\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(CONFIG['early_stopping'], verbose=False)]\n",
    "    )\n",
    "    lgb_models.append(lgb_model)\n",
    "    print(f\"  LGB iter: {lgb_model.best_iteration}\")\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_params = CONFIG['xgb_params'].copy()\n",
    "    xgb_params['seed'] = 42 + seed\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    xgb_model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=CONFIG['num_boost_round'],\n",
    "        evals=[(dval, 'val')],\n",
    "        early_stopping_rounds=CONFIG['early_stopping'],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    xgb_models.append(xgb_model)\n",
    "    print(f\"  XGB iter: {xgb_model.best_iteration}\")\n",
    "\n",
    "print(f\"\\nTrained {len(lgb_models)} LGB + {len(xgb_models)} XGB models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate with FULL PROMETHEUS inference\n",
    "print(\"\\nValidating with FULL PROMETHEUS inference...\")\n",
    "\n",
    "# Get predictions from all models\n",
    "all_predictions = []\n",
    "reliabilities = []\n",
    "\n",
    "for model in lgb_models:\n",
    "    pred = model.predict(X_val)\n",
    "    all_predictions.append(pred)\n",
    "    reliabilities.append(0.9)  # LGB slightly more reliable\n",
    "\n",
    "dval = xgb.DMatrix(X_val)\n",
    "for model in xgb_models:\n",
    "    pred = model.predict(dval)\n",
    "    all_predictions.append(pred)\n",
    "    reliabilities.append(0.85)  # XGB\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "reliabilities = np.array(reliabilities)\n",
    "\n",
    "# Apply FULL PROMETHEUS FUSION per sample\n",
    "final_predictions = []\n",
    "confidences = []\n",
    "conflicts = []\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    preds_i = all_predictions[:, i]\n",
    "    \n",
    "    # Dempster-Shafer fusion (Insight #5)\n",
    "    ds_pred, ds_conf, ds_conflict = dempster_shafer_fusion(preds_i, reliabilities)\n",
    "    \n",
    "    # Value clustering (Insight #6)\n",
    "    cluster_pred, cluster_conf = value_clustering(preds_i)\n",
    "    \n",
    "    # CIC functional (Insight #17)\n",
    "    cic_F = compute_cic_functional(preds_i)\n",
    "    \n",
    "    # Combined prediction\n",
    "    final_pred = 0.4 * ds_pred + 0.4 * cluster_pred + 0.2 * np.mean(preds_i)\n",
    "    \n",
    "    # Combined confidence\n",
    "    confidence = 0.4 * ds_conf + 0.4 * cluster_conf + 0.2 * cic_F\n",
    "    \n",
    "    final_predictions.append(final_pred)\n",
    "    confidences.append(confidence)\n",
    "    conflicts.append(ds_conflict)\n",
    "\n",
    "final_predictions = np.array(final_predictions)\n",
    "confidences = np.array(confidences)\n",
    "conflicts = np.array(conflicts)\n",
    "\n",
    "print(f\"Average confidence: {confidences.mean():.3f}\")\n",
    "print(f\"Average conflict: {np.mean(conflicts):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL PROMETHEUS POSITION SIZING with risk signals\n",
    "print(\"\\nApplying FULL PROMETHEUS position sizing...\")\n",
    "\n",
    "cfg = CONFIG\n",
    "positions = []\n",
    "\n",
    "# Get validation data features for risk signals\n",
    "val_df = df_valid.iloc[-val_size:]\n",
    "\n",
    "for i in range(len(final_predictions)):\n",
    "    pred = final_predictions[i]\n",
    "    conf = confidences[i]\n",
    "    conflict = conflicts[i]\n",
    "    \n",
    "    # Get risk signals from features\n",
    "    row = val_df.iloc[i]\n",
    "    var_compression = row.get('var_compression', 0)\n",
    "    ac1_rising = row.get('ac1_rising', 0)\n",
    "    delta_rising = row.get('delta_rising', 0)\n",
    "    correlation_surge = row.get('correlation_surge', 0)\n",
    "    vol_expanding = row.get('vol_expanding', 0)\n",
    "    \n",
    "    # Base Kelly position\n",
    "    std_pred = np.std(all_predictions[:, i])\n",
    "    uncertainty = max(std_pred, 1e-5)\n",
    "    kelly = pred / (cfg['risk_aversion'] * uncertainty**2 + 1e-8)\n",
    "    base_pos = cfg['base_position'] + cfg['scale_factor'] * kelly\n",
    "    \n",
    "    # Apply risk adjustments\n",
    "    risk_factor = 1.0\n",
    "    \n",
    "    # Variance compression → reduce (Insight #1)\n",
    "    if var_compression > 0.5:\n",
    "        risk_factor *= cfg['var_compression_factor']\n",
    "    \n",
    "    # AC1 rising → reduce (Insight #2)\n",
    "    if ac1_rising > 0.5:\n",
    "        risk_factor *= cfg['ac1_rising_factor']\n",
    "    \n",
    "    # Delta rising → crash risk → reduce aggressively (Insight #3)\n",
    "    if delta_rising > 0.5:\n",
    "        risk_factor *= 0.5  # Aggressive reduction\n",
    "    \n",
    "    # Correlation surge → reduce (Insight #12)\n",
    "    if correlation_surge > 0.5:\n",
    "        risk_factor *= cfg['correlation_surge_factor']\n",
    "    \n",
    "    # Vol expanding → reduce (Insight #13)\n",
    "    if vol_expanding > 0.5:\n",
    "        risk_factor *= cfg['vol_expanding_factor']\n",
    "    \n",
    "    # High conflict → reduce (Insight #5)\n",
    "    if conflict > 0.5:\n",
    "        risk_factor *= cfg['high_conflict_factor']\n",
    "    \n",
    "    # Apply confidence scaling\n",
    "    risk_factor *= conf\n",
    "    \n",
    "    # Final position\n",
    "    position = cfg['base_position'] + (base_pos - cfg['base_position']) * risk_factor\n",
    "    position = np.clip(position, cfg['min_position'], cfg['max_position'])\n",
    "    positions.append(position)\n",
    "\n",
    "positions = np.array(positions)\n",
    "print(f\"Position Mean: {positions.mean():.3f}\")\n",
    "print(f\"Position Std: {positions.std():.3f}\")\n",
    "print(f\"Position Range: [{positions.min():.3f}, {positions.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate validation metrics\n",
    "strategy_returns = positions * y_val.values\n",
    "market_returns = y_val.values\n",
    "\n",
    "strategy_mean = strategy_returns.mean() * 252\n",
    "strategy_vol = strategy_returns.std() * np.sqrt(252)\n",
    "market_vol = market_returns.std() * np.sqrt(252)\n",
    "sharpe = strategy_mean / (strategy_vol + 1e-8)\n",
    "\n",
    "# Calculate max drawdown\n",
    "cumulative = np.cumprod(1 + strategy_returns) - 1\n",
    "running_max = np.maximum.accumulate(cumulative + 1)\n",
    "drawdown = (cumulative + 1) / running_max - 1\n",
    "max_dd = drawdown.min()\n",
    "\n",
    "# Volatility-adjusted Sharpe (competition metric)\n",
    "vol_penalty = max(1.0, (strategy_vol / market_vol) / 1.2) if market_vol > 0 else 1.0\n",
    "adjusted_sharpe = sharpe / vol_penalty\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"V5 FULL PROMETHEUS VALIDATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Strategy Annual Return: {strategy_mean:.4f}\")\n",
    "print(f\"Strategy Annual Vol: {strategy_vol:.4f}\")\n",
    "print(f\"Market Annual Vol: {market_vol:.4f}\")\n",
    "print(f\"Sharpe Ratio: {sharpe:.4f}\")\n",
    "print(f\"Adjusted Sharpe: {adjusted_sharpe:.4f}\")\n",
    "print(f\"Max Drawdown: {max_dd:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save V5 artifacts\n",
    "print(f\"\\nSaving V5 artifacts to {ARTIFACTS_DIR}...\")\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'feature_cols.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_cols, f)\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'lgb_models.pkl', 'wb') as f:\n",
    "    pickle.dump(lgb_models, f)\n",
    "\n",
    "for i, model in enumerate(xgb_models):\n",
    "    model.save_model(str(ARTIFACTS_DIR / f'xgb_model_{i}.json'))\n",
    "\n",
    "config_to_save = {\n",
    "    'base_position': cfg['base_position'],\n",
    "    'risk_aversion': cfg['risk_aversion'],\n",
    "    'scale_factor': cfg['scale_factor'],\n",
    "    'min_position': cfg['min_position'],\n",
    "    'max_position': cfg['max_position'],\n",
    "    'var_compression_factor': cfg['var_compression_factor'],\n",
    "    'ac1_rising_factor': cfg['ac1_rising_factor'],\n",
    "    'correlation_surge_factor': cfg['correlation_surge_factor'],\n",
    "    'high_conflict_factor': cfg['high_conflict_factor'],\n",
    "    'vol_expanding_factor': cfg['vol_expanding_factor']\n",
    "}\n",
    "with open(ARTIFACTS_DIR / 'config.pkl', 'wb') as f:\n",
    "    pickle.dump(config_to_save, f)\n",
    "\n",
    "recent_data = df_valid.tail(300).copy()\n",
    "recent_data.to_parquet(ARTIFACTS_DIR / 'recent_data.parquet')\n",
    "\n",
    "print(\"\\nV5 FULL PROMETHEUS artifacts saved!\")\n",
    "print(f\"Feature count: {len(feature_cols)}\")\n",
    "print(f\"Models: {len(lgb_models)} LGB + {len(xgb_models)} XGB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
