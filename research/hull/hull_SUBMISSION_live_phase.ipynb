{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hull Tactical - SUBMISSION (Live Phase Ready)\n",
    "\n",
    "**This notebook submits predictions using pre-trained artifacts.**\n",
    "\n",
    "Attach dataset: `ryancardwell/hull-artifacts-v3` (or your latest artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Hull Tactical Submission - LIVE PHASE READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find artifacts directory\n",
    "ARTIFACTS_DIRS = [\n",
    "    '/kaggle/input/hull-artifacts-v3',\n",
    "    '/kaggle/input/hull-artifacts-v4', \n",
    "    '/kaggle/input/hull-artifacts-v5',\n",
    "    '/kaggle/input/hull-artifacts',\n",
    "]\n",
    "\n",
    "ARTIFACTS_DIR = None\n",
    "for d in ARTIFACTS_DIRS:\n",
    "    if os.path.exists(d):\n",
    "        ARTIFACTS_DIR = Path(d)\n",
    "        break\n",
    "\n",
    "if ARTIFACTS_DIR is None:\n",
    "    raise FileNotFoundError(\"No artifacts found! Attach hull-artifacts dataset.\")\n",
    "\n",
    "print(f\"Using artifacts: {ARTIFACTS_DIR}\")\n",
    "print(f\"Contents: {list(ARTIFACTS_DIR.iterdir())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load artifacts\n",
    "with open(ARTIFACTS_DIR / 'scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'feature_cols.pkl', 'rb') as f:\n",
    "    feature_cols = pickle.load(f)\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'lgb_models.pkl', 'rb') as f:\n",
    "    lgb_models = pickle.load(f)\n",
    "\n",
    "with open(ARTIFACTS_DIR / 'config.pkl', 'rb') as f:\n",
    "    CONFIG = pickle.load(f)\n",
    "\n",
    "# Load XGB models\n",
    "xgb_models = []\n",
    "for i in range(len(lgb_models)):\n",
    "    xgb_path = ARTIFACTS_DIR / f'xgb_model_{i}.json'\n",
    "    if xgb_path.exists():\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(str(xgb_path))\n",
    "        xgb_models.append(model)\n",
    "\n",
    "# Load recent historical data for feature computation\n",
    "recent_data = pd.read_parquet(ARTIFACTS_DIR / 'recent_data.parquet')\n",
    "\n",
    "print(f\"Loaded {len(lgb_models)} LGB + {len(xgb_models)} XGB models\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering (simplified for inference)\n",
    "def add_features_for_inference(df: pd.DataFrame, history: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add features using historical context.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Combine with history for rolling calculations\n",
    "    combined = pd.concat([history, df], ignore_index=True)\n",
    "    \n",
    "    key_cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'M1', 'M2', 'M3', 'S1', 'S2', 'S3', 'E1', 'E2', 'P1', 'I1', 'I2']\n",
    "    key_cols = [c for c in key_cols if c in combined.columns]\n",
    "    \n",
    "    for col in key_cols:\n",
    "        for window in [5, 10, 21, 63]:\n",
    "            combined[f'{col}_ma{window}'] = combined[col].rolling(window, min_periods=1).mean()\n",
    "            combined[f'{col}_std{window}'] = combined[col].rolling(window, min_periods=1).std().fillna(0)\n",
    "            combined[f'{col}_z{window}'] = (combined[col] - combined[f'{col}_ma{window}']) / (combined[f'{col}_std{window}'] + 1e-8)\n",
    "    \n",
    "    # Lagged returns\n",
    "    if 'lagged_forward_returns' in combined.columns:\n",
    "        combined['lagged_ret'] = combined['lagged_forward_returns']\n",
    "    else:\n",
    "        combined['lagged_ret'] = 0\n",
    "    \n",
    "    for w in [5, 10, 21, 63, 126]:\n",
    "        combined[f'ret_cumsum_{w}'] = combined['lagged_ret'].rolling(w, min_periods=1).sum()\n",
    "        combined[f'ret_vol_{w}'] = combined['lagged_ret'].rolling(w, min_periods=1).std().fillna(0)\n",
    "        combined[f'sharpe_{w}'] = combined[f'ret_cumsum_{w}'] / (combined[f'ret_vol_{w}'] * np.sqrt(w) + 1e-8)\n",
    "    \n",
    "    # Volatility features\n",
    "    v_cols = [c for c in combined.columns if c.startswith('V') and c[1:].isdigit()]\n",
    "    if v_cols:\n",
    "        combined['v_mean'] = combined[v_cols].mean(axis=1)\n",
    "        combined['v_std'] = combined[v_cols].std(axis=1)\n",
    "        combined['temperature'] = combined['v_std'] / (combined['v_mean'].abs() + 1e-8)\n",
    "        combined['vol_regime'] = combined[v_cols].mean(axis=1)\n",
    "        combined['vol_regime_ma21'] = combined['vol_regime'].rolling(21, min_periods=1).mean()\n",
    "    \n",
    "    # Return only the new rows\n",
    "    result = combined.tail(len(df)).reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "print(\"Feature engineering ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble prediction functions\n",
    "def dempster_shafer_fusion(predictions, reliabilities=None):\n",
    "    if reliabilities is None:\n",
    "        reliabilities = np.ones(len(predictions))\n",
    "    weights = reliabilities / reliabilities.sum()\n",
    "    fused = np.sum(predictions * weights)\n",
    "    \n",
    "    # Conflict\n",
    "    n = len(predictions)\n",
    "    conflict = 0.0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if np.sign(predictions[i]) != np.sign(predictions[j]):\n",
    "                conflict += reliabilities[i] * reliabilities[j]\n",
    "    conflict /= max(1, n * (n-1) / 2)\n",
    "    \n",
    "    return fused, 1 - conflict, conflict\n",
    "\n",
    "print(\"Ensemble functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main prediction function\n",
    "history_buffer = recent_data.copy()\n",
    "\n",
    "def predict(test: pl.DataFrame) -> pl.DataFrame:\n",
    "    global history_buffer\n",
    "    \n",
    "    # Convert to pandas\n",
    "    test_pd = test.to_pandas()\n",
    "    \n",
    "    # Add features using history\n",
    "    test_features = add_features_for_inference(test_pd, history_buffer)\n",
    "    \n",
    "    # Prepare feature matrix\n",
    "    X = test_features[feature_cols].fillna(0)\n",
    "    \n",
    "    # Handle missing columns\n",
    "    for col in feature_cols:\n",
    "        if col not in X.columns:\n",
    "            X[col] = 0\n",
    "    X = X[feature_cols]\n",
    "    \n",
    "    # Scale\n",
    "    X_scaled = pd.DataFrame(scaler.transform(X), columns=feature_cols)\n",
    "    \n",
    "    # Get predictions from all models\n",
    "    predictions = []\n",
    "    reliabilities = []\n",
    "    \n",
    "    for model in lgb_models:\n",
    "        pred = model.predict(X_scaled)\n",
    "        predictions.append(pred[0] if len(pred) == 1 else pred.mean())\n",
    "        reliabilities.append(0.9)\n",
    "    \n",
    "    dmatrix = xgb.DMatrix(X_scaled)\n",
    "    for model in xgb_models:\n",
    "        pred = model.predict(dmatrix)\n",
    "        predictions.append(pred[0] if len(pred) == 1 else pred.mean())\n",
    "        reliabilities.append(0.85)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    reliabilities = np.array(reliabilities)\n",
    "    \n",
    "    # Dempster-Shafer fusion\n",
    "    fused_pred, confidence, conflict = dempster_shafer_fusion(predictions, reliabilities)\n",
    "    \n",
    "    # Position sizing\n",
    "    std_pred = np.std(predictions)\n",
    "    uncertainty = max(std_pred, 1e-5)\n",
    "    kelly = fused_pred / (CONFIG['risk_aversion'] * uncertainty**2 + 1e-8)\n",
    "    base_pos = CONFIG['base_position'] + CONFIG['scale_factor'] * kelly\n",
    "    \n",
    "    # Risk adjustment based on confidence\n",
    "    risk_factor = confidence\n",
    "    if conflict > 0.5:\n",
    "        risk_factor *= CONFIG.get('high_conflict_factor', 0.6)\n",
    "    \n",
    "    position = CONFIG['base_position'] + (base_pos - CONFIG['base_position']) * risk_factor\n",
    "    position = np.clip(position, CONFIG['min_position'], CONFIG['max_position'])\n",
    "    \n",
    "    # Update history buffer\n",
    "    history_buffer = pd.concat([history_buffer, test_pd], ignore_index=True).tail(500)\n",
    "    \n",
    "    return test.with_columns(pl.lit(float(position)).alias('prediction'))\n",
    "\n",
    "print(\"Prediction function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference server\n",
    "import kaggle_evaluation.default_inference_server as kes\n",
    "\n",
    "print(\"Starting inference server...\")\n",
    "inference_server = kes.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(\"Competition mode - serving predictions\")\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    print(\"Local mode - running validation\")\n",
    "    inference_server.run_local_gateway(\n",
    "        ('/kaggle/input/hull-tactical-market-prediction/',)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
