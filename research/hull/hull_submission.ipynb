{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hull Tactical Market Prediction - Ensemble Strategy\n",
    "\n",
    "## Strategy Overview\n",
    "- **Models**: LightGBM + XGBoost + CatBoost ensemble with uncertainty quantification\n",
    "- **Features**: Advanced feature engineering with rolling statistics, momentum, and regime detection\n",
    "- **Position Sizing**: Kelly criterion-inspired sizing with volatility scaling\n",
    "- **Validation**: Walk-forward validation to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q lightgbm xgboost catboost polars scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    SEED = 42\n",
    "    N_SPLITS = 5\n",
    "    LOOKBACK_WINDOWS = [5, 10, 21, 63, 126, 252]  # Trading days\n",
    "    TARGET_COL = 'market_forward_excess_returns'\n",
    "    DATE_COL = 'date_id'\n",
    "    \n",
    "    # Position sizing\n",
    "    BASE_POSITION = 1.0\n",
    "    MAX_POSITION = 2.0\n",
    "    MIN_POSITION = 0.0\n",
    "    RISK_AVERSION = 50.0\n",
    "    SCALE_FACTOR = 50.0\n",
    "    \n",
    "    # Model params\n",
    "    LGB_PARAMS = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': SEED,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    XGB_PARAMS = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.01,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'seed': SEED,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "\n",
    "np.random.seed(Config.SEED)\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "if os.path.exists('/kaggle/input/hull-tactical-market-prediction'):\n",
    "    DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "    ARTIFACTS_DIR = Path('/kaggle/working')\n",
    "else:\n",
    "    DATA_DIR = Path('/home/user/aimo3/hull/hull-tactical-market-prediction')\n",
    "    ARTIFACTS_DIR = Path('/home/user/aimo3/hull/artifacts')\n",
    "\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Artifacts directory: {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(DATA_DIR / 'train.csv')\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Date range: {train_df['date_id'].min()} - {train_df['date_id'].max()}\")\n",
    "print(f\"\\nTarget stats:\")\n",
    "print(train_df['market_forward_excess_returns'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"Advanced feature engineering for market prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, lookback_windows: List[int] = None):\n",
    "        self.lookback_windows = lookback_windows or [5, 10, 21, 63, 126, 252]\n",
    "        self.feature_cols = None\n",
    "        self.scaler = RobustScaler()\n",
    "        \n",
    "    def _get_base_features(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Get base feature columns.\"\"\"\n",
    "        exclude = ['date_id', 'forward_returns', 'risk_free_rate', \n",
    "                   'market_forward_excess_returns', 'is_scored',\n",
    "                   'lagged_forward_returns', 'lagged_risk_free_rate',\n",
    "                   'lagged_market_forward_excess_returns']\n",
    "        return [c for c in df.columns if c not in exclude]\n",
    "    \n",
    "    def _add_rolling_features(self, df: pd.DataFrame, base_cols: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Add rolling statistics for key features.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Select key features for rolling stats (top by variance)\n",
    "        numeric_cols = [c for c in base_cols if df[c].dtype in ['float64', 'int64']]\n",
    "        variances = df[numeric_cols].var().sort_values(ascending=False)\n",
    "        key_features = variances.head(20).index.tolist()\n",
    "        \n",
    "        for col in key_features:\n",
    "            for window in [5, 21, 63]:\n",
    "                if len(df) > window:\n",
    "                    # Rolling mean\n",
    "                    df[f'{col}_ma{window}'] = df[col].rolling(window, min_periods=1).mean()\n",
    "                    # Rolling std\n",
    "                    df[f'{col}_std{window}'] = df[col].rolling(window, min_periods=1).std()\n",
    "                    # Z-score\n",
    "                    df[f'{col}_zscore{window}'] = (\n",
    "                        (df[col] - df[f'{col}_ma{window}']) / \n",
    "                        (df[f'{col}_std{window}'] + 1e-8)\n",
    "                    )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _add_momentum_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add momentum and trend features.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Use lagged returns if available\n",
    "        if 'lagged_forward_returns' in df.columns:\n",
    "            ret_col = 'lagged_forward_returns'\n",
    "        elif 'forward_returns' in df.columns:\n",
    "            ret_col = 'forward_returns'\n",
    "            df[ret_col] = df[ret_col].shift(1)  # Lag to avoid lookahead\n",
    "        else:\n",
    "            return df\n",
    "        \n",
    "        # Cumulative returns\n",
    "        for window in [5, 10, 21, 63]:\n",
    "            if len(df) > window:\n",
    "                df[f'cum_ret_{window}d'] = df[ret_col].rolling(window, min_periods=1).sum()\n",
    "                df[f'ret_vol_{window}d'] = df[ret_col].rolling(window, min_periods=1).std()\n",
    "                # Sharpe-like ratio\n",
    "                df[f'sharpe_{window}d'] = (\n",
    "                    df[f'cum_ret_{window}d'] / (df[f'ret_vol_{window}d'] * np.sqrt(window) + 1e-8)\n",
    "                )\n",
    "        \n",
    "        # Trend strength\n",
    "        if len(df) > 21:\n",
    "            df['trend_21d'] = df[ret_col].rolling(21).apply(\n",
    "                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 21 else 0,\n",
    "                raw=False\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _add_regime_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add market regime detection features.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Volatility regime (using V features)\n",
    "        vol_cols = [c for c in df.columns if c.startswith('V')]\n",
    "        if vol_cols:\n",
    "            df['vol_regime'] = df[vol_cols].mean(axis=1)\n",
    "            df['vol_regime_ma21'] = df['vol_regime'].rolling(21, min_periods=1).mean()\n",
    "            df['vol_regime_high'] = (df['vol_regime'] > df['vol_regime_ma21']).astype(int)\n",
    "        \n",
    "        # Sentiment regime (using S features)\n",
    "        sent_cols = [c for c in df.columns if c.startswith('S')]\n",
    "        if sent_cols:\n",
    "            df['sent_regime'] = df[sent_cols].mean(axis=1)\n",
    "            df['sent_regime_ma21'] = df['sent_regime'].rolling(21, min_periods=1).mean()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _add_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add feature interactions.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Category means\n",
    "        categories = {'M': 'market', 'E': 'econ', 'I': 'interest', \n",
    "                      'P': 'price', 'V': 'vol', 'S': 'sent'}\n",
    "        \n",
    "        for prefix, name in categories.items():\n",
    "            cols = [c for c in df.columns if c.startswith(prefix) and \n",
    "                    not c.startswith(f'{prefix}_') and \n",
    "                    df[c].dtype in ['float64', 'int64']]\n",
    "            if cols:\n",
    "                df[f'{name}_mean'] = df[cols].mean(axis=1)\n",
    "                df[f'{name}_std'] = df[cols].std(axis=1)\n",
    "        \n",
    "        # Key interactions\n",
    "        if 'vol_mean' in df.columns and 'sent_mean' in df.columns:\n",
    "            df['vol_sent_interact'] = df['vol_mean'] * df['sent_mean']\n",
    "        \n",
    "        if 'market_mean' in df.columns and 'vol_mean' in df.columns:\n",
    "            df['market_vol_interact'] = df['market_mean'] / (df['vol_mean'] + 1e-8)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Fit and transform features.\"\"\"\n",
    "        base_cols = self._get_base_features(df)\n",
    "        \n",
    "        # Apply feature engineering\n",
    "        df = self._add_rolling_features(df, base_cols)\n",
    "        df = self._add_momentum_features(df)\n",
    "        df = self._add_regime_features(df)\n",
    "        df = self._add_interaction_features(df)\n",
    "        \n",
    "        # Get all numeric feature columns\n",
    "        exclude = ['date_id', 'forward_returns', 'risk_free_rate', \n",
    "                   'market_forward_excess_returns', 'is_scored',\n",
    "                   'lagged_forward_returns', 'lagged_risk_free_rate',\n",
    "                   'lagged_market_forward_excess_returns']\n",
    "        self.feature_cols = [c for c in df.columns \n",
    "                             if c not in exclude and \n",
    "                             df[c].dtype in ['float64', 'int64']]\n",
    "        \n",
    "        # Fill NaN and scale\n",
    "        df[self.feature_cols] = df[self.feature_cols].fillna(0)\n",
    "        df[self.feature_cols] = self.scaler.fit_transform(df[self.feature_cols])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform features using fitted scaler.\"\"\"\n",
    "        base_cols = self._get_base_features(df)\n",
    "        \n",
    "        df = self._add_rolling_features(df, base_cols)\n",
    "        df = self._add_momentum_features(df)\n",
    "        df = self._add_regime_features(df)\n",
    "        df = self._add_interaction_features(df)\n",
    "        \n",
    "        # Ensure all feature columns exist\n",
    "        for col in self.feature_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "        \n",
    "        df[self.feature_cols] = df[self.feature_cols].fillna(0)\n",
    "        df[self.feature_cols] = self.scaler.transform(df[self.feature_cols])\n",
    "        \n",
    "        return df\n",
    "\n",
    "print(\"FeatureEngineer class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Walk-Forward Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    \"\"\"Ensemble of gradient boosting models with uncertainty estimation.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_seeds: int = 3):\n",
    "        self.n_seeds = n_seeds\n",
    "        self.lgb_models = []\n",
    "        self.xgb_models = []\n",
    "        self.feature_cols = None\n",
    "        \n",
    "    def train(self, X: pd.DataFrame, y: pd.Series, \n",
    "              X_val: pd.DataFrame = None, y_val: pd.Series = None):\n",
    "        \"\"\"Train ensemble models.\"\"\"\n",
    "        self.feature_cols = X.columns.tolist()\n",
    "        \n",
    "        for seed in range(self.n_seeds):\n",
    "            # LightGBM\n",
    "            lgb_params = Config.LGB_PARAMS.copy()\n",
    "            lgb_params['seed'] = Config.SEED + seed\n",
    "            \n",
    "            train_data = lgb.Dataset(X, label=y)\n",
    "            val_data = lgb.Dataset(X_val, label=y_val) if X_val is not None else None\n",
    "            \n",
    "            lgb_model = lgb.train(\n",
    "                lgb_params,\n",
    "                train_data,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=[val_data] if val_data else None,\n",
    "                callbacks=[lgb.early_stopping(50, verbose=False)] if val_data else None\n",
    "            )\n",
    "            self.lgb_models.append(lgb_model)\n",
    "            \n",
    "            # XGBoost\n",
    "            xgb_params = Config.XGB_PARAMS.copy()\n",
    "            xgb_params['seed'] = Config.SEED + seed\n",
    "            \n",
    "            dtrain = xgb.DMatrix(X, label=y)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val) if X_val is not None else None\n",
    "            \n",
    "            evals = [(dval, 'val')] if dval else []\n",
    "            xgb_model = xgb.train(\n",
    "                xgb_params,\n",
    "                dtrain,\n",
    "                num_boost_round=1000,\n",
    "                evals=evals,\n",
    "                early_stopping_rounds=50 if evals else None,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            self.xgb_models.append(xgb_model)\n",
    "        \n",
    "        print(f\"Trained {len(self.lgb_models)} LGB + {len(self.xgb_models)} XGB models\")\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Predict with uncertainty estimation.\"\"\"\n",
    "        # Ensure column order matches training\n",
    "        X = X[self.feature_cols]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # LightGBM predictions\n",
    "        for model in self.lgb_models:\n",
    "            pred = model.predict(X)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # XGBoost predictions\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        for model in self.xgb_models:\n",
    "            pred = model.predict(dtest)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # Mean and std for uncertainty\n",
    "        mean_pred = predictions.mean(axis=0)\n",
    "        std_pred = predictions.std(axis=0)\n",
    "        \n",
    "        return mean_pred, std_pred\n",
    "\n",
    "print(\"EnsembleModel class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionSizer:\n",
    "    \"\"\"Position sizing with uncertainty-aware Kelly criterion.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_position: float = 1.0,\n",
    "                 risk_aversion: float = 50.0,\n",
    "                 scale_factor: float = 50.0,\n",
    "                 min_position: float = 0.0,\n",
    "                 max_position: float = 2.0):\n",
    "        self.base_position = base_position\n",
    "        self.risk_aversion = risk_aversion\n",
    "        self.scale_factor = scale_factor\n",
    "        self.min_position = min_position\n",
    "        self.max_position = max_position\n",
    "        \n",
    "    def size_position(self, \n",
    "                      prediction: float, \n",
    "                      uncertainty: float,\n",
    "                      recent_vol: float = None) -> float:\n",
    "        \"\"\"Calculate position size based on prediction and uncertainty.\"\"\"\n",
    "        \n",
    "        # Base Kelly-inspired sizing\n",
    "        # position = prediction / (risk_aversion * uncertainty^2)\n",
    "        uncertainty = max(uncertainty, 1e-6)\n",
    "        \n",
    "        kelly_fraction = prediction / (self.risk_aversion * uncertainty**2 + 1e-8)\n",
    "        \n",
    "        # Scale and add to base\n",
    "        position = self.base_position + self.scale_factor * kelly_fraction\n",
    "        \n",
    "        # Volatility adjustment if available\n",
    "        if recent_vol is not None and recent_vol > 0:\n",
    "            vol_adj = min(1.0, 0.015 / recent_vol)  # Target ~1.5% daily vol\n",
    "            position = self.base_position + (position - self.base_position) * vol_adj\n",
    "        \n",
    "        # Clip to valid range\n",
    "        position = np.clip(position, self.min_position, self.max_position)\n",
    "        \n",
    "        return position\n",
    "\n",
    "print(\"PositionSizer class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward validation\n",
    "def walk_forward_validation(df: pd.DataFrame, \n",
    "                            n_splits: int = 5,\n",
    "                            test_size: int = 180) -> dict:\n",
    "    \"\"\"Walk-forward validation with proper time series splits.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    df = df.sort_values('date_id').reset_index(drop=True)\n",
    "    \n",
    "    # Use only rows with valid target\n",
    "    df_valid = df[df['market_forward_excess_returns'].notna()].copy()\n",
    "    \n",
    "    total_rows = len(df_valid)\n",
    "    min_train_size = total_rows - n_splits * test_size\n",
    "    \n",
    "    if min_train_size < 500:\n",
    "        min_train_size = 500\n",
    "        n_splits = (total_rows - min_train_size) // test_size\n",
    "    \n",
    "    print(f\"Total rows: {total_rows}, Train min: {min_train_size}, Test size: {test_size}, Splits: {n_splits}\")\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        train_end = min_train_size + fold * test_size\n",
    "        test_end = train_end + test_size\n",
    "        \n",
    "        if test_end > total_rows:\n",
    "            break\n",
    "        \n",
    "        train_df = df_valid.iloc[:train_end]\n",
    "        test_df = df_valid.iloc[train_end:test_end]\n",
    "        \n",
    "        print(f\"\\nFold {fold+1}: Train {len(train_df)} rows, Test {len(test_df)} rows\")\n",
    "        \n",
    "        # Feature engineering\n",
    "        fe = FeatureEngineer()\n",
    "        train_fe = fe.fit_transform(train_df.copy())\n",
    "        test_fe = fe.transform(test_df.copy())\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train = train_fe[fe.feature_cols]\n",
    "        y_train = train_df['market_forward_excess_returns']\n",
    "        X_test = test_fe[fe.feature_cols]\n",
    "        y_test = test_df['market_forward_excess_returns']\n",
    "        \n",
    "        # Use last 20% of train as validation\n",
    "        val_size = int(len(X_train) * 0.2)\n",
    "        X_val = X_train.iloc[-val_size:]\n",
    "        y_val = y_train.iloc[-val_size:]\n",
    "        X_train_sub = X_train.iloc[:-val_size]\n",
    "        y_train_sub = y_train.iloc[:-val_size]\n",
    "        \n",
    "        # Train model\n",
    "        model = EnsembleModel(n_seeds=3)\n",
    "        model.train(X_train_sub, y_train_sub, X_val, y_val)\n",
    "        \n",
    "        # Predict\n",
    "        preds, uncertainty = model.predict(X_test)\n",
    "        \n",
    "        # Position sizing\n",
    "        sizer = PositionSizer(\n",
    "            base_position=Config.BASE_POSITION,\n",
    "            risk_aversion=Config.RISK_AVERSION,\n",
    "            scale_factor=Config.SCALE_FACTOR\n",
    "        )\n",
    "        \n",
    "        positions = []\n",
    "        for i in range(len(preds)):\n",
    "            pos = sizer.size_position(preds[i], uncertainty[i])\n",
    "            positions.append(pos)\n",
    "        positions = np.array(positions)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        strategy_returns = positions * y_test.values\n",
    "        market_returns = y_test.values\n",
    "        \n",
    "        strategy_mean = strategy_returns.mean() * 252\n",
    "        strategy_vol = strategy_returns.std() * np.sqrt(252)\n",
    "        market_vol = market_returns.std() * np.sqrt(252)\n",
    "        \n",
    "        sharpe = strategy_mean / (strategy_vol + 1e-8)\n",
    "        \n",
    "        # Adjusted sharpe with penalties\n",
    "        vol_penalty = max(1.0, (strategy_vol / market_vol) / 1.2) if market_vol > 0 else 1.0\n",
    "        adjusted_sharpe = sharpe / vol_penalty\n",
    "        \n",
    "        print(f\"  Strategy return: {strategy_mean:.4f}, Vol: {strategy_vol:.4f}\")\n",
    "        print(f\"  Sharpe: {sharpe:.4f}, Adjusted: {adjusted_sharpe:.4f}\")\n",
    "        print(f\"  Position mean: {positions.mean():.3f}, std: {positions.std():.3f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'fold': fold,\n",
    "            'sharpe': sharpe,\n",
    "            'adjusted_sharpe': adjusted_sharpe,\n",
    "            'strategy_vol': strategy_vol,\n",
    "            'market_vol': market_vol,\n",
    "            'position_mean': positions.mean(),\n",
    "            'position_std': positions.std()\n",
    "        })\n",
    "    \n",
    "    # Summary\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"WALK-FORWARD VALIDATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Mean Adjusted Sharpe: {results_df['adjusted_sharpe'].mean():.4f} Â± {results_df['adjusted_sharpe'].std():.4f}\")\n",
    "    print(f\"Mean Position: {results_df['position_mean'].mean():.3f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run validation\n",
    "print(\"Running walk-forward validation...\")\n",
    "val_results = walk_forward_validation(train_df, n_splits=5, test_size=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Final Model on Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on all available data\n",
    "print(\"Training final model on full dataset...\")\n",
    "\n",
    "# Prepare full dataset\n",
    "df_full = train_df[train_df['market_forward_excess_returns'].notna()].copy()\n",
    "df_full = df_full.sort_values('date_id').reset_index(drop=True)\n",
    "\n",
    "# Feature engineering\n",
    "feature_engineer = FeatureEngineer()\n",
    "df_fe = feature_engineer.fit_transform(df_full.copy())\n",
    "\n",
    "# Prepare features and target\n",
    "X_full = df_fe[feature_engineer.feature_cols]\n",
    "y_full = df_full['market_forward_excess_returns']\n",
    "\n",
    "# Use last 20% as validation for early stopping\n",
    "val_size = int(len(X_full) * 0.2)\n",
    "X_train = X_full.iloc[:-val_size]\n",
    "y_train = y_full.iloc[:-val_size]\n",
    "X_val = X_full.iloc[-val_size:]\n",
    "y_val = y_full.iloc[-val_size:]\n",
    "\n",
    "print(f\"Training size: {len(X_train)}, Validation size: {len(X_val)}\")\n",
    "print(f\"Number of features: {len(feature_engineer.feature_cols)}\")\n",
    "\n",
    "# Train ensemble\n",
    "final_model = EnsembleModel(n_seeds=5)  # More seeds for final model\n",
    "final_model.train(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Position sizer\n",
    "position_sizer = PositionSizer(\n",
    "    base_position=Config.BASE_POSITION,\n",
    "    risk_aversion=Config.RISK_AVERSION,\n",
    "    scale_factor=Config.SCALE_FACTOR\n",
    ")\n",
    "\n",
    "print(\"\\nFinal model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts\n",
    "print(\"Saving model artifacts...\")\n",
    "\n",
    "# Save feature engineer\n",
    "with open(ARTIFACTS_DIR / 'feature_engineer.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_engineer, f)\n",
    "\n",
    "# Save models\n",
    "with open(ARTIFACTS_DIR / 'lgb_models.pkl', 'wb') as f:\n",
    "    pickle.dump(final_model.lgb_models, f)\n",
    "\n",
    "# Save XGBoost models\n",
    "for i, model in enumerate(final_model.xgb_models):\n",
    "    model.save_model(str(ARTIFACTS_DIR / f'xgb_model_{i}.json'))\n",
    "\n",
    "# Save feature columns\n",
    "with open(ARTIFACTS_DIR / 'feature_cols.pkl', 'wb') as f:\n",
    "    pickle.dump(final_model.feature_cols, f)\n",
    "\n",
    "# Save position sizer config\n",
    "position_config = {\n",
    "    'base_position': Config.BASE_POSITION,\n",
    "    'risk_aversion': Config.RISK_AVERSION,\n",
    "    'scale_factor': Config.SCALE_FACTOR,\n",
    "    'min_position': Config.MIN_POSITION,\n",
    "    'max_position': Config.MAX_POSITION\n",
    "}\n",
    "with open(ARTIFACTS_DIR / 'position_config.pkl', 'wb') as f:\n",
    "    pickle.dump(position_config, f)\n",
    "\n",
    "# Save recent training data for online features\n",
    "recent_data = df_full.tail(300).copy()  # Last 300 days for feature calculation\n",
    "recent_data.to_parquet(ARTIFACTS_DIR / 'recent_data.parquet')\n",
    "\n",
    "print(f\"Artifacts saved to {ARTIFACTS_DIR}\")\n",
    "print(f\"Files: {list(ARTIFACTS_DIR.glob('*'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Server Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference code for submission\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# Import evaluation API\n",
    "import kaggle_evaluation.default_inference_server\n",
    "\n",
    "# Load artifacts\n",
    "if os.path.exists('/kaggle/input/hull-submission-artifacts'):\n",
    "    ARTIFACTS_DIR = Path('/kaggle/input/hull-submission-artifacts')\n",
    "elif os.path.exists('/kaggle/working'):\n",
    "    ARTIFACTS_DIR = Path('/kaggle/working')\n",
    "else:\n",
    "    ARTIFACTS_DIR = Path('/home/user/aimo3/hull/artifacts')\n",
    "\n",
    "# Global variables for inference\n",
    "feature_engineer = None\n",
    "lgb_models = None\n",
    "xgb_models = None\n",
    "feature_cols = None\n",
    "position_config = None\n",
    "recent_data = None\n",
    "history_buffer = []\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load all model artifacts.\"\"\"\n",
    "    global feature_engineer, lgb_models, xgb_models, feature_cols, position_config, recent_data\n",
    "    \n",
    "    print(f\"Loading artifacts from {ARTIFACTS_DIR}...\")\n",
    "    \n",
    "    with open(ARTIFACTS_DIR / 'feature_engineer.pkl', 'rb') as f:\n",
    "        feature_engineer = pickle.load(f)\n",
    "    \n",
    "    with open(ARTIFACTS_DIR / 'lgb_models.pkl', 'rb') as f:\n",
    "        lgb_models = pickle.load(f)\n",
    "    \n",
    "    xgb_models = []\n",
    "    i = 0\n",
    "    while (ARTIFACTS_DIR / f'xgb_model_{i}.json').exists():\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(str(ARTIFACTS_DIR / f'xgb_model_{i}.json'))\n",
    "        xgb_models.append(model)\n",
    "        i += 1\n",
    "    \n",
    "    with open(ARTIFACTS_DIR / 'feature_cols.pkl', 'rb') as f:\n",
    "        feature_cols = pickle.load(f)\n",
    "    \n",
    "    with open(ARTIFACTS_DIR / 'position_config.pkl', 'rb') as f:\n",
    "        position_config = pickle.load(f)\n",
    "    \n",
    "    recent_data = pd.read_parquet(ARTIFACTS_DIR / 'recent_data.parquet')\n",
    "    \n",
    "    print(f\"Loaded {len(lgb_models)} LGB + {len(xgb_models)} XGB models\")\n",
    "    print(f\"Feature columns: {len(feature_cols)}\")\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"Main prediction function for the evaluation API.\"\"\"\n",
    "    global feature_engineer, lgb_models, xgb_models, feature_cols, position_config, recent_data, history_buffer\n",
    "    \n",
    "    # Load models on first call\n",
    "    if feature_engineer is None:\n",
    "        load_models()\n",
    "    \n",
    "    # Convert to pandas\n",
    "    test_pd = test.to_pandas()\n",
    "    \n",
    "    # Add to history buffer\n",
    "    history_buffer.append(test_pd)\n",
    "    if len(history_buffer) > 300:\n",
    "        history_buffer = history_buffer[-300:]\n",
    "    \n",
    "    # Create feature dataframe with history\n",
    "    if len(history_buffer) < 21:\n",
    "        # Use recent training data for initial predictions\n",
    "        combined = pd.concat([recent_data.tail(300 - len(history_buffer))] + history_buffer, ignore_index=True)\n",
    "    else:\n",
    "        combined = pd.concat(history_buffer, ignore_index=True)\n",
    "    \n",
    "    # Transform features\n",
    "    combined_fe = feature_engineer.transform(combined.copy())\n",
    "    \n",
    "    # Get features for current row\n",
    "    X = combined_fe[feature_cols].iloc[[-1]]\n",
    "    \n",
    "    # Predict with ensemble\n",
    "    predictions = []\n",
    "    \n",
    "    for model in lgb_models:\n",
    "        pred = model.predict(X)\n",
    "        predictions.append(pred[0])\n",
    "    \n",
    "    dtest = xgb.DMatrix(X)\n",
    "    for model in xgb_models:\n",
    "        pred = model.predict(dtest)\n",
    "        predictions.append(pred[0])\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    mean_pred = predictions.mean()\n",
    "    std_pred = predictions.std()\n",
    "    \n",
    "    # Position sizing\n",
    "    uncertainty = max(std_pred, 1e-6)\n",
    "    kelly_fraction = mean_pred / (position_config['risk_aversion'] * uncertainty**2 + 1e-8)\n",
    "    position = position_config['base_position'] + position_config['scale_factor'] * kelly_fraction\n",
    "    position = np.clip(position, position_config['min_position'], position_config['max_position'])\n",
    "    \n",
    "    return float(position)\n",
    "\n",
    "# Create inference server\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "print(\"Inference server configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    # Production mode - serve the API\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Local testing mode\n",
    "    print(\"Running local gateway test...\")\n",
    "    inference_server.run_local_gateway((str(DATA_DIR),))\n",
    "    print(\"Local test completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
