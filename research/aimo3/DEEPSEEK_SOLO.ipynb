{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# DeepSeek-R1-70B Solo\n", "Just DeepSeek. No drama."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os, sys, subprocess, gc, time, re, math\n",
    "import glob as globmod\n",
    "from collections import Counter\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"WARNING\"\n",
    "\n",
    "import torch\n",
    "print(f\"Torch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Find wheels\n",
    "wheel_dirs = [\"/kaggle/input/vllm-wheels-py311\", \"/kaggle/input/deepseek-offline-wheels\"]\n",
    "wheel_dir = next((d for d in wheel_dirs if os.path.exists(d) and globmod.glob(f\"{d}/*.whl\")), None)\n",
    "if wheel_dir:\n",
    "    print(f\"Installing from {wheel_dir}\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-index\", f\"--find-links={wheel_dir}\", \"--no-deps\", \"-q\", \"vllm\"], timeout=120)\n",
    "    for pkg in [\"transformers\", \"accelerate\", \"safetensors\", \"tokenizers\", \"sentencepiece\", \"huggingface_hub\", \"pydantic\", \"msgspec\", \"cloudpickle\", \"einops\", \"filelock\", \"regex\", \"tqdm\", \"packaging\", \"typing_extensions\", \"jinja2\", \"triton\", \"xgrammar\"]:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-index\", f\"--find-links={wheel_dir}\", \"--no-deps\", \"-q\", pkg], timeout=60, capture_output=True)\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "print(\"vLLM OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import polars as pl\n",
    "\n",
    "START = time.time()\n",
    "CUTOFF = START + (4*60+50)*60\n",
    "MODEL_PATH = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-llama-70b/1\"\n",
    "PROBLEM_COUNT = 0\n",
    "\n",
    "print(f\"Loading {MODEL_PATH}...\")\n",
    "LLM_MODEL = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.92,\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=8192,\n",
    "    enforce_eager=True,\n",
    "    dtype=\"bfloat16\",\n",
    ")\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "PROMPTS = {\n",
    "    'algebraic': \"Solve using algebra. Think in <think> tags. Final integer in \\\\boxed{}.\",\n",
    "    'casework': \"Solve by case analysis. Think in <think> tags. Final integer in \\\\boxed{}.\",\n",
    "    'verification': \"Solve and verify by substitution. Think in <think> tags. Final integer in \\\\boxed{}.\",\n",
    "    'computational': \"Solve step by step. Think in <think> tags. Final integer in \\\\boxed{}.\",\n",
    "    'backwards': \"Work backwards from constraints. Think in <think> tags. Final integer in \\\\boxed{}.\",\n",
    "}\n",
    "TEMPS = [0.7, 0.85, 1.0]\n",
    "STOPS = [\"<\\uff5cend\\u2581of\\u2581sentence\\uff5c>\", \"<|endoftext|>\", \"</s>\"]\n",
    "\n",
    "def extract_boxed(text: str) -> Optional[int]:\n",
    "    for p in [r'\\\\boxed\\{(\\d+)\\}', r'boxed\\{(\\d+)\\}']:\n",
    "        m = re.findall(p, text)\n",
    "        if m:\n",
    "            v = int(m[-1])\n",
    "            if 0 <= v <= 99999: return v\n",
    "    m = re.findall(r'answer\\s*(?:is|=|:)\\s*(\\d+)', text[-500:], re.I)\n",
    "    if m:\n",
    "        v = int(m[-1])\n",
    "        if 0 <= v <= 99999: return v\n",
    "    return None\n",
    "\n",
    "def generate(question: str, system: str, temp: float) -> Optional[int]:\n",
    "    if time.time() >= CUTOFF: return None\n",
    "    prompt = f\"<\\uff5cbegin\\u2581of\\u2581sentence\\uff5c><\\uff5cUser\\uff5c>{system}\\n\\n{question}<\\uff5cAssistant\\uff5c><think>\\n\"\n",
    "    params = SamplingParams(temperature=temp, top_p=0.95, max_tokens=6144, stop=STOPS)\n",
    "    try:\n",
    "        out = LLM_MODEL.generate([prompt], sampling_params=params)\n",
    "        txt = out[0].outputs[0].text\n",
    "        return extract_boxed(txt.split(\"</think>\")[-1] if \"</think>\" in txt else txt)\n",
    "    except: return None\n",
    "\n",
    "def solve(question: str) -> int:\n",
    "    global PROBLEM_COUNT\n",
    "    PROBLEM_COUNT += 1\n",
    "    \n",
    "    time_left = CUTOFF - time.time()\n",
    "    time_per = time_left / max(1, 50 - PROBLEM_COUNT + 1)\n",
    "    max_samples = 20 if time_per > 300 else 15 if time_per > 180 else 10 if time_per > 60 else 5\n",
    "    \n",
    "    answers = []\n",
    "    tasks = [(p, t) for t in TEMPS for p in PROMPTS.keys()][:max_samples]\n",
    "    for ptype, temp in tasks:\n",
    "        if time.time() >= CUTOFF: break\n",
    "        ans = generate(question, PROMPTS[ptype], temp)\n",
    "        if ans is not None: answers.append(ans)\n",
    "        if len(answers) >= 5 and Counter(answers).most_common(1)[0][1] >= 4: break\n",
    "    \n",
    "    if not answers:\n",
    "        nums = [int(x) for x in re.findall(r'\\b\\d+\\b', question) if 0 < int(x) < 100000]\n",
    "        return nums[0] if nums else 42\n",
    "    \n",
    "    # Weighted vote\n",
    "    weighted = {v: math.log(1.25 + abs(v)) * c for v, c in Counter(answers).items()}\n",
    "    final = max(weighted, key=weighted.get)\n",
    "    print(f\"  -> {final} (from {Counter(answers).most_common(3)})\")\n",
    "    return final\n",
    "\n",
    "print(\"Solver ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame:\n",
    "    qid, question = id_.item(0), problem.item(0)\n",
    "    print(f\"\\n[{PROBLEM_COUNT+1}] {qid} | {(CUTOFF-time.time())/60:.1f}m left\")\n",
    "    print(f\"Q: {question[:80]}...\")\n",
    "    answer = solve(question)\n",
    "    print(f\"A: {answer}\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    return pl.DataFrame({\"id\": id_, \"answer\": answer})\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"DeepSeek-R1-70B Solo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    import kaggle_evaluation.aimo_3_inference_server\n",
    "    server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "        server.serve()\n",
    "    else:\n",
    "        test_file = '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv'\n",
    "        if os.path.exists(test_file):\n",
    "            server.run_local_gateway((test_file,))\n",
    "        else:\n",
    "            result = predict(pl.Series([\"test\"]), pl.Series([\"Find 2^10 mod 1000.\"]))\n",
    "            print(result)\n",
    "except ImportError:\n",
    "    result = predict(pl.Series([\"test\"]), pl.Series([\"Find 2^10 mod 1000.\"]))\n",
    "    print(result)\n",
    "\n",
    "print(f\"\\nDone in {(time.time()-START)/60:.1f}m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
