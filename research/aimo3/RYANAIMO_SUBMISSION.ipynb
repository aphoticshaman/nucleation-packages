{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RYANAIMO - Ω-AIMO3 Dual-Model Pipeline\n\n## Target: 47/50 → $1.59M\n\n## DUAL-MODEL STRATEGY\n\n| Model | Strength | Prompts | Quantization |\n|-------|----------|---------|--------------|\n| **DeepSeek-R1-32B** | Deep reasoning, proofs, symbolic math | algebraic, backwards, verification | AWQ 4-bit |\n| **Qwen-Coder-32B** | Code execution, enumeration, brute force | computational, casework | AWQ 4-bit |\n\nEach model covers the other's weakness:\n- **DeepSeek**: Native `<think>` reasoning, algebraic manipulation, proof construction\n- **Qwen-Coder**: Python execution, numerical verification, systematic enumeration\n\n## Pipeline:\n1. CLASSIFY - Detect problem type (reasoning vs computational)\n2. DIVERGE - Route to specialist model, 30 solution paths\n3. EXECUTE - Sandboxed Python + SymPy verification\n4. CONVERGE - Value clustering (88% error reduction) + log-weighted voting\n5. VERIFY - Cross-model verification on low-confidence answers\n\n## Required Datasets (create on RunPod, upload to Kaggle):\n1. **`ryanaimo-vllm-wheels`** - vLLM + all dependencies\n2. **`deepseek-r1-32b-awq`** - Reasoning model (AWQ 4-bit)\n3. **`qwen-coder-32b-awq`** - Code execution model (AWQ 4-bit)\n\n## RunPod Setup:\n```bash\n# See RUNPOD_COMMANDS.md for full instructions\n# 1. Setup Kaggle CLI\n# 2. Create wheel dataset\n# 3. Quantize models (AWQ)\n# 4. Upload directly to Kaggle\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 1: ENVIRONMENT + OFFLINE WHEEL INSTALLATION\n# =============================================================================\n# Requires dataset: ryanaimo-vllm-wheels (created via runpod_create_wheels.sh)\n# =============================================================================\nprint(\"CELL1 START\", flush=True)\n\nimport os\nimport sys\nimport subprocess\nimport glob as globmod\n\n# === OFFLINE MODE - NO NETWORK ACCESS ===\nos.environ[\"HF_HUB_OFFLINE\"] = \"1\"\nos.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\nos.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"VLLM_LOGGING_LEVEL\"] = \"WARNING\"\n\nprint(\"ENV SET\", flush=True)\n\n# === WHEEL DATASETS (in priority order) ===\nWHEEL_DATASETS = [\n    \"/kaggle/input/ryanaimo-vllm-wheels\",      # Our complete dataset\n    \"/kaggle/input/vllm-085-wheels\",           # Fallback\n    \"/kaggle/input/aimo3-offline-wheels\",      # Legacy\n]\n\ndef find_wheel_dir():\n    \"\"\"Find available wheel directory.\"\"\"\n    for d in WHEEL_DATASETS:\n        if os.path.exists(d):\n            whl_count = len(globmod.glob(f\"{d}/*.whl\"))\n            print(f\"FOUND {d}: {whl_count} wheels\", flush=True)\n            return d\n    return None\n\ndef install_from_wheels(wheel_dir):\n    \"\"\"Install vLLM and deps from wheel directory.\"\"\"\n    print(f\"INSTALLING FROM {wheel_dir}\", flush=True)\n    \n    # List what's available\n    all_files = sorted(os.listdir(wheel_dir))\n    wheels = [f for f in all_files if f.endswith('.whl')]\n    print(f\"  Available wheels: {len(wheels)}\", flush=True)\n    \n    # Show key packages\n    key_pkgs = ['vllm', 'msgspec', 'transformers', 'accelerate', 'polars', 'sympy']\n    for pkg in key_pkgs:\n        matches = [w for w in wheels if pkg in w.lower()]\n        if matches:\n            print(f\"    {pkg}: {matches[0][:50]}...\", flush=True)\n        else:\n            print(f\"    {pkg}: NOT FOUND\", flush=True)\n    \n    # Install ALL wheels (handles dependencies)\n    result = subprocess.run([\n        sys.executable, \"-m\", \"pip\", \"install\",\n        \"--no-index\",\n        f\"--find-links={wheel_dir}\",\n        \"--quiet\",\n        \"vllm\", \"transformers\", \"accelerate\", \"polars\", \"sympy\"\n    ], capture_output=True, text=True, timeout=600)\n    \n    if result.returncode != 0:\n        print(f\"INSTALL ERROR:\", flush=True)\n        print(result.stderr[-2000:], flush=True)\n        # Try installing individual wheels as fallback\n        print(\"TRYING INDIVIDUAL WHEEL INSTALL...\", flush=True)\n        for whl in wheels:\n            subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\",\n                \"--quiet\", f\"{wheel_dir}/{whl}\"\n            ], capture_output=True, timeout=60)\n    else:\n        print(\"INSTALL OK\", flush=True)\n    \n    return result.returncode == 0\n\n# === MAIN INSTALLATION LOGIC ===\nwheel_dir = find_wheel_dir()\n\nif wheel_dir:\n    install_from_wheels(wheel_dir)\nelse:\n    print(\"NO WHEEL DATASET FOUND!\", flush=True)\n    print(\"Available inputs:\", flush=True)\n    if os.path.exists(\"/kaggle/input\"):\n        for item in sorted(os.listdir(\"/kaggle/input\")):\n            print(f\"  - {item}\", flush=True)\n    print(\"\\nATTACH: ryanaimo-vllm-wheels dataset\", flush=True)\n\n# === VERIFY IMPORTS ===\nprint(\"\\nVERIFYING IMPORTS...\", flush=True)\ntry:\n    from vllm import LLM, SamplingParams\n    import vllm\n    print(f\"  vllm: {vllm.__version__}\", flush=True)\nexcept ImportError as e:\n    print(f\"  vllm: FAILED - {e}\", flush=True)\n    raise\n\ntry:\n    import transformers\n    print(f\"  transformers: {transformers.__version__}\", flush=True)\nexcept ImportError as e:\n    print(f\"  transformers: FAILED - {e}\", flush=True)\n\ntry:\n    import polars\n    print(f\"  polars: {polars.__version__}\", flush=True)\nexcept ImportError as e:\n    print(f\"  polars: FAILED - {e}\", flush=True)\n\ntry:\n    import sympy\n    print(f\"  sympy: {sympy.__version__}\", flush=True)\nexcept ImportError as e:\n    print(f\"  sympy: FAILED - {e}\", flush=True)\n\ntry:\n    import torch\n    print(f\"  torch: {torch.__version__} (CUDA: {torch.cuda.is_available()})\", flush=True)\nexcept ImportError as e:\n    print(f\"  torch: FAILED - {e}\", flush=True)\n\nprint(\"\\nCELL1 DONE\", flush=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 2: IMPORTS\nprint(\"CELL2 START\", flush=True)\nimport time\nimport math\nimport re\nimport gc\nimport tempfile\nimport subprocess\nimport statistics\nfrom typing import Optional, List, Dict, Tuple, Any\nfrom collections import Counter, defaultdict\n\nprint(\"numpy\", flush=True)\nimport numpy as np\n\nprint(\"torch\", flush=True)\nimport torch\n\nprint(\"polars\", flush=True)\nimport polars as pl\n\nSTART_TIME = time.time()\nTOTAL_BUDGET = (4 * 60 + 50) * 60\nCUTOFF_TIME = START_TIME + TOTAL_BUDGET\nPROBLEMS_EXPECTED = 50\nANSWER_MIN, ANSWER_MAX = 0, 99999\n\nprint(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\", flush=True)\nif torch.cuda.is_available():\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\", flush=True)\nprint(f\"Budget: {TOTAL_BUDGET//3600}h {(TOTAL_BUDGET%3600)//60}m\", flush=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 3: DUAL-MODEL LOADING (AWQ QUANTIZED)\n# =============================================================================\n# Strategy: Load BOTH models if memory allows, otherwise best single model\n# - DeepSeek-R1: Reasoning (algebraic, backwards, verification)\n# - Qwen-Coder: Code execution (computational, casework)\n# =============================================================================\nprint(\"CELL3 START\", flush=True)\nimport glob\n\n# === MODEL PATHS (AWQ quantized preferred) ===\n\n# DeepSeek-R1 paths (REASONING SPECIALIST)\nDEEPSEEK_PATHS = [\n    # AWQ quantized (our custom)\n    \"/kaggle/input/deepseek-r1-32b-awq\",\n    \"/kaggle/input/ryancardwell/deepseek-r1-32b-awq\",\n    # Original weights\n    \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-32b/1\",\n    \"/kaggle/input/deepseek-r1-distill-qwen-32b\",\n    \"/kaggle/input/deepseek-r1\",\n]\n\n# Qwen-Coder paths (CODE EXECUTION SPECIALIST)\nQWEN_CODER_PATHS = [\n    # AWQ quantized (our custom)\n    \"/kaggle/input/qwen-coder-32b-awq\",\n    \"/kaggle/input/ryancardwell/qwen-coder-32b-awq\",\n    # Original weights\n    \"/kaggle/input/qwen-coder-32b\",\n    \"/kaggle/input/Qwen/Qwen2.5-Coder-32B-Instruct\",\n]\n\n# Qwen-Math fallback paths\nQWEN_MATH_PATHS = [\n    \"/kaggle/input/qwen-72b-math-int4\",\n    \"/kaggle/input/qwen-72b-math-nf4\",\n    \"/kaggle/input/qwen2.5-math-72b-instruct\",\n]\n\ndef find_model(paths):\n    \"\"\"Find first available model path.\"\"\"\n    for p in paths:\n        if os.path.exists(p):\n            if os.path.exists(os.path.join(p, \"config.json\")):\n                return p\n            configs = glob.glob(f\"{p}/**/config.json\", recursive=True)\n            if configs:\n                return os.path.dirname(configs[0])\n    return None\n\ndef is_awq_model(path):\n    \"\"\"Check if model is AWQ quantized.\"\"\"\n    if path is None:\n        return False\n    return \"awq\" in path.lower() or os.path.exists(os.path.join(path, \"quant_config.json\"))\n\ndef get_vram_gb():\n    \"\"\"Get available VRAM in GB.\"\"\"\n    if torch.cuda.is_available():\n        return torch.cuda.get_device_properties(0).total_memory / 1e9\n    return 0\n\n# === FIND AVAILABLE MODELS ===\nprint(\"SEARCHING MODELS...\", flush=True)\n\nDEEPSEEK_PATH = find_model(DEEPSEEK_PATHS)\nQWEN_CODER_PATH = find_model(QWEN_CODER_PATHS)\nQWEN_MATH_PATH = find_model(QWEN_MATH_PATHS)\n\nprint(f\"  DeepSeek: {DEEPSEEK_PATH}\", flush=True)\nprint(f\"  Qwen-Coder: {QWEN_CODER_PATH}\", flush=True)\nprint(f\"  Qwen-Math: {QWEN_MATH_PATH}\", flush=True)\n\n# === DETERMINE LOADING STRATEGY ===\nVRAM_GB = get_vram_gb()\nprint(f\"  VRAM: {VRAM_GB:.1f}GB\", flush=True)\n\n# AWQ 4-bit: ~16GB for 32B model, ~8GB for 32B with aggressive settings\n# Two AWQ models need ~24-32GB\n# Single AWQ model fits in 16GB T4\n\nMODELS = {}  # {\"reasoning\": LLM, \"coding\": LLM}\nMODEL_CONFIGS = {}  # {\"reasoning\": {\"path\": ..., \"is_deepseek\": ...}, ...}\n\ndef load_model(path, name, gpu_util=0.45, max_len=8192):\n    \"\"\"Load a model with vLLM.\"\"\"\n    print(f\"LOADING {name} from {path}...\", flush=True)\n    print(f\"  gpu_util={gpu_util}, max_len={max_len}\", flush=True)\n    \n    is_awq = is_awq_model(path)\n    \n    kwargs = {\n        \"model\": path,\n        \"tensor_parallel_size\": 1,\n        \"gpu_memory_utilization\": gpu_util,\n        \"trust_remote_code\": True,\n        \"max_model_len\": max_len,\n        \"enforce_eager\": True,\n        \"seed\": 42,\n        \"tokenizer_mode\": \"auto\",\n    }\n    \n    # AWQ models need quantization flag\n    if is_awq:\n        kwargs[\"quantization\"] = \"awq\"\n        print(f\"  Using AWQ quantization\", flush=True)\n    \n    model = LLM(**kwargs)\n    print(f\"  {name} LOADED!\", flush=True)\n    return model\n\n# === LOADING LOGIC ===\nif DEEPSEEK_PATH and QWEN_CODER_PATH and VRAM_GB >= 28:\n    # DUAL MODEL MODE - Load both (need ~28GB+ VRAM)\n    print(\"\\n=== DUAL MODEL MODE ===\", flush=True)\n    \n    MODELS[\"reasoning\"] = load_model(DEEPSEEK_PATH, \"DeepSeek-R1\", gpu_util=0.45, max_len=8192)\n    MODEL_CONFIGS[\"reasoning\"] = {\"path\": DEEPSEEK_PATH, \"is_deepseek\": True}\n    \n    MODELS[\"coding\"] = load_model(QWEN_CODER_PATH, \"Qwen-Coder\", gpu_util=0.45, max_len=8192)\n    MODEL_CONFIGS[\"coding\"] = {\"path\": QWEN_CODER_PATH, \"is_deepseek\": False}\n    \n    DUAL_MODEL_MODE = True\n\nelif DEEPSEEK_PATH:\n    # SINGLE MODEL MODE - DeepSeek (best overall)\n    print(\"\\n=== SINGLE MODEL MODE (DeepSeek) ===\", flush=True)\n    \n    is_awq = is_awq_model(DEEPSEEK_PATH)\n    gpu_util = 0.92 if is_awq else 0.90\n    max_len = 16384 if is_awq else 12288\n    \n    MODELS[\"reasoning\"] = load_model(DEEPSEEK_PATH, \"DeepSeek-R1\", gpu_util=gpu_util, max_len=max_len)\n    MODEL_CONFIGS[\"reasoning\"] = {\"path\": DEEPSEEK_PATH, \"is_deepseek\": True}\n    MODELS[\"coding\"] = MODELS[\"reasoning\"]  # Use same model for both\n    MODEL_CONFIGS[\"coding\"] = MODEL_CONFIGS[\"reasoning\"]\n    \n    DUAL_MODEL_MODE = False\n\nelif QWEN_CODER_PATH:\n    # SINGLE MODEL MODE - Qwen-Coder\n    print(\"\\n=== SINGLE MODEL MODE (Qwen-Coder) ===\", flush=True)\n    \n    is_awq = is_awq_model(QWEN_CODER_PATH)\n    gpu_util = 0.92 if is_awq else 0.90\n    max_len = 16384 if is_awq else 8192\n    \n    MODELS[\"coding\"] = load_model(QWEN_CODER_PATH, \"Qwen-Coder\", gpu_util=gpu_util, max_len=max_len)\n    MODEL_CONFIGS[\"coding\"] = {\"path\": QWEN_CODER_PATH, \"is_deepseek\": False}\n    MODELS[\"reasoning\"] = MODELS[\"coding\"]\n    MODEL_CONFIGS[\"reasoning\"] = MODEL_CONFIGS[\"coding\"]\n    \n    DUAL_MODEL_MODE = False\n\nelif QWEN_MATH_PATH:\n    # FALLBACK - Qwen-Math\n    print(\"\\n=== FALLBACK MODE (Qwen-Math) ===\", flush=True)\n    \n    MODELS[\"reasoning\"] = load_model(QWEN_MATH_PATH, \"Qwen-Math\", gpu_util=0.95, max_len=8192)\n    MODEL_CONFIGS[\"reasoning\"] = {\"path\": QWEN_MATH_PATH, \"is_deepseek\": False}\n    MODELS[\"coding\"] = MODELS[\"reasoning\"]\n    MODEL_CONFIGS[\"coding\"] = MODEL_CONFIGS[\"reasoning\"]\n    \n    DUAL_MODEL_MODE = False\n\nelse:\n    print(\"\\nNO MODEL FOUND! Available inputs:\", flush=True)\n    if os.path.exists(\"/kaggle/input\"):\n        for item in sorted(os.listdir(\"/kaggle/input\")):\n            print(f\"  - {item}\", flush=True)\n    raise FileNotFoundError(\"Attach a model dataset!\")\n\n# === SUMMARY ===\nprint(f\"\\n{'='*50}\", flush=True)\nprint(f\"MODELS LOADED:\", flush=True)\nprint(f\"  Dual mode: {DUAL_MODEL_MODE}\", flush=True)\nprint(f\"  Reasoning: {MODEL_CONFIGS['reasoning']['path']}\", flush=True)\nprint(f\"  Coding: {MODEL_CONFIGS['coding']['path']}\", flush=True)\nprint(f\"{'='*50}\", flush=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 4: TACTICAL PROMPTS (Model-Specialized)\n# =============================================================================\n# Routing:\n#   - algebraic, backwards, verification → REASONING model (DeepSeek)\n#   - computational, casework → CODING model (Qwen-Coder)\n# =============================================================================\n\n# PROMPT ROUTING: which model handles which prompts\nPROMPT_ROUTING = {\n    'algebraic': 'reasoning',      # Pure math, symbolic manipulation\n    'backwards': 'reasoning',      # Working from goal to given\n    'verification': 'reasoning',   # Proof-based verification\n    'computational': 'coding',     # Python code execution\n    'casework': 'coding',          # Systematic enumeration\n}\n\n# DeepSeek prompts (reasoning specialist)\nPROMPTS_DEEPSEEK = {\n    'algebraic': \"\"\"Solve this mathematics olympiad problem using algebraic manipulation.\nThink deeply in <think> tags. Define all variables carefully. Check all cases.\nVerify your answer by substitution. Return the final integer in \\\\boxed{}.\nAnswer must be 0-99999.\"\"\",\n\n    'backwards': \"\"\"Solve this mathematics olympiad problem by working backwards.\nIn <think> tags, analyze what form the answer must take.\nWhat constraints does the problem impose? Derive from goal to given.\nReturn the final integer answer in \\\\boxed{}. Answer must be 0-99999.\"\"\",\n\n    'verification': \"\"\"Solve this mathematics olympiad problem with rigorous verification.\nFirst solve in <think> tags. Then VERIFY: substitute answer into all constraints.\nIf verification fails, try a different approach. Only accept verified answers.\nReturn the final integer answer in \\\\boxed{}. Answer must be 0-99999.\"\"\",\n\n    # DeepSeek can also code (fallback if no Qwen-Coder)\n    'computational': \"\"\"Solve this mathematics olympiad problem by writing Python code.\nIn <think> tags, reason about the approach. Then write clean Python with sympy.\nExecute mentally and verify. Print the final answer.\nReturn the final integer answer in \\\\boxed{}. Answer must be 0-99999.\"\"\",\n\n    'casework': \"\"\"Solve this mathematics olympiad problem by systematic case analysis.\nIn <think> tags, enumerate every possible case exhaustively.\nFor each case, compute the contribution. Sum all cases.\nReturn the final integer answer in \\\\boxed{}. Answer must be 0-99999.\"\"\",\n}\n\n# Qwen-Coder prompts (code execution specialist)\nPROMPTS_QWEN_CODER = {\n    'computational': \"\"\"Write Python code to solve this mathematics olympiad problem.\nUse sympy for symbolic math, numpy for numerical computation.\nPrint intermediate results for verification. Print the final answer clearly.\nThe answer must be an integer between 0 and 99999. Put it in \\\\boxed{}.\"\"\",\n\n    'casework': \"\"\"Write Python code to enumerate all cases for this olympiad problem.\nUse itertools for systematic enumeration. Check every possibility.\nCount or sum contributions from each valid case.\nPrint the final answer. It must be an integer 0-99999 in \\\\boxed{}.\"\"\",\n\n    # Qwen-Coder can also reason (fallback)\n    'algebraic': \"\"\"Solve this mathematics olympiad problem step-by-step.\nFirst reason about the approach. Then write Python code to verify.\nShow your algebraic work, then confirm with code.\nReturn the final integer answer in \\\\boxed{}. Answer must be 0-99999.\"\"\",\n\n    'backwards': \"\"\"Solve this problem by working backwards. Write Python to verify.\nWhat form must the answer take? Work from the goal to the given.\nVerify your reasoning with code.\nReturn the final integer answer in \\\\boxed{}. Answer must be 0-99999.\"\"\",\n\n    'verification': \"\"\"Solve this problem, then verify with Python code.\nFirst derive the answer algebraically. Then write code that checks every constraint.\nOnly accept the answer if code verification passes.\nReturn the final integer answer in \\\\boxed{}. Answer must be 0-99999.\"\"\",\n}\n\n# Function to get prompt for a given type and model\ndef get_prompt(prompt_type, model_key):\n    \"\"\"Get the appropriate prompt for the model type.\"\"\"\n    is_deepseek = MODEL_CONFIGS[model_key].get(\"is_deepseek\", False)\n    if is_deepseek:\n        return PROMPTS_DEEPSEEK.get(prompt_type, PROMPTS_DEEPSEEK['algebraic'])\n    else:\n        return PROMPTS_QWEN_CODER.get(prompt_type, PROMPTS_QWEN_CODER['computational'])\n\n# Temperature schedules\nTEMPERATURES = [1.0, 0.85, 0.7]\n\nprint(f\"Prompts configured:\", flush=True)\nprint(f\"  Reasoning prompts: algebraic, backwards, verification\", flush=True)\nprint(f\"  Coding prompts: computational, casework\", flush=True)\nprint(f\"  Temperatures: {TEMPERATURES}\", flush=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 5: CODE EXECUTION ENGINE (TIR)\n# =============================================================================\n\nSTDLIB = '''\nimport sys; sys.setrecursionlimit(20000)\nimport math, numpy as np\nfrom itertools import *\nfrom collections import *\nfrom functools import lru_cache, reduce\nfrom fractions import Fraction\ntry:\n    from sympy import *\n    from sympy.ntheory import factorint, divisors, totient, isprime, primefactors\n    from sympy.combinatorics import Permutation, PermutationGroup\nexcept: pass\n\ndef C(n, k):\n    if k < 0 or k > n: return 0\n    return math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n\ndef P(n, k):\n    if k < 0 or k > n: return 0\n    return math.factorial(n) // math.factorial(n - k)\n'''\n\ndef execute_code(code: str, timeout: int = 30) -> Tuple[Optional[int], str]:\n    \"\"\"Execute Python code and extract integer answer.\"\"\"\n    if not code:\n        return None, \"\"\n    \n    # Add print snooping if no print\n    has_print = 'print(' in code\n    snoop = '''\n_vars = dict(globals())\nfor _v in ['answer', 'result', 'ans', 'res', 'final', 'output', 'solution', 'total', 'count', 'n', 'ret']:\n    if _v in _vars and _vars[_v] is not None:\n        try: print(int(_vars[_v])); break\n        except: pass\n'''\n    \n    full_code = STDLIB + \"\\n\" + code + (\"\" if has_print else \"\\n\" + snoop)\n    \n    tmp_path = None\n    try:\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n            f.write(full_code)\n            f.flush()\n            tmp_path = f.name\n        \n        result = subprocess.run(['python', tmp_path], capture_output=True, text=True, timeout=timeout)\n        \n        if result.returncode == 0 and result.stdout.strip():\n            numbers = re.findall(r'-?\\d+', result.stdout)\n            if numbers:\n                val = int(numbers[-1])\n                if 0 <= val <= 99999:\n                    return val, code\n    except subprocess.TimeoutExpired:\n        pass\n    except Exception:\n        pass\n    finally:\n        # Always cleanup temp file\n        if tmp_path and os.path.exists(tmp_path):\n            try:\n                os.unlink(tmp_path)\n            except:\n                pass\n    \n    return None, code\n\nprint(\"TIR engine ready\"); sys.stdout.flush()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 6: ANSWER EXTRACTION\n# =============================================================================\n\ndef extract_boxed(text: str) -> Optional[int]:\n    \"\"\"Extract integer from \\\\boxed{}. Returns int or None.\"\"\"\n    patterns = [r'\\\\boxed\\{(\\d+)\\}', r'boxed\\{(\\d+)\\}', r'\\\\boxed\\s*\\{\\s*(\\d+)\\s*\\}']\n    for pattern in patterns:\n        matches = re.findall(pattern, text)\n        if matches:\n            try:\n                val = int(matches[-1])\n                if 0 <= val <= 99999:\n                    return val\n            except:\n                pass\n    \n    # Fallback patterns\n    patterns2 = [r'answer\\s*(?:is|=|:)\\s*(\\d+)', r'=\\s*(\\d+)\\s*$', r'final answer[:\\s]+(\\d+)']\n    for pattern in patterns2:\n        matches = re.findall(pattern, text[-500:], re.IGNORECASE)\n        if matches:\n            try:\n                val = int(matches[-1])\n                if 0 <= val <= 99999:\n                    return val\n            except:\n                pass\n    \n    return None\n\ndef extract_python_code(text: str) -> Optional[str]:\n    \"\"\"Extract Python code from markdown blocks.\"\"\"\n    patterns = [r'```python\\s*\\n(.*?)```', r'```py\\s*\\n(.*?)```', r'```\\s*\\n(.*?)```']\n    for pattern in patterns:\n        matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n        if matches:\n            return matches[-1].strip()\n    return None\n\nprint(\"Answer extraction ready\"); sys.stdout.flush()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 7: VALUE CLUSTERING (88% error reduction)\n# =============================================================================\n\ndef relative_distance(a: int, b: int) -> float:\n    \"\"\"Relative distance: |a-b| / max(|a|, |b|)\"\"\"\n    if a == b:\n        return 0.0\n    if a == 0 or b == 0:\n        return 1.0\n    return abs(a - b) / max(abs(a), abs(b))\n\ndef value_clustering(answers: List[int], threshold: float = 0.05) -> Tuple[int, float]:\n    \"\"\"Cluster answers by relative proximity.\"\"\"\n    if not answers:\n        return 0, 0.0\n    if len(answers) == 1:\n        return answers[0], 0.5\n    \n    # Union-Find clustering\n    n = len(answers)\n    parent = list(range(n))\n    \n    def find(i):\n        if parent[i] != i:\n            parent[i] = find(parent[i])\n        return parent[i]\n    \n    def union(i, j):\n        pi, pj = find(i), find(j)\n        if pi != pj:\n            parent[pi] = pj\n    \n    for i in range(n):\n        for j in range(i + 1, n):\n            if relative_distance(answers[i], answers[j]) < threshold:\n                union(i, j)\n    \n    clusters = defaultdict(list)\n    for i in range(n):\n        clusters[find(i)].append(answers[i])\n    \n    # Find best cluster\n    best_cluster = None\n    best_score = -1\n    \n    for members in clusters.values():\n        size = len(members)\n        if size > 1:\n            std = statistics.stdev(members) if len(members) > 1 else 0\n            mean = statistics.mean(members)\n            tightness = 1 / (1 + std / (mean + 1))\n        else:\n            tightness = 0.5\n        \n        score = size * math.sqrt(tightness)\n        if score > best_score:\n            best_score = score\n            best_cluster = members\n    \n    if best_cluster is None:\n        best_cluster = [answers[0]]\n    \n    # Basin refinement\n    median_val = int(statistics.median(best_cluster))\n    if len(best_cluster) > 2:\n        sorted_c = sorted(best_cluster)\n        trimmed = sorted_c[1:-1] if len(sorted_c) > 4 else sorted_c\n        mean_val = int(statistics.mean(trimmed))\n    else:\n        mean_val = median_val\n    \n    center = (median_val + mean_val) // 2\n    confidence = len(best_cluster) / len(answers)\n    \n    return center, confidence\n\ndef mad_filter(answers: List[int], threshold: float = 3.0) -> List[int]:\n    \"\"\"Filter outliers using MAD.\"\"\"\n    if len(answers) < 3:\n        return answers\n    \n    median_val = statistics.median(answers)\n    deviations = [abs(a - median_val) for a in answers]\n    mad = statistics.median(deviations)\n    \n    if mad == 0:\n        return answers\n    \n    filtered = [a for a, d in zip(answers, deviations) if d / mad < threshold]\n    return filtered if filtered else answers\n\nprint(\"Value clustering ready\"); sys.stdout.flush()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 8: LOG-WEIGHTED VOTING + CIC\n# =============================================================================\n\ndef log_weighted_vote(counter: Counter, force_answer: bool = False) -> Tuple[int, float]:\n    \"\"\"Log-weighted voting (penalize small trivial answers).\"\"\"\n    if not counter:\n        return (12453 if force_answer else 0, 0.1)\n    \n    modified_counter = Counter()\n    for value, count in counter.items():\n        weight = math.log(1.25 + abs(value)) * count\n        modified_counter[value] = weight\n    \n    total_score = sum(modified_counter.values())\n    score_list = sorted(\n        [(score, counter[value], value) for value, score in modified_counter.items()],\n        key=lambda x: -x[0]\n    )\n    \n    best_score, best_count, best_value = score_list[0]\n    threshold = total_score / (2 + math.log(1 + total_score))\n    is_confident = best_score > max(3, threshold)\n    \n    if len(score_list) == 1:\n        is_confident = True\n    elif len(score_list) > 1 and best_score - score_list[1][0] > 1:\n        is_confident = True\n    \n    confidence = 0.7 if is_confident else 0.3\n    \n    if force_answer:\n        print(f\"    Log-vote: {[(v, f'{s:.1f}', c) for s, c, v in score_list[:5]]}\")\n        sys.stdout.flush()\n    \n    return (best_value, confidence)\n\ndef cic_confidence(answers: List[int], lambda_: float = 0.3, gamma: float = 0.1) -> float:\n    \"\"\"CIC Functional confidence.\"\"\"\n    if not answers:\n        return 0.0\n    if len(answers) == 1:\n        return 0.5\n    \n    _, cluster_conf = value_clustering(answers)\n    phi = cluster_conf\n    \n    counts = Counter(answers)\n    probs = [c / len(answers) for c in counts.values()]\n    h = -sum(p * math.log(p + 1e-10) for p in probs)\n    h_max = math.log(len(answers))\n    h_norm = h / (h_max + 1e-10)\n    \n    most_common_count = counts.most_common(1)[0][1]\n    c = most_common_count / len(answers)\n    \n    F = phi - lambda_ * h_norm + gamma * c\n    confidence = 0.5 + 0.5 * np.clip(F, 0, 1)\n    \n    return confidence\n\nprint(\"Log-weighted voting + CIC ready\"); sys.stdout.flush()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 9: DIVERGENT GENERATION (DUAL-MODEL ROUTING)\n# =============================================================================\n# Routes prompts to specialist models:\n#   - algebraic, backwards, verification → REASONING model\n#   - computational, casework → CODING model\n# =============================================================================\n\n# Stop tokens by model type\nSTOP_TOKENS_DEEPSEEK = [\"<｜end▁of▁sentence｜>\", \"<|endoftext|>\"]\nSTOP_TOKENS_QWEN = [\"```output\", \"```\\nOutput\", \"<|im_end|>\", \"<|endoftext|>\"]\n\n# Reflexion prompts\nREFLEXION_SMALL = \"Are you sure that is the answer? Double-check your work.\"\nREFLEXION_QUICK = \"Have you verified your answer using an alternative method?\"\nREFLEXION_NOBOX = \"Place your final answer in \\\\boxed{}. Answer must be 0-99999.\"\n\ndef format_prompt(messages: List[Dict], is_deepseek: bool) -> str:\n    \"\"\"Format messages for model type.\"\"\"\n    if is_deepseek:\n        parts = [\"<｜begin▁of▁sentence｜>\"]\n        for m in messages:\n            role_tag = \"<｜User｜>\" if m[\"role\"] in [\"system\", \"user\"] else \"<｜Assistant｜>\"\n            parts.append(f\"{role_tag}{m['content']}\")\n        parts.append(\"<｜Assistant｜>\")\n        return \"\".join(parts)\n    else:\n        parts = []\n        for m in messages:\n            parts.append(f\"<|im_start|>{m['role']}\\n{m['content']}<|im_end|>\")\n        parts.append(\"<|im_start|>assistant\\n\")\n        return \"\\n\".join(parts)\n\ndef generate_single(question: str, prompt_type: str, temp: float) -> Optional[int]:\n    \"\"\"Generate solution using the appropriate model.\"\"\"\n    if time.time() >= CUTOFF_TIME:\n        return None\n    \n    # Route to correct model\n    model_key = PROMPT_ROUTING.get(prompt_type, 'reasoning')\n    model = MODELS[model_key]\n    config = MODEL_CONFIGS[model_key]\n    is_deepseek = config.get(\"is_deepseek\", False)\n    \n    # Get prompt text\n    prompt_text = get_prompt(prompt_type, model_key)\n    \n    messages = [\n        {\"role\": \"system\", \"content\": prompt_text},\n        {\"role\": \"user\", \"content\": question}\n    ]\n    \n    full_response = \"\"\n    max_iterations = 2\n    stop_tokens = STOP_TOKENS_DEEPSEEK if is_deepseek else STOP_TOKENS_QWEN\n    \n    for iteration in range(max_iterations):\n        if time.time() >= CUTOFF_TIME:\n            break\n        \n        cur_temp = temp * (0.7 ** iteration)\n        prompt = format_prompt(messages, is_deepseek)\n        \n        params = SamplingParams(\n            temperature=cur_temp,\n            top_p=0.95,\n            max_tokens=8192 if is_deepseek else 6144,\n            stop=stop_tokens,\n        )\n        \n        try:\n            outputs = model.generate([prompt], sampling_params=params)\n            response = outputs[0].outputs[0].text\n        except Exception as e:\n            print(f\"      Gen error ({prompt_type}): {e}\", flush=True)\n            break\n        \n        messages.append({\"role\": \"assistant\", \"content\": response})\n        full_response += response\n        \n        # Extract answer\n        boxed = extract_boxed(response)\n        \n        # Reflexion logic\n        if boxed is None:\n            messages.append({\"role\": \"user\", \"content\": REFLEXION_NOBOX})\n        elif boxed <= 10:\n            messages.append({\"role\": \"user\", \"content\": REFLEXION_SMALL})\n        elif iteration == 0 and len(response) < 1000:\n            messages.append({\"role\": \"user\", \"content\": REFLEXION_QUICK})\n        else:\n            break\n    \n    # Final extraction\n    answer = extract_boxed(full_response)\n    \n    # DeepSeek: check after </think>\n    if answer is None and is_deepseek and \"</think>\" in full_response:\n        answer = extract_boxed(full_response.split(\"</think>\")[-1])\n    \n    # Code execution for computational prompts\n    if answer is None and prompt_type in ['computational', 'casework']:\n        code = extract_python_code(full_response)\n        if code:\n            answer, _ = execute_code(code)\n    \n    return answer\n\ndef divergent_sampling(question: str, max_samples: int = 30) -> List[int]:\n    \"\"\"\n    Dual-model divergent sampling.\n    Routes each prompt type to its specialist model.\n    \"\"\"\n    answers = []\n    \n    # Build task list with model routing\n    tasks = []\n    for temp in TEMPERATURES:\n        for prompt_type in PROMPT_ROUTING.keys():\n            # 2 samples per prompt/temp combo\n            tasks.append((prompt_type, temp))\n            tasks.append((prompt_type, temp))\n    \n    tasks = tasks[:max_samples]\n    \n    # Show routing info\n    if DUAL_MODEL_MODE:\n        print(f\"    DUAL MODE: Routing to specialist models\", flush=True)\n    print(f\"    Generating {len(tasks)} paths...\", flush=True)\n    \n    # Sequential generation\n    for i, (prompt_type, temp) in enumerate(tasks):\n        if time.time() >= CUTOFF_TIME:\n            break\n        \n        ans = generate_single(question, prompt_type, temp)\n        if ans is not None:\n            answers.append(ans)\n        \n        if (i + 1) % 5 == 0:\n            print(f\"      {i+1}/{len(tasks)} done, {len(answers)} valid\", flush=True)\n        \n        # Early exit on high confidence\n        if len(answers) >= 8:\n            counter = Counter(answers)\n            _, conf = log_weighted_vote(counter)\n            if conf >= 0.7:\n                print(f\"    Early exit: conf={conf:.2f}\", flush=True)\n                return answers\n    \n    return answers\n\nprint(f\"Divergent sampling ready (dual-model routing: {DUAL_MODEL_MODE})\", flush=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 10: CONVERGENT SELECTION\n# =============================================================================\n\ndef convergent_selection(answers: List[int]) -> Tuple[int, float]:\n    \"\"\"Combine value clustering + log-weighted voting.\"\"\"\n    if not answers:\n        return 12453, 0.1\n    \n    if len(answers) == 1:\n        return answers[0], 0.5\n    \n    # MAD filter\n    filtered = mad_filter(answers)\n    print(f\"    MAD: {len(answers)} → {len(filtered)}\"); sys.stdout.flush()\n    \n    # Value clustering\n    cluster_center, cluster_conf = value_clustering(filtered)\n    print(f\"    Cluster: {cluster_center}, conf={cluster_conf:.2f}\"); sys.stdout.flush()\n    \n    # Log-weighted voting\n    counter = Counter(filtered)\n    log_answer, log_conf = log_weighted_vote(counter, force_answer=True)\n    print(f\"    Log-vote: {log_answer}, conf={log_conf:.2f}\"); sys.stdout.flush()\n    \n    # CIC confidence\n    cic_conf = cic_confidence(filtered)\n    print(f\"    CIC: {cic_conf:.2f}\"); sys.stdout.flush()\n    \n    # Decision\n    if cluster_conf > 0.8 and cluster_center != log_answer:\n        final_answer = cluster_center\n        final_conf = cluster_conf\n        print(f\"    → Using cluster center\"); sys.stdout.flush()\n    else:\n        final_answer = log_answer\n        final_conf = 0.5 * log_conf + 0.3 * cluster_conf + 0.2 * cic_conf\n    \n    final_answer = max(0, min(99999, final_answer))\n    \n    return final_answer, final_conf\n\nprint(\"Convergent selection ready\"); sys.stdout.flush()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 11: MAIN SOLVER\n# =============================================================================\n\nPROBLEM_COUNT = 0\nSOLVED_IDS = set()\n\ndef solve(question: str, question_id: str) -> int:\n    \"\"\"Ω-AIMO3 Pipeline.\"\"\"\n    global PROBLEM_COUNT\n    PROBLEM_COUNT += 1\n    \n    time_remaining = CUTOFF_TIME - time.time()\n    problems_remaining = max(1, PROBLEMS_EXPECTED - PROBLEM_COUNT + 1)\n    time_per_problem = time_remaining / problems_remaining\n    \n    # Adaptive samples\n    if time_per_problem > 300:\n        max_samples = 30\n    elif time_per_problem > 180:\n        max_samples = 20\n    elif time_per_problem > 60:\n        max_samples = 10\n    else:\n        max_samples = 5\n    \n    print(f\"  Budget: {time_per_problem:.0f}s, samples: {max_samples}\"); sys.stdout.flush()\n    \n    # DIVERGE\n    answers = divergent_sampling(question, max_samples)\n    print(f\"    Got {len(answers)} answers: {Counter(answers).most_common(5)}\"); sys.stdout.flush()\n    \n    if not answers:\n        nums = [int(x) for x in re.findall(r'\\b\\d+\\b', question) if 0 < int(x) < 100000]\n        return nums[0] if nums else 12453\n    \n    # CONVERGE\n    final_answer, confidence = convergent_selection(answers)\n    \n    print(f\"  FINAL: {final_answer} (conf: {confidence:.2f})\"); sys.stdout.flush()\n    \n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    return final_answer\n\nprint(\"Solver ready\"); sys.stdout.flush()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 12: KAGGLE API\n# =============================================================================\n\ndef predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame:\n    \"\"\"AIMO3 API predict function.\"\"\"\n    question_id = id_.item(0)\n    question = problem.item(0)\n    \n    time_left = (CUTOFF_TIME - time.time()) / 60\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Problem {PROBLEM_COUNT + 1} | {question_id} | {time_left:.1f}m remaining\")\n    print(f\"Q: {question[:80]}...\")\n    sys.stdout.flush()\n    \n    answer = solve(question, question_id)\n    SOLVED_IDS.add(question_id)\n    \n    print(f\"ANSWER: {answer}\")\n    print(f\"{'='*60}\")\n    sys.stdout.flush()\n    \n    return pl.DataFrame({\"id\": id_, \"answer\": answer})\n\nprint(\"API ready\"); sys.stdout.flush()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 13: RUN\n# =============================================================================\n\nprint(\"=\"*60, flush=True)\nprint(\"RYANAIMO - Ω-AIMO3 Dual-Model Pipeline\", flush=True)\nprint(\"=\"*60, flush=True)\nprint(f\"Mode: {'DUAL MODEL' if DUAL_MODEL_MODE else 'SINGLE MODEL'}\", flush=True)\nprint(f\"Reasoning: {MODEL_CONFIGS['reasoning']['path']}\", flush=True)\nprint(f\"Coding: {MODEL_CONFIGS['coding']['path']}\", flush=True)\nprint(f\"Pipeline: 5 prompts × 3 temps × 2 samples = 30 max\", flush=True)\nprint(f\"Selection: Value clustering + Log-weighted voting\", flush=True)\nprint(f\"Budget: {TOTAL_BUDGET//3600}h {(TOTAL_BUDGET%3600)//60}m\", flush=True)\nprint(\"=\"*60, flush=True)\n\ntry:\n    import kaggle_evaluation.aimo_3_inference_server\n    HAS_KAGGLE_EVAL = True\nexcept ImportError:\n    print(\"WARNING: kaggle_evaluation not available\", flush=True)\n    HAS_KAGGLE_EVAL = False\n\nif HAS_KAGGLE_EVAL:\n    server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n    \n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        print(\"\\n>>> COMPETITION MODE <<<\", flush=True)\n        server.serve()\n    else:\n        print(\"\\n>>> LOCAL MODE <<<\", flush=True)\n        test_paths = [\n            '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',\n            '/kaggle/input/aimo-validation-aime/aime_problems.csv',\n            'test.csv',\n        ]\n        test_file = None\n        for p in test_paths:\n            if os.path.exists(p):\n                test_file = p\n                break\n        \n        if test_file:\n            print(f\"Using: {test_file}\", flush=True)\n            server.run_local_gateway((test_file,))\n        else:\n            print(\"No test file found\", flush=True)\nelse:\n    print(\"\\n>>> VALIDATION MODE <<<\", flush=True)\n    sample_id = pl.Series([\"test_001\"])\n    sample_problem = pl.Series([\"Find the remainder when 2^100 is divided by 127.\"])\n    result = predict(sample_id, sample_problem)\n    print(f\"\\nResult: {result}\", flush=True)\n\nprint(f\"\\nCompleted in {(time.time() - START_TIME)/60:.1f}m\", flush=True)\nprint(f\"Solved: {len(SOLVED_IDS)} problems\", flush=True)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}