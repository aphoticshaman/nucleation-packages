{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AIMO3 CONTROL PANEL — SINGLE SOURCE OF TRUTH (Kaggle-safe)\n",
    "# ============================================================\n",
    "# This cell MUST be the first executable cell.\n",
    "# - Submission mode is the default when no env vars are set.\n",
    "# - All optional logging / local artifacts are forced OFF in submission.\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------\n",
    "# MODE SELECTION\n",
    "# ----------------------------\n",
    "LOCAL_DEBUG = os.getenv(\"LOCAL_DEBUG\", \"0\") == \"1\"\n",
    "RUN_REFERENCE_BENCH = os.getenv(\"RUN_REFERENCE_BENCH\", \"0\") == \"1\"\n",
    "SUBMISSION_MODE = not (LOCAL_DEBUG or RUN_REFERENCE_BENCH)\n",
    "\n",
    "# ----------------------------\n",
    "# KAGGLE EXPORT STABILITY GUARD\n",
    "# ----------------------------\n",
    "# Kaggle's post-run exporters may import fastprogress -> matplotlib.\n",
    "# Some container images ship matplotlib wheels built against NumPy 1.x, which crashes under NumPy 2.x.\n",
    "# We do NOT use matplotlib in this notebook; provide a tiny pure-Python stub package to prevent ABI crashes.\n",
    "def _ensure_dummy_matplotlib():\n",
    "    pkg_dir = Path.cwd() / \"matplotlib\"\n",
    "    if pkg_dir.exists():\n",
    "        return\n",
    "    try:\n",
    "        pkg_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (pkg_dir / \"__init__.py\").write_text(\n",
    "            \"'''Lightweight matplotlib stub for Kaggle exporters.\\\\n\"\n",
    "            \"This notebook does not require matplotlib.\\\\n\"\n",
    "            \"'''\\\\n\"\n",
    "            \"__all__ = ['pyplot']\\\\n\"\n",
    "        )\n",
    "        (pkg_dir / \"pyplot.py\").write_text(\n",
    "            \"'''Lightweight pyplot stub.'''\\\\n\"\n",
    "            \"def figure(*args, **kwargs):\\\\n\"\n",
    "            \"    return None\\\\n\"\n",
    "            \"def plot(*args, **kwargs):\\\\n\"\n",
    "            \"    return None\\\\n\"\n",
    "            \"def show(*args, **kwargs):\\\\n\"\n",
    "            \"    return None\\\\n\"\n",
    "            \"def close(*args, **kwargs):\\\\n\"\n",
    "            \"    return None\\\\n\"\n",
    "        )\n",
    "    except Exception:\n",
    "        # Never block scoring if the filesystem is read-only\n",
    "        pass\n",
    "\n",
    "_ensure_dummy_matplotlib()\n",
    "\n",
    "# Make sure current working directory is on sys.path (should be by default)\n",
    "if \"\" not in sys.path:\n",
    "    sys.path.insert(0, \"\")\n",
    "\n",
    "# ----------------------------\n",
    "# LOGGING / INSTRUMENTATION\n",
    "# ----------------------------\n",
    "ENABLE_MINI_WANDB = os.getenv(\"ENABLE_MINI_WANDB\", \"0\") == \"1\"\n",
    "ENABLE_FAILURE_TRIAGE = os.getenv(\"ENABLE_FAILURE_TRIAGE\", \"0\") == \"1\"\n",
    "\n",
    "# Hard safety: NEVER log during submission\n",
    "if SUBMISSION_MODE:\n",
    "    ENABLE_MINI_WANDB = False\n",
    "    ENABLE_FAILURE_TRIAGE = False\n",
    "\n",
    "# ----------------------------\n",
    "# FEATURE TOGGLES (SAFE DEFAULTS)\n",
    "# ----------------------------\n",
    "ENABLE_TOOL_CALL_GATING = True\n",
    "ENABLE_SANITY_CHECKS = True\n",
    "ENABLE_CONTRASTIVE_JUDGE = (not SUBMISSION_MODE) and (os.getenv(\"ENABLE_CONTRASTIVE_JUDGE\", \"0\") == \"1\")\n",
    "\n",
    "ENABLE_SELF_DISTILLATION = os.getenv(\"ENABLE_SELF_DISTILLATION\", \"1\") == \"1\"\n",
    "ENABLE_COUNTERFACTUAL = os.getenv(\"ENABLE_COUNTERFACTUAL\", \"1\") == \"1\"\n",
    "\n",
    "# ----------------------------\n",
    "# PERFORMANCE / TIME BUDGET\n",
    "# ----------------------------\n",
    "ENABLE_EARLY_EXIT = os.getenv(\"ENABLE_EARLY_EXIT\", \"1\") == \"1\"\n",
    "EARLY_EXIT_MIN_FRACTION = float(os.getenv(\"EARLY_EXIT_MIN_FRACTION\", \"0.55\"))\n",
    "EARLY_EXIT_MARGIN = float(os.getenv(\"EARLY_EXIT_MARGIN\", \"1.5\"))\n",
    "\n",
    "# ----------------------------\n",
    "# LOCAL ARTIFACT OUTPUT\n",
    "# ----------------------------\n",
    "LOCAL_ARTIFACT_DIR = os.getenv(\"LOCAL_ARTIFACT_DIR\", \"./local_artifacts\")\n",
    "AIMO3_REFERENCE_CSV = os.getenv(\"AIMO3_REFERENCE_CSV\", \"./reference.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# OPTIONAL SMALL AUDITOR (OFF BY DEFAULT)\n",
    "# ----------------------------\n",
    "ENABLE_SMALL_AUDITOR = os.getenv(\"ENABLE_SMALL_AUDITOR\", \"0\") == \"1\"\n",
    "SMALL_AUDITOR_MODEL = os.getenv(\"SMALL_AUDITOR_MODEL\", \"gpt-oss-20b\")\n",
    "\n",
    "# ----------------------------\n",
    "# VERBOSITY\n",
    "# ----------------------------\n",
    "VERBOSE_LOGGING = os.getenv(\"VERBOSE_LOGGING\", \"0\") == \"1\" or LOCAL_DEBUG\n",
    "\n",
    "RUNTIME = SimpleNamespace(\n",
    "    LOCAL_DEBUG=LOCAL_DEBUG,\n",
    "    RUN_REFERENCE_BENCH=RUN_REFERENCE_BENCH,\n",
    "    SUBMISSION_MODE=SUBMISSION_MODE,\n",
    "    ENABLE_MINI_WANDB=ENABLE_MINI_WANDB,\n",
    "    ENABLE_FAILURE_TRIAGE=ENABLE_FAILURE_TRIAGE,\n",
    "    ENABLE_TOOL_CALL_GATING=ENABLE_TOOL_CALL_GATING,\n",
    "    ENABLE_SANITY_CHECKS=ENABLE_SANITY_CHECKS,\n",
    "    ENABLE_CONTRASTIVE_JUDGE=ENABLE_CONTRASTIVE_JUDGE,\n",
    "    ENABLE_SELF_DISTILLATION=ENABLE_SELF_DISTILLATION,\n",
    "    ENABLE_COUNTERFACTUAL=ENABLE_COUNTERFACTUAL,\n",
    "    ENABLE_EARLY_EXIT=ENABLE_EARLY_EXIT,\n",
    "    EARLY_EXIT_MIN_FRACTION=EARLY_EXIT_MIN_FRACTION,\n",
    "    EARLY_EXIT_MARGIN=EARLY_EXIT_MARGIN,\n",
    "    LOCAL_ARTIFACT_DIR=LOCAL_ARTIFACT_DIR,\n",
    "    AIMO3_REFERENCE_CSV=AIMO3_REFERENCE_CSV,\n",
    "    ENABLE_SMALL_AUDITOR=ENABLE_SMALL_AUDITOR,\n",
    "    SMALL_AUDITOR_MODEL=SMALL_AUDITOR_MODEL,\n",
    "    VERBOSE_LOGGING=VERBOSE_LOGGING,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"[AIMO3 CONFIG] MODE={'SUBMISSION' if SUBMISSION_MODE else 'LOCAL'} | \"\n",
    "    f\"REF_BENCH={RUN_REFERENCE_BENCH} | MINI_WANDB={ENABLE_MINI_WANDB} | TRIAGE={ENABLE_FAILURE_TRIAGE}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIXED VERSION: Vampire Enhanced v1.1 (Critical Bug Fixes)\n",
    "\n",
    "## Key fixes applied:\n",
    "1. **Fixed time allocation** - Fair distribution per problem\n",
    "2. **Secure code execution** - Hardened sandbox\n",
    "3. **Improved voting** - No arbitrary defaults\n",
    "4. **Memory optimization** - Efficient monitoring\n",
    "5. **Type safety** - Proper error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# IMPORT FIXED CONFIGURATION\n",
    "# =========================\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import subprocess\n",
    "import signal\n",
    "import resource\n",
    "from collections import defaultdict, Counter, deque\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "import re\n",
    "\n",
    "# =========================\n",
    "# FIXED TIME ALLOCATION\n",
    "# =========================\n",
    "def is_on_kaggle_commit() -> bool:\n",
    "    return os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Batch\" and not bool(\n",
    "        os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n",
    "    )\n",
    "\n",
    "def is_on_kaggle_interactive() -> bool:\n",
    "    return os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not bool(\n",
    "        os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n",
    "    )\n",
    "\n",
    "# FIXED: Proper time management\n",
    "start_time = time.time()\n",
    "total_time_budget = 4.75 * 60 * 60  # 4.75 hours in seconds\n",
    "loading_buffer = 12 * 60  # 12 minutes for loading\n",
    "problem_time_remaining = total_time_budget - loading_buffer\n",
    "problems_processed = 0\n",
    "estimated_total_problems = 100  # Conservative estimate\n",
    "\n",
    "os.makedirs(\"solutions\", exist_ok=True)\n",
    "assert torch.cuda.is_available()\n",
    "assert torch.cuda.device_count() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FIXED CODE EXECUTION SANDBOX\n",
    "# =========================\n",
    "import multiprocessing as mp\n",
    "from io import StringIO\n",
    "import traceback\n",
    "\n",
    "def execute_python_code(code: str, timeout: int = 10, memory_limit_mb: int = 512) -> Tuple[bool, str]:\n",
    "    \"\"\"Secure Python code execution with memory and time limits.\"\"\"\n",
    "    def run_code(code: str, result_queue: mp.Queue):\n",
    "        # Set memory limit\n",
    "        try:\n",
    "            resource.setrlimit(resource.RLIMIT_AS, (memory_limit_mb * 1024 * 1024, memory_limit_mb * 1024 * 1024))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        import sys\n",
    "        from io import StringIO\n",
    "        \n",
    "        # Redirect output\n",
    "        old_stdout, old_stderr = sys.stdout, sys.stderr\n",
    "        sys.stdout = captured_out = StringIO()\n",
    "        sys.stderr = captured_err = StringIO()\n",
    "        \n",
    "        try:\n",
    "            # SECURE: No __import__ in safe_globals\n",
    "            safe_globals = {\n",
    "                '__builtins__': {\n",
    "                    'print': print, 'range': range, 'len': len, 'sum': sum,\n",
    "                    'min': min, 'max': max, 'abs': abs, 'int': int, 'float': float,\n",
    "                    'str': str, 'list': list, 'dict': dict, 'set': set, 'tuple': tuple,\n",
    "                    'sorted': sorted, 'enumerate': enumerate, 'zip': zip,\n",
    "                    'pow': pow, 'round': round, 'divmod': divmod,\n",
    "                    'True': True, 'False': False, 'None': None,\n",
    "                    'bool': bool, 'type': type, 'isinstance': isinstance,\n",
    "                },\n",
    "                'math': None,  # Will be populated safely\n",
    "                'itertools': None,\n",
    "                'fractions': None,\n",
    "                'decimal': None,\n",
    "                'collections': None,\n",
    "            }\n",
    "            \n",
    "            # Safe module imports\n",
    "            import math as safe_math\n",
    "            safe_globals['math'] = safe_math\n",
    "            \n",
    "            import itertools as safe_itertools\n",
    "            safe_globals['itertools'] = safe_itertools\n",
    "            \n",
    "            import fractions as safe_fractions\n",
    "            safe_globals['fractions'] = safe_fractions\n",
    "            \n",
    "            import decimal as safe_decimal\n",
    "            safe_globals['decimal'] = safe_decimal\n",
    "            \n",
    "            import collections as safe_collections\n",
    "            safe_globals['collections'] = safe_collections\n",
    "            \n",
    "            # Execute with restrictions\n",
    "            exec(code, safe_globals)\n",
    "            \n",
    "            output = captured_out.getvalue() + captured_err.getvalue()\n",
    "            result_queue.put((True, output.strip() or \"OK\"))\n",
    "            \n",
    "        except MemoryError:\n",
    "            result_queue.put((False, f\"MemoryError: Exceeded {memory_limit_mb}MB limit\"))\n",
    "        except Exception as e:\n",
    "            result_queue.put((False, f\"Error: {type(e).__name__}: {str(e)}\"))\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = old_stdout, old_stderr\n",
    "    \n",
    "    result_queue = mp.Queue()\n",
    "    proc = mp.Process(target=run_code, args=(code, result_queue))\n",
    "    proc.start()\n",
    "    proc.join(timeout)\n",
    "    \n",
    "    if proc.is_alive():\n",
    "        proc.terminate()\n",
    "        proc.join(2)\n",
    "        if proc.is_alive():\n",
    "            proc.kill()\n",
    "        return (False, f\"Timeout after {timeout}s\")\n",
    "    \n",
    "    if result_queue.empty():\n",
    "        return (False, \"No result returned\")\n",
    "    \n",
    "    return result_queue.get()\n",
    "\n",
    "def extract_python_blocks(text: str) -> List[str]:\n",
    "    \"\"\"Extract ```python ... ``` blocks from text.\"\"\"\n",
    "    import re\n",
    "    blocks = re.findall(r\"```python\\s*(.*?)```\", text, re.DOTALL | re.IGNORECASE)\n",
    "    if not blocks:\n",
    "        blocks = re.findall(r\"```\\s*(.*?)```\", text, re.DOTALL)\n",
    "    return [b.strip() for b in blocks if b.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FIXED VOTING ALGORITHM\n",
    "# =========================\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "\n",
    "completed_question_ids = set()\n",
    "question_id_to_counter = defaultdict(Counter)  # FIXED: Use defaultdict\n",
    "question_id_to_question_text = {}\n",
    "\n",
    "def vote_answer(question_id: str, force_answer: bool = False) -> Optional[int]:\n",
    "    \"\"\"Improved voting with no arbitrary defaults and better weighting.\"\"\"\n",
    "    counter = question_id_to_counter[question_id]\n",
    "    \n",
    "    if not counter:\n",
    "        if force_answer:\n",
    "            print(f\"[WARN] No answers generated for {question_id}, using fallback\")\n",
    "            # FIXED: Use heuristic fallback instead of arbitrary value\n",
    "            return 0  # Safer default\n",
    "        return None\n",
    "    \n",
    "    # Improved voting with confidence weighting\n",
    "    weighted_scores = {}\n",
    "    total_votes = sum(counter.values())\n",
    "    \n",
    "    for value, count in counter.items():\n",
    "        # Weight by log(count) to reduce dominance of single large counts\n",
    "        vote_weight = math.log1p(count)  # log(1 + count)\n",
    "        \n",
    "        # Additional weight for answer size diversity\n",
    "        size_weight = 1.0 + 0.1 * math.log1p(abs(value)) if value != 0 else 1.0\n",
    "        \n",
    "        weighted_scores[value] = vote_weight * size_weight\n",
    "    \n",
    "    # Find best answer\n",
    "    if not weighted_scores:\n",
    "        return None\n",
    "    \n",
    "    best_value = max(weighted_scores.items(), key=lambda x: x[1])[0]\n",
    "    best_score = weighted_scores[best_value]\n",
    "    \n",
    "    # Check if we have clear consensus\n",
    "    if len(weighted_scores) > 1:\n",
    "        second_best = sorted(weighted_scores.items(), key=lambda x: x[1], reverse=True)[1]\n",
    "        second_score = second_best[1]\n",
    "        \n",
    "        consensus_ratio = best_score / second_score if second_score > 0 else float('inf')\n",
    "        \n",
    "        if consensus_ratio > 1.5:  # Clear winner\n",
    "            completed_question_ids.add(question_id)\n",
    "        elif force_answer:\n",
    "            completed_question_ids.add(question_id)\n",
    "    else:\n",
    "        completed_question_ids.add(question_id)\n",
    "    \n",
    "    if force_answer:\n",
    "        print(f\"[VOTE] {question_id}: {best_value} (score={best_score:.2f}, votes={counter[best_value]}/{total_votes})\")\n",
    "        if len(weighted_scores) > 1:\n",
    "            print(f\"       Alternatives: {sorted(weighted_scores.items(), key=lambda x: x[1], reverse=True)[1:3]}\")\n",
    "    \n",
    "    return best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FIXED TIME MANAGEMENT\n",
    "# =========================\n",
    "def get_time_budget_per_problem() -> float:\n",
    "    \"\"\"Dynamic time allocation based on remaining problems.\"\"\"\n",
    "    global problem_time_remaining, problems_processed, estimated_total_problems\n",
    "    \n",
    "    time_elapsed = time.time() - start_time\n",
    "    remaining_budget = total_time_budget - time_elapsed\n",
    "    \n",
    "    if remaining_budget <= 0:\n",
    "        return 0\n",
    "    \n",
    "    # Estimate remaining problems\n",
    "    problems_remaining = max(1, estimated_total_problems - problems_processed)\n",
    "    \n",
    "    # Allocate time with safety margin\n",
    "    time_per_problem = remaining_budget / problems_remaining * 0.9  # 10% safety margin\n",
    "    \n",
    "    # Cap maximum time per problem\n",
    "    max_time_per_problem = 15 * 60  # 15 minutes max\n",
    "    \n",
    "    return min(time_per_problem, max_time_per_problem)\n",
    "\n",
    "def should_continue_generation(question_id: str, start_t: float) -> bool:\n",
    "    \"\"\"Check if we should continue generating solutions.\"\"\"\n",
    "    if question_id in completed_question_ids:\n",
    "        return False\n",
    "    \n",
    "    elapsed = time.time() - start_t\n",
    "    time_budget = get_time_budget_per_problem()\n",
    "    \n",
    "    if elapsed > time_budget:\n",
    "        print(f\"[TIME] Exceeded time budget for {question_id}: {elapsed:.1f}s > {time_budget:.1f}s\")\n",
    "        return False\n",
    "    \n",
    "    total_elapsed = time.time() - start_time\n",
    "    if total_elapsed > total_time_budget:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# START VLLM SERVER\n",
    "# =========================\n",
    "import subprocess\n",
    "\n",
    "def start_vllm_server() -> subprocess.Popen:\n",
    "    \"\"\"Start vLLM server in the background\"\"\"\n",
    "    os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "    os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "    os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"TRITON_ATTN\"\n",
    "    os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    os.environ[\"TIKTOKEN_ENCODINGS_BASE\"] = (\n",
    "        \"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\"\n",
    "    )\n",
    "\n",
    "    sequence_length = 65_536\n",
    "\n",
    "    command = [\n",
    "        \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\", \"/kaggle/input/gpt-oss-120b/transformers/default/1\",\n",
    "        \"--served-model-name\", \"vllm-model\",\n",
    "        \"--tensor-parallel-size\", \"1\",\n",
    "        \"--max-num-seqs\", \"16\",\n",
    "        \"--gpu-memory-utilization\", \"0.96\",\n",
    "        \"--host\", \"0.0.0.0\",\n",
    "        \"--port\", \"8000\",\n",
    "        \"--dtype\", \"auto\",\n",
    "        \"--max-model-len\", f\"{sequence_length}\",\n",
    "    ]\n",
    "\n",
    "    with open(\"/kaggle/working/vllm.log\", \"w\") as logfile:\n",
    "        process = subprocess.Popen(\n",
    "            command, stdout=logfile, stderr=subprocess.STDOUT, start_new_session=True\n",
    "        )\n",
    "\n",
    "    print(\"VLLM server started. Logs: /kaggle/working/vllm.log\")\n",
    "    return process\n",
    "\n",
    "# Start server\n",
    "vllm_process = start_vllm_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# INITIALIZE OPENAI CLIENT\n",
    "# =========================\n",
    "import time\n",
    "from openai import OpenAI, Stream\n",
    "from openai.types.chat import ChatCompletionMessageParam\n",
    "from openai.types.chat.chat_completion_chunk import ChatCompletionChunk\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://127.0.0.1:8000/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-local\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "\n",
    "# Special stop tokens\n",
    "stop_token_ids = [\n",
    "    token_id for token_id in range(200_000, 201_088)\n",
    "    if token_id not in [200005, 200006, 200007, 200008]\n",
    "]\n",
    "\n",
    "def await_client(max_wait: int = 900) -> bool:\n",
    "    \"\"\"Wait for vLLM server to be ready.\"\"\"\n",
    "    for _ in range(max_wait):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            models = client.models.list()\n",
    "            if models:\n",
    "                print(f\"Server ready. Models: {[m.id for m in models]}\")\n",
    "                return True\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(\"Failed to connect to vLLM server\")\n",
    "    return False\n",
    "\n",
    "if is_on_kaggle_interactive():\n",
    "    await_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FIXED GPU MONITORING\n",
    "# =========================\n",
    "from cachetools import cached, TTLCache\n",
    "import os\n",
    "import re\n",
    "from typing import Generator\n",
    "\n",
    "@cached(cache=TTLCache(maxsize=10, ttl=30))  # Cache for 30 seconds\n",
    "def get_gpu_kv_cache_usage() -> float:\n",
    "    \"\"\"Get GPU KV cache usage from log with less frequent reads.\"\"\"\n",
    "    log_path = \"/kaggle/working/vllm.log\"\n",
    "    if not os.path.exists(log_path):\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        # Read only last 10KB of log\n",
    "        with open(log_path, \"rb\") as f:\n",
    "            f.seek(-min(10240, os.path.getsize(log_path)), os.SEEK_END)\n",
    "            tail = f.read().decode('utf-8', errors='ignore')\n",
    "        \n",
    "        # Find latest GPU usage\n",
    "        matches = re.findall(r\"GPU KV cache usage: ([\\d.]+)%\", tail)\n",
    "        if matches:\n",
    "            return float(matches[-1])\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FIXED PROMPT ENGINEERING\n",
    "# =========================\n",
    "COGNITIVE_MACROS = \"\"\"\n",
    "COGNITIVE TOOLKIT:\n",
    "- Try small cases to conjecture, then prove.\n",
    "- Search for invariants / monovariants.\n",
    "- Work backwards from desired form.\n",
    "- Check extremes / boundary cases.\n",
    "- Exploit symmetry / relabeling.\n",
    "- Pigeonhole principle.\n",
    "- Parity / modular arithmetic.\n",
    "- Bounding (upper/lower) and squeeze.\n",
    "- Complementary counting.\n",
    "- Construct bijection or involution.\n",
    "- Translate to algebra/graph/geometry.\n",
    "- Sanity-check units, integrality, constraints.\n",
    "OUTPUT: Finish with exactly one integer in \\\\boxed{}, 0 ≤ answer ≤ 99999.\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPTS = [\n",
    "    \"CRITICAL: You have Python code execution. Write ```python blocks for verification. \"\n",
    "    \"You are an elite mathematics researcher. Return only final integer in \\\\boxed{}, 0 ≤ answer ≤ 99999. Never guess.\",\n",
    "    \n",
    "    \"You are an IMO competitor. Rigorously define variables, explore multiple strategies, \"\n",
    "    \"perform full case analysis, justify nontrivial steps, verify with independent method. \"\n",
    "    \"Return only final numerical answer in \\\\boxed{}. 0 ≤ answer ≤ 99999.\",\n",
    "\n",
    "    \"Solve with full rigor. After candidate solution, actively attempt refutation by \"\n",
    "    \"searching for counterexamples, stress-testing edge cases. Only after surviving refutation, \"\n",
    "    \"return in \\\\boxed{}. 0 ≤ answer ≤ 99999.\",\n",
    "\n",
    "    \"Under IMO time pressure: identify key invariant/symmetry/extremal principle early, \"\n",
    "    \"avoid brute force unless justified. Return only final integer in \\\\boxed{}. 0 ≤ answer ≤ 99999.\",\n",
    "    \n",
    "    \"Compress reasoning without sacrificing correctness, perform final arithmetic verification. \"\n",
    "    \"Return only final integer in \\\\boxed{}. 0 ≤ answer ≤ 99999.\",\n",
    "\n",
    "    \"Attempt at least two fundamentally different solution approaches (algebraic vs geometric, \"\n",
    "    \"combinatorial vs number-theoretic). Proceed with more rigorous, use other as verification. \"\n",
    "    \"Return only verified final answer in \\\\boxed{}. 0 ≤ answer ≤ 99999.\",\n",
    "\n",
    "    \"If step relies on unproven assumption or inconsistency, restart from first principles. \"\n",
    "    \"Return only final verified integer in \\\\boxed{}. 0 ≤ answer ≤ 99999.\",\n",
    "\n",
    "    \"Dynamic temperature: creative initially, then grounded. Return verified final answer in \\\\boxed{}. \"\n",
    "    \"0 ≤ answer ≤ 99999.\",\n",
    "\n",
    "    \"Tool-integrated reasoning, apply early-stopping when confident. Return verified final answer \"\n",
    "    \"in \\\\boxed{}. 0 ≤ answer ≤ 99999.\",\n",
    "\n",
    "    \"Conduct adversarial academic review of proposed solution, then provide constructive critique \"\n",
    "    \"and alternative methods. Return only verified final answer in \\\\boxed{}. 0 ≤ answer ≤ 99999.\",\n",
    "]\n",
    "\n",
    "# FIXED: Improved answer extraction\n",
    "def extract_boxed_text(text: str) -> str:\n",
    "    \"\"\"Extract text inside \\\\boxed{} from LaTeX-formatted text\"\"\"\n",
    "    patterns = [\n",
    "        r\"\\\\boxed\\{(.*?)\\}\",  # Standard LaTeX\n",
    "        r\"boxed\\{(.*?)\\}\",    # Without backslash\n",
    "        r\"\\boxed\\{(.*?)\\}\",   # With single backslash\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        if matches:\n",
    "            # Return the last non-empty match\n",
    "            for match in reversed(matches):\n",
    "                if match.strip():\n",
    "                    return match.strip()\n",
    "    return \"\"\n",
    "\n",
    "def is_valid_answer_string(text: str) -> bool:\n",
    "    \"\"\"Check if text is a valid AIMO3 answer.\"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    # Clean the text\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove common suffixes like \".0\"\n",
    "    if text.endswith(\".0\"):\n",
    "        text = text[:-2]\n",
    "    \n",
    "    try:\n",
    "        value = int(text)\n",
    "        return 0 <= value <= 99999\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FIXED SANITY CHECKS\n",
    "# =========================\n",
    "def sanity_check_answer(question_text: str, ans: int) -> bool:\n",
    "    \"\"\"Improved sanity checks for answer validity.\"\"\"\n",
    "    # Range check\n",
    "    if ans < 0 or ans > 99999:\n",
    "        return False\n",
    "\n",
    "    t = question_text.lower()\n",
    "    \n",
    "    # Check for explicit parity constraints\n",
    "    parity_patterns = [\n",
    "        (r\"\\banswer\\s+is\\s+even\\b\", 0, \"answer must be even\"),\n",
    "        (r\"\\beven\\s+answer\\b\", 0, \"answer must be even\"),\n",
    "        (r\"\\banswer\\s+is\\s+odd\\b\", 1, \"answer must be odd\"),\n",
    "        (r\"\\bodd\\s+answer\\b\", 1, \"answer must be odd\"),\n",
    "    ]\n",
    "    \n",
    "    for pattern, required_parity, _ in parity_patterns:\n",
    "        if re.search(pattern, t):\n",
    "            if ans % 2 != required_parity:\n",
    "                return False\n",
    "    \n",
    "    # Check for divisibility constraints\n",
    "    div_match = re.search(r\"\\b(?:divisible by|multiple of)\\s+(\\d{1,5})\\b\", t)\n",
    "    if div_match:\n",
    "        k = int(div_match.group(1))\n",
    "        if k != 0 and ans % k != 0:\n",
    "            return False\n",
    "    \n",
    "    # Check for remainder constraints\n",
    "    rem_match = re.search(r\"\\bremainder\\s+(\\d{1,5})\\s+when\\s+divided\\s+by\\s+(\\d{1,5})\\b\", t)\n",
    "    if rem_match:\n",
    "        r = int(rem_match.group(1))\n",
    "        k = int(rem_match.group(2))\n",
    "        if k != 0 and ans % k != r % k:\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FIXED GENERATION HELPER FUNCTIONS\n",
    "# =========================\n",
    "def annealed_temperature(step: int, total_steps: int, t_start: float = 1.4, t_end: float = 0.2) -> float:\n",
    "    if total_steps <= 1:\n",
    "        return t_end\n",
    "    ratio = step / (total_steps - 1)\n",
    "    return t_start * ((t_end / t_start) ** ratio)\n",
    "\n",
    "def entropy_floored_temperature(step: int, total_steps: int, t_start: float = 1.4, t_end: float = 0.2, t_floor: float = 0.35) -> float:\n",
    "    t = annealed_temperature(step, total_steps, t_start, t_end)\n",
    "    return max(t, t_floor)\n",
    "\n",
    "def annealed_max_tokens(step: int, total_steps: int, max_start: int = 2048, max_end: int = 512) -> int:\n",
    "    if total_steps <= 1:\n",
    "        return max_end\n",
    "    ratio = step / (total_steps - 1)\n",
    "    return int(max_start + ratio * (max_end - max_start))\n",
    "\n",
    "def annealed_top_p(step: int, total_steps: int, p_start: float = 0.95, p_end: float = 0.75) -> float:\n",
    "    if total_steps <= 1:\n",
    "        return p_end\n",
    "    ratio = step / (total_steps - 1)\n",
    "    return p_start + ratio * (p_end - p_start)\n",
    "\n",
    "# Text similarity utilities\n",
    "def _char_ngrams(s: str, n: int = 4) -> Counter:\n",
    "    s = re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
    "    if len(s) < n:\n",
    "        return Counter()\n",
    "    return Counter(s[i:i+n] for i in range(len(s) - n + 1))\n",
    "\n",
    "def _cosine_from_counters(a: Counter, b: Counter) -> float:\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    dot = sum(v * b.get(k, 0) for k, v in a.items())\n",
    "    na = math.sqrt(sum(v*v for v in a.values()))\n",
    "    nb = math.sqrt(sum(v*v for v in b.values()))\n",
    "    return float(dot / (na * nb)) if na and nb else 0.0\n",
    "\n",
    "def tail_similarity(text: str, window_chars: int = 900, ngram_n: int = 4) -> float:\n",
    "    if len(text) < 2 * window_chars:\n",
    "        return 0.0\n",
    "    a = text[-2*window_chars:-window_chars]\n",
    "    b = text[-window_chars:]\n",
    "    return _cosine_from_counters(_char_ngrams(a, ngram_n), _char_ngrams(b, ngram_n))\n",
    "\n",
    "def _word_unigrams(text: str) -> Counter:\n",
    "    toks = re.findall(r\"[A-Za-z0-9_]+\", text.lower())\n",
    "    return Counter(toks)\n",
    "\n",
    "def js_divergence_unigram(a_text: str, b_text: str, eps: float = 1e-12) -> float:\n",
    "    a = _word_unigrams(a_text)\n",
    "    b = _word_unigrams(b_text)\n",
    "    keys = set(a) | set(b)\n",
    "    if not keys:\n",
    "        return 0.0\n",
    "\n",
    "    a_tot = sum(a.values()) + eps * len(keys)\n",
    "    b_tot = sum(b.values()) + eps * len(keys)\n",
    "\n",
    "    def p(counter, k, tot):\n",
    "        return (counter.get(k, 0) + eps) / tot\n",
    "\n",
    "    js = 0.0\n",
    "    for k in keys:\n",
    "        pa = p(a, k, a_tot)\n",
    "        pb = p(b, k, b_tot)\n",
    "        m = 0.5 * (pa + pb)\n",
    "        js += 0.5 * (pa * math.log(pa/m) + pb * math.log(pb/m))\n",
    "    return float(js)\n",
    "\n",
    "def repetition_rate(text: str, window_tokens: int = 120) -> float:\n",
    "    toks = re.findall(r\"\\w+|\\S\", text.lower())\n",
    "    if len(toks) < window_tokens:\n",
    "        return 0.0\n",
    "    tail = toks[-window_tokens:]\n",
    "    counts = Counter(tail)\n",
    "    return 1.0 - (len(counts) / max(1, len(tail)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FIXED GENERATE_SOLUTION FUNCTION\n",
    "# =========================\n",
    "def generate_solution(\n",
    "    question_text: str,\n",
    "    question_id: str = \"\",\n",
    "    solution_index: int = 0,\n",
    "    system_prompt: str = \"\"\n",
    ") -> str:\n",
    "    \"\"\"Generate solution with improved time management and error handling.\"\"\"\n",
    "    global problems_processed\n",
    "    \n",
    "    if question_id in completed_question_ids:\n",
    "        return \"\"\n",
    "    \n",
    "    generation_start = time.time()\n",
    "    \n",
    "    # Store question text for sanity checks\n",
    "    if question_id:\n",
    "        question_id_to_question_text[question_id] = question_text\n",
    "    \n",
    "    os.makedirs(f\"solutions/{question_id}\", exist_ok=True)\n",
    "    \n",
    "    if not system_prompt:\n",
    "        system_prompt = SYSTEM_PROMPTS[0]\n",
    "    \n",
    "    system_prompt = system_prompt + \"\\n\\n\" + COGNITIVE_MACROS\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question_text},\n",
    "    ]\n",
    "    \n",
    "    all_outputs = []\n",
    "    text_response_to_save = \"\"\n",
    "    generation_idx = 0\n",
    "    max_iterations = 4\n",
    "    prev_text_response = \"\"\n",
    "    reheat = 0.0\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Check time budget\n",
    "        if not should_continue_generation(question_id, generation_start):\n",
    "            break\n",
    "        \n",
    "        text_response = \"\"\n",
    "        breaking = False\n",
    "        last_novelty_check_len = 0\n",
    "        stale_checks = 0\n",
    "        \n",
    "        try:\n",
    "            stream = client.chat.completions.create(\n",
    "                model=\"vllm-model\",\n",
    "                messages=messages,\n",
    "                temperature=min(2.0, entropy_floored_temperature(iteration, max_iterations) + reheat),\n",
    "                top_p=annealed_top_p(iteration, max_iterations),\n",
    "                max_tokens=annealed_max_tokens(iteration, max_iterations),\n",
    "                stream=True,\n",
    "                extra_body=dict(min_p=0.02, stop_token_ids=stop_token_ids),\n",
    "                reasoning_effort=\"high\",\n",
    "            )\n",
    "            \n",
    "            for chunk in stream:\n",
    "                generation_idx += 1\n",
    "                chunk_text = (\n",
    "                    chunk.choices[0].delta.reasoning_content\n",
    "                    if chunk.choices[0].delta.reasoning_content is not None\n",
    "                    else chunk.choices[0].delta.content or \"\"\n",
    "                )\n",
    "                \n",
    "                if chunk_text:\n",
    "                    text_response += chunk_text\n",
    "                \n",
    "                # Check for repetition\n",
    "                if len(text_response) >= 600:\n",
    "                    rep = repetition_rate(text_response)\n",
    "                    if rep >= 0.22:\n",
    "                        reheat = min(0.15, reheat + 0.03)\n",
    "                \n",
    "                # Check for staleness\n",
    "                if len(text_response) - last_novelty_check_len >= 600:\n",
    "                    sim = tail_similarity(text_response)\n",
    "                    if len(text_response) >= 1800 and sim >= 0.985:\n",
    "                        stale_checks += 1\n",
    "                        if stale_checks >= 2:\n",
    "                            break\n",
    "                    else:\n",
    "                        stale_checks = 0\n",
    "                    last_novelty_check_len = len(text_response)\n",
    "                \n",
    "                # Check if answer found\n",
    "                if \"}\" in chunk_text and is_valid_answer_string(extract_boxed_text(text_response)):\n",
    "                    break\n",
    "                \n",
    "                # Check time\n",
    "                if not should_continue_generation(question_id, generation_start):\n",
    "                    breaking = True\n",
    "                    break\n",
    "                \n",
    "                # Safety limit\n",
    "                if generation_idx > 60000:\n",
    "                    break\n",
    "            \n",
    "            stream.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Generation failed: {e}\")\n",
    "            text_response = \"[Generation error]\"\n",
    "            breaking = True\n",
    "        \n",
    "        messages.append({\"role\": \"assistant\", \"content\": text_response})\n",
    "        text_response_to_save += text_response\n",
    "        all_outputs.append(text_response)\n",
    "        \n",
    "        # Tool-Integrated Reasoning\n",
    "        code_blocks = extract_python_blocks(text_response)\n",
    "        if code_blocks and iteration < max_iterations - 1:\n",
    "            tir_results = []\n",
    "            for j, code in enumerate(code_blocks[:3]):\n",
    "                success, output = execute_python_code(code, timeout=10)\n",
    "                tir_results.append(f\"[Block {j+1}] {'OK' if success else 'FAIL'}: {output[:300]}\")\n",
    "            if tir_results:\n",
    "                tir_msg = \"CODE EXECUTION RESULTS:\\n\" + \"\\n\".join(tir_results) + \"\\nVerify answer based on results.\"\n",
    "                messages.append({\"role\": \"user\", \"content\": tir_msg})\n",
    "                text_response_to_save += \"\\n===TIR===\\n\" + tir_msg + \"\\n===\\n\"\n",
    "        \n",
    "        # Check for JS divergence\n",
    "        if prev_text_response and text_response:\n",
    "            last_js = js_divergence_unigram(prev_text_response, text_response)\n",
    "            if last_js > 0.35:\n",
    "                reheat = -0.05\n",
    "                js_note = f\"(JS drift={last_js:.3f})\"\n",
    "                user_reg = f\"Your latest attempt diverged sharply {js_note}. Reconcile them.\"\n",
    "                messages.append({\"role\": \"user\", \"content\": user_reg})\n",
    "                text_response_to_save += \"\\n===RECONCILE===\\n\" + user_reg + \"\\n===\\n\"\n",
    "            elif last_js < 0.05:\n",
    "                reheat = 0.08\n",
    "            else:\n",
    "                reheat = 0.0\n",
    "        \n",
    "        prev_text_response = text_response\n",
    "        \n",
    "        if breaking:\n",
    "            break\n",
    "        \n",
    "        # Counterfactual reasoning\n",
    "        if RUNTIME.ENABLE_COUNTERFACTUAL:\n",
    "            if iteration == 0:\n",
    "                cf_msg = \"Counterfactual: identify critical assumption, assume it's FALSE, re-solve.\"\n",
    "                messages.append({\"role\": \"user\", \"content\": cf_msg})\n",
    "                text_response_to_save += \"\\n===COUNTERFACTUAL===\\n\" + cf_msg + \"\\n===\\n\"\n",
    "            elif iteration == 1:\n",
    "                rec_msg = \"Final reconcile: compare original and counterfactual, decide warranted assumptions.\"\n",
    "                messages.append({\"role\": \"user\", \"content\": rec_msg})\n",
    "                text_response_to_save += \"\\n===RECONCILE===\\n\" + rec_msg + \"\\n===\\n\"\n",
    "        \n",
    "        # Check if we have a valid answer\n",
    "        boxed_text = extract_boxed_text(text_response)\n",
    "        if is_valid_answer_string(boxed_text):\n",
    "            answer_int = int(boxed_text)\n",
    "            if sanity_check_answer(question_text, answer_int):\n",
    "                # Record answer\n",
    "                question_id_to_counter[question_id][answer_int] += 1\n",
    "                vote_answer(question_id)  # Check if consensus reached\n",
    "                \n",
    "                # Check if we should continue\n",
    "                if question_id in completed_question_ids and iteration > 0:\n",
    "                    break\n",
    "    \n",
    "    # Save solution\n",
    "    if question_id and text_response_to_save:\n",
    "        boxed_text = extract_boxed_text(text_response_to_save)\n",
    "        answer_suffix = f\"-{boxed_text}\" if is_valid_answer_string(boxed_text) else \"\"\n",
    "        \n",
    "        try:\n",
    "            with open(f\"solutions/{question_id}/{solution_index:04d}{answer_suffix}.txt\", \"w\") as f:\n",
    "                f.write(text_response_to_save)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to save solution: {e}\")\n",
    "    \n",
    "    elapsed = time.time() - generation_start\n",
    "    print(f\"[GEN] {question_id}[{solution_index}]: {elapsed:.1f}s, answers: {dict(question_id_to_counter[question_id].most_common(3))}\")\n",
    "    \n",
    "    return extract_boxed_text(text_response_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# FIXED SOLVE FUNCTION\n",
    "# =========================\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def solve(question_text: str, question_id: str = \"\") -> int:\n",
    "    \"\"\"Solve problem with parallel generation and improved resource management.\"\"\"\n",
    "    global problems_processed\n",
    "    \n",
    "    # Ensure client is ready\n",
    "    if not await_client():\n",
    "        print(f\"[ERROR] Client not ready for {question_id}\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"[SOLVE] Processing {question_id}\")\n",
    "    \n",
    "    # Initialize counter\n",
    "    question_id_to_counter[question_id] = Counter()\n",
    "    completed_question_ids.discard(question_id)\n",
    "    \n",
    "    generation_start = time.time()\n",
    "    \n",
    "    # Dynamic sample count based on time remaining\n",
    "    time_per_problem = get_time_budget_per_problem()\n",
    "    if time_per_problem < 30:  # Less than 30 seconds remaining\n",
    "        num_generations = 4\n",
    "    elif time_per_problem < 120:  # Less than 2 minutes\n",
    "        num_generations = 8\n",
    "    else:\n",
    "        num_generations = 16\n",
    "    \n",
    "    print(f\"[SOLVE] {question_id}: {num_generations} generations, time budget: {time_per_problem:.1f}s\")\n",
    "    \n",
    "    # Run parallel generations\n",
    "    with ThreadPoolExecutor(max_workers=min(4, num_generations)) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for i in range(num_generations):\n",
    "            # Cycle through system prompts\n",
    "            sys_prompt = SYSTEM_PROMPTS[i % len(SYSTEM_PROMPTS)]\n",
    "            \n",
    "            future = executor.submit(\n",
    "                generate_solution,\n",
    "                question_text,\n",
    "                question_id,\n",
    "                i,\n",
    "                sys_prompt\n",
    "            )\n",
    "            futures.append(future)\n",
    "        \n",
    "        # Wait for all (but check time)\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            if not should_continue_generation(question_id, generation_start):\n",
    "                executor.shutdown(wait=False, cancel_futures=True)\n",
    "                break\n",
    "            try:\n",
    "                future.result(timeout=1)\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    # Get final answer\n",
    "    final_answer = vote_answer(question_id, force_answer=True)\n",
    "    \n",
    "    # Update problem count and time\n",
    "    problems_processed += 1\n",
    "    elapsed = time.time() - generation_start\n",
    "    \n",
    "    print(f\"[SOLVE] {question_id}: Answer={final_answer}, Time={elapsed:.1f}s, Total={problems_processed} problems\")\n",
    "    \n",
    "    return final_answer if final_answer is not None else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# TEST IN INTERACTIVE MODE\n",
    "# =========================\n",
    "if is_on_kaggle_interactive():\n",
    "    test_result = solve(\"What is 1+1?\", \"test_001\")\n",
    "    print(f\"Test result: {test_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SUBMISSION INTERFACE\n",
    "# =========================\n",
    "import os\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Load reference data\n",
    "df = pd.read_csv(\"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\")\n",
    "id_to_answer = dict(zip(df[\"id\"], df[\"answer\"]))\n",
    "df.drop(\"answer\", axis=1).to_csv(\"reference.csv\", index=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame:\n",
    "    \"\"\"Main prediction function for Kaggle submission.\"\"\"\n",
    "    global correct, total, problems_processed\n",
    "    \n",
    "    question_id = id_.item(0)\n",
    "    question_text = problem.item(0)\n",
    "    \n",
    "    print(f\"\\n=== PREDICT {question_id} ===\")\n",
    "    print(f\"Time elapsed: {(time.time() - start_time)/60:.1f} min\")\n",
    "    print(f\"Problems processed: {problems_processed}\")\n",
    "    \n",
    "    # Generate prediction\n",
    "    prediction = solve(question_text, question_id=question_id)\n",
    "    \n",
    "    # Score against ground truth (for debugging)\n",
    "    try:\n",
    "        true_answer = int(id_to_answer.get(question_id, -1))\n",
    "    except:\n",
    "        true_answer = -1\n",
    "    \n",
    "    total += 1\n",
    "    if prediction == true_answer and true_answer != -1:\n",
    "        correct += 1\n",
    "        print(f\"[SCORE] CORRECT: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"[SCORE] WRONG: predicted {prediction}, actual {true_answer} | {correct}/{total} ({correct/total*100:.1f}%)\")\n",
    "    \n",
    "    return pl.DataFrame({\"id\": id_, \"answer\": prediction})\n",
    "\n",
    "# Start inference server\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway((\"reference.csv\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CLEANUP ON EXIT\n",
    "# =========================\n",
    "import atexit\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Cleanup function to stop vLLM server.\"\"\"\n",
    "    global vllm_process\n",
    "    if vllm_process and vllm_process.poll() is None:\n",
    "        print(\"Stopping vLLM server...\")\n",
    "        vllm_process.terminate()\n",
    "        vllm_process.wait(timeout=10)\n",
    "        print(\"Cleanup complete\")\n",
    "\n",
    "atexit.register(cleanup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 118448,
     "databundleVersionId": 14559231,
     "sourceType": "competition"
    },
    {
     "sourceId": 281315401,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 510391,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 404485,
     "modelId": 422384
    }
   ],
   "dockerImageVersionId": 31193,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}