{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# RYANAIMO - AIMO3 (Three-Model Strategy)\n\n## Pass 1: DeepSeek-R1-Distill-Llama-70B\n- Primary reasoning model with `<think>` tags\n- 94.5% MATH-500, 70.0% AIME 2024\n- Solves all 50 problems first\n\n## Pass 2: Qwen-72B-Math (IMO fine-tuned)\n- Re-tackles unsolved OR confidence < 70%\n- Specifically fine-tuned on olympiad math\n\n## Pass 3: DeepSeek-Coder-V2-Lite (Verifier)\n- Code verification on disagreements\n- Tiebreaker between DeepSeek vs Qwen\n\n**Kaggle Inputs:**\n1. `deepseek-r1` - DeepSeek-R1-Distill-Llama-70B\n2. `qwen-72b-math-nf4` - Your fine-tuned Qwen model (optional)\n3. `deepseek-coder-v2-lite-nf4` - Coder verifier (optional)\n4. `vllm-wheels-py311` - Python 3.11 + torch 2.6.0 compatible wheels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 1: ENVIRONMENT SETUP\n# =============================================================================\nprint(\"CELL1 START\", flush=True)\n\nimport os\nimport sys\nimport subprocess\nimport glob as globmod\n\nos.environ[\"HF_HUB_OFFLINE\"] = \"1\"\nos.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\nos.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\nos.environ[\"VLLM_LOGGING_LEVEL\"] = \"WARNING\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Check Kaggle's torch BEFORE any installs\nimport torch\nprint(f\"Kaggle torch: {torch.__version__}\", flush=True)\nprint(f\"CUDA: {torch.cuda.is_available()}\", flush=True)\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\", flush=True)\n\n# Find wheel directory (try multiple names)\nWHEEL_CANDIDATES = [\n    \"/kaggle/input/vllm-wheels-py311\",\n    \"/kaggle/input/deepseek-offline-wheels\", \n    \"/kaggle/input/aimo3-vllm-wheels\",\n    \"/kaggle/input/vllm-cp311-wheels\",\n]\n\nwheel_dir = None\nfor candidate in WHEEL_CANDIDATES:\n    if os.path.exists(candidate):\n        whl_files = globmod.glob(f\"{candidate}/*.whl\")\n        if whl_files:\n            wheel_dir = candidate\n            print(f\"Found {len(whl_files)} wheels in {candidate}\", flush=True)\n            break\n\nif wheel_dir is None:\n    # List available inputs\n    inputs = os.listdir(\"/kaggle/input\") if os.path.exists(\"/kaggle/input\") else []\n    raise RuntimeError(f\"No wheel dir found! Available inputs: {inputs}\")\n\nprint(f\"Installing from {wheel_dir}...\", flush=True)\n\n# Install vLLM without touching torch (keep Kaggle's)\nsubprocess.run([\n    sys.executable, \"-m\", \"pip\", \"install\",\n    \"--no-index\", f\"--find-links={wheel_dir}\",\n    \"--no-deps\", \"vllm\"\n], timeout=120)\n\n# Install dependencies (skip torch, numpy - use Kaggle's)\ndeps = [\"transformers\", \"accelerate\", \"safetensors\", \"tokenizers\", \n        \"sentencepiece\", \"huggingface_hub\", \"pydantic\", \"msgspec\", \n        \"cloudpickle\", \"einops\", \"filelock\", \"regex\", \"tqdm\", \n        \"packaging\", \"typing_extensions\", \"jinja2\", \"triton\",\n        \"xgrammar\", \"compressed_tensors\", \"outlines_core\", \"lark\"]\n\nfor pkg in deps:\n    subprocess.run([\n        sys.executable, \"-m\", \"pip\", \"install\",\n        \"--no-index\", f\"--find-links={wheel_dir}\",\n        \"--no-deps\", \"--quiet\", pkg\n    ], timeout=60, capture_output=True)\n\nprint(\"Install done\", flush=True)\n\n# Test import\nfrom vllm import LLM, SamplingParams\nprint(\"vLLM imported OK\", flush=True)\nprint(\"CELL1 DONE\", flush=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: IMPORTS + CONSTANTS\n",
    "# =============================================================================\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import gc\n",
    "import statistics\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "START_TIME = time.time()\n",
    "TOTAL_BUDGET = (4 * 60 + 50) * 60  # 4h50m\n",
    "CUTOFF_TIME = START_TIME + TOTAL_BUDGET\n",
    "PROBLEMS_EXPECTED = 50\n",
    "\n",
    "print(f\"Budget: {TOTAL_BUDGET//3600}h {(TOTAL_BUDGET%3600)//60}m\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 3: MODEL PATHS + LOAD/UNLOAD FUNCTIONS\n# =============================================================================\nprint(\"CELL3: SETTING UP MODELS\", flush=True)\n\n# Model 1: DeepSeek-R1-70B (primary reasoning)\nDEEPSEEK_PATHS = [\n    \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-llama-70b/1\",\n]\n\n# Model 2: Qwen-72B-Math (your fine-tuned model)\nQWEN_PATHS = [\n    \"/kaggle/input/qwen-72b-math-nf4\",\n]\n\n# Model 3: DeepSeek-Coder-V2-Lite (verifier)\nCODER_PATHS = [\n    \"/kaggle/input/deepseek-coder-v2-lite-nf4/deepseek-coder-v2-lite-nf4\",\n]\n\ndef find_model(paths):\n    for path in paths:\n        if os.path.exists(path):\n            if os.path.exists(os.path.join(path, \"config.json\")):\n                return path\n            configs = globmod.glob(f\"{path}/**/config.json\", recursive=True)\n            if configs:\n                return os.path.dirname(configs[0])\n    return None\n\n# Find all model paths\nDEEPSEEK_PATH = find_model(DEEPSEEK_PATHS)\nQWEN_PATH = find_model(QWEN_PATHS)\nCODER_PATH = find_model(CODER_PATHS)\n\nprint(f\"DeepSeek: {DEEPSEEK_PATH}\", flush=True)\nprint(f\"Qwen: {QWEN_PATH}\", flush=True)\nprint(f\"Coder: {CODER_PATH}\", flush=True)\n\n# Current loaded model\nCURRENT_MODEL = None\nCURRENT_MODEL_NAME = None\n\ndef unload_model():\n    \"\"\"Unload current model to free VRAM.\"\"\"\n    global CURRENT_MODEL, CURRENT_MODEL_NAME\n    if CURRENT_MODEL is not None:\n        print(f\"  Unloading {CURRENT_MODEL_NAME}...\", flush=True)\n        del CURRENT_MODEL\n        CURRENT_MODEL = None\n        CURRENT_MODEL_NAME = None\n        gc.collect()\n        torch.cuda.empty_cache()\n        time.sleep(2)  # Give CUDA time to release memory\n\ndef load_deepseek():\n    \"\"\"Load DeepSeek-R1-70B with vLLM.\"\"\"\n    global CURRENT_MODEL, CURRENT_MODEL_NAME\n    unload_model()\n    \n    if DEEPSEEK_PATH is None:\n        raise RuntimeError(\"DeepSeek model not found!\")\n    \n    print(f\"  Loading DeepSeek from {DEEPSEEK_PATH}...\", flush=True)\n    CURRENT_MODEL = LLM(\n        model=DEEPSEEK_PATH,\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.90,\n        trust_remote_code=True,\n        max_model_len=8192,\n        enforce_eager=True,\n        dtype=\"bfloat16\",\n    )\n    CURRENT_MODEL_NAME = \"deepseek\"\n    print(\"  DeepSeek loaded!\", flush=True)\n    return CURRENT_MODEL\n\ndef load_qwen():\n    \"\"\"Load Qwen-72B-Math with vLLM.\"\"\"\n    global CURRENT_MODEL, CURRENT_MODEL_NAME\n    unload_model()\n    \n    if QWEN_PATH is None:\n        print(\"  Qwen model not found, skipping\", flush=True)\n        return None\n    \n    print(f\"  Loading Qwen from {QWEN_PATH}...\", flush=True)\n    CURRENT_MODEL = LLM(\n        model=QWEN_PATH,\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.90,\n        trust_remote_code=True,\n        max_model_len=8192,\n        enforce_eager=True,\n        dtype=\"bfloat16\",\n    )\n    CURRENT_MODEL_NAME = \"qwen\"\n    print(\"  Qwen loaded!\", flush=True)\n    return CURRENT_MODEL\n\ndef load_coder():\n    \"\"\"Load DeepSeek-Coder-V2-Lite with transformers (smaller model).\"\"\"\n    global CURRENT_MODEL, CURRENT_MODEL_NAME\n    unload_model()\n    \n    if CODER_PATH is None:\n        print(\"  Coder model not found, skipping\", flush=True)\n        return None\n    \n    print(f\"  Loading Coder from {CODER_PATH}...\", flush=True)\n    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n    \n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16,\n    )\n    \n    tokenizer = AutoTokenizer.from_pretrained(CODER_PATH, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        CODER_PATH,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )\n    CURRENT_MODEL = (model, tokenizer)\n    CURRENT_MODEL_NAME = \"coder\"\n    print(\"  Coder loaded!\", flush=True)\n    return CURRENT_MODEL\n\nprint(\"Model functions ready\", flush=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: PROMPTS (DeepSeek-R1 format)\n",
    "# =============================================================================\n",
    "\n",
    "# DeepSeek-R1 uses <think> tags for reasoning\n",
    "PROMPTS = {\n",
    "    'algebraic': \"\"\"Solve this mathematics olympiad problem using algebraic manipulation.\n",
    "Think step-by-step in <think> tags. Define variables. Check all cases.\n",
    "Verify by substitution. Final integer answer in \\\\boxed{}. Answer: 0-99999.\"\"\",\n",
    "\n",
    "    'backwards': \"\"\"Solve by working backwards from what the answer must look like.\n",
    "In <think> tags, analyze constraints. What form must the answer take?\n",
    "Derive from goal to given. Final integer in \\\\boxed{}. Answer: 0-99999.\"\"\",\n",
    "\n",
    "    'verification': \"\"\"Solve with rigorous verification.\n",
    "In <think> tags, solve then VERIFY: substitute back into all constraints.\n",
    "If verification fails, try different approach. Final integer in \\\\boxed{}.\"\"\",\n",
    "\n",
    "    'computational': \"\"\"Solve by writing Python code.\n",
    "In <think> tags, plan approach. Write clean Python with sympy.\n",
    "Print intermediate results. Final integer in \\\\boxed{}. Answer: 0-99999.\"\"\",\n",
    "\n",
    "    'casework': \"\"\"Solve by systematic case analysis.\n",
    "In <think> tags, enumerate ALL cases exhaustively.\n",
    "Compute each case's contribution. Sum all. Final integer in \\\\boxed{}.\"\"\",\n",
    "}\n",
    "\n",
    "TEMPERATURES = [1.0, 0.85, 0.7]\n",
    "STOP_TOKENS = [\"<｜end▁of▁sentence｜>\", \"<|endoftext|>\", \"</s>\"]\n",
    "\n",
    "print(f\"Prompts: {list(PROMPTS.keys())}\", flush=True)\n",
    "print(f\"Temperatures: {TEMPERATURES}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: ANSWER EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "def extract_boxed(text: str) -> Optional[int]:\n",
    "    \"\"\"Extract integer from \\\\boxed{}.\"\"\"\n",
    "    patterns = [\n",
    "        r'\\\\boxed\\{(\\d+)\\}',\n",
    "        r'boxed\\{(\\d+)\\}',\n",
    "        r'\\\\boxed\\s*\\{\\s*(\\d+)\\s*\\}',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        if matches:\n",
    "            val = int(matches[-1])\n",
    "            if 0 <= val <= 99999:\n",
    "                return val\n",
    "    \n",
    "    # Fallback\n",
    "    fallback = re.findall(r'answer\\s*(?:is|=|:)\\s*(\\d+)', text[-500:], re.I)\n",
    "    if fallback:\n",
    "        val = int(fallback[-1])\n",
    "        if 0 <= val <= 99999:\n",
    "            return val\n",
    "    return None\n",
    "\n",
    "print(\"Extraction ready\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: VALUE CLUSTERING + VOTING\n",
    "# =============================================================================\n",
    "\n",
    "def value_clustering(answers: List[int], threshold: float = 0.05) -> Tuple[int, float]:\n",
    "    \"\"\"Cluster answers by relative proximity.\"\"\"\n",
    "    if not answers:\n",
    "        return 0, 0.0\n",
    "    if len(answers) == 1:\n",
    "        return answers[0], 0.5\n",
    "    \n",
    "    # Simple clustering: group answers within threshold\n",
    "    clusters = defaultdict(list)\n",
    "    for ans in answers:\n",
    "        placed = False\n",
    "        for center in clusters:\n",
    "            if abs(ans - center) / max(abs(center), 1) < threshold:\n",
    "                clusters[center].append(ans)\n",
    "                placed = True\n",
    "                break\n",
    "        if not placed:\n",
    "            clusters[ans].append(ans)\n",
    "    \n",
    "    # Find largest cluster\n",
    "    best_cluster = max(clusters.values(), key=len)\n",
    "    center = int(statistics.median(best_cluster))\n",
    "    confidence = len(best_cluster) / len(answers)\n",
    "    \n",
    "    return center, confidence\n",
    "\n",
    "def log_weighted_vote(answers: List[int]) -> Tuple[int, float]:\n",
    "    \"\"\"Log-weighted voting (penalize trivial small answers).\"\"\"\n",
    "    if not answers:\n",
    "        return 12453, 0.1\n",
    "    \n",
    "    counter = Counter(answers)\n",
    "    weighted = {}\n",
    "    for val, count in counter.items():\n",
    "        weight = math.log(1.25 + abs(val)) * count\n",
    "        weighted[val] = weight\n",
    "    \n",
    "    best = max(weighted, key=weighted.get)\n",
    "    total = sum(weighted.values())\n",
    "    conf = weighted[best] / total if total > 0 else 0.5\n",
    "    \n",
    "    return best, conf\n",
    "\n",
    "def select_answer(answers: List[int]) -> Tuple[int, float]:\n",
    "    \"\"\"Combine clustering + voting.\"\"\"\n",
    "    if not answers:\n",
    "        return 12453, 0.1\n",
    "    if len(answers) == 1:\n",
    "        return answers[0], 0.5\n",
    "    \n",
    "    # Cluster\n",
    "    cluster_ans, cluster_conf = value_clustering(answers)\n",
    "    \n",
    "    # Vote\n",
    "    vote_ans, vote_conf = log_weighted_vote(answers)\n",
    "    \n",
    "    # Prefer cluster if high confidence, else vote\n",
    "    if cluster_conf > 0.7:\n",
    "        return cluster_ans, cluster_conf\n",
    "    return vote_ans, vote_conf\n",
    "\n",
    "print(\"Selection ready\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 7: GENERATION FUNCTIONS (ALL THREE MODELS)\n# =============================================================================\n\ndef format_deepseek_prompt(question: str, system: str) -> str:\n    \"\"\"Format for DeepSeek-R1.\"\"\"\n    return f\"<｜begin▁of▁sentence｜><｜User｜>{system}\\n\\n{question}<｜Assistant｜><think>\\n\"\n\ndef format_qwen_prompt(question: str) -> str:\n    \"\"\"Format for Qwen-Math.\"\"\"\n    return f\"\"\"<|im_start|>system\nYou are a mathematics olympiad expert. Solve problems step-by-step with rigorous reasoning.\n<|im_end|>\n<|im_start|>user\n{question}\n\nProvide your final answer as an integer in \\\\boxed{{}}.\n<|im_end|>\n<|im_start|>assistant\n\"\"\"\n\ndef format_coder_prompt(question: str, answers_to_verify: List[int]) -> str:\n    \"\"\"Format for Coder verification.\"\"\"\n    answers_str = \", \".join(map(str, answers_to_verify))\n    return f\"\"\"You are verifying math competition answers with Python code.\n\nProblem: {question}\n\nCandidate answers to verify: {answers_str}\n\nWrite Python code to check which answer(s) satisfy the problem constraints.\nThen state which answer is correct in \\\\boxed{{}}.\n\"\"\"\n\ndef generate_deepseek(question: str, prompt_type: str, temp: float) -> Optional[int]:\n    \"\"\"Generate with DeepSeek-R1.\"\"\"\n    if CURRENT_MODEL is None or CURRENT_MODEL_NAME != \"deepseek\":\n        return None\n    if time.time() >= CUTOFF_TIME:\n        return None\n    \n    system = PROMPTS[prompt_type]\n    prompt = format_deepseek_prompt(question, system)\n    \n    params = SamplingParams(\n        temperature=temp,\n        top_p=0.95,\n        max_tokens=6144,\n        stop=STOP_TOKENS,\n    )\n    \n    try:\n        outputs = CURRENT_MODEL.generate([prompt], sampling_params=params)\n        response = outputs[0].outputs[0].text\n        if \"</think>\" in response:\n            return extract_boxed(response.split(\"</think>\")[-1])\n        return extract_boxed(response)\n    except Exception as e:\n        print(f\"    DeepSeek error: {e}\", flush=True)\n        return None\n\ndef generate_qwen(question: str, temp: float) -> Optional[int]:\n    \"\"\"Generate with Qwen-Math.\"\"\"\n    if CURRENT_MODEL is None or CURRENT_MODEL_NAME != \"qwen\":\n        return None\n    if time.time() >= CUTOFF_TIME:\n        return None\n    \n    prompt = format_qwen_prompt(question)\n    \n    params = SamplingParams(\n        temperature=temp,\n        top_p=0.95,\n        max_tokens=6144,\n        stop=[\"<|im_end|>\", \"<|endoftext|>\"],\n    )\n    \n    try:\n        outputs = CURRENT_MODEL.generate([prompt], sampling_params=params)\n        response = outputs[0].outputs[0].text\n        return extract_boxed(response)\n    except Exception as e:\n        print(f\"    Qwen error: {e}\", flush=True)\n        return None\n\ndef generate_coder_verify(question: str, answers: List[int]) -> Optional[int]:\n    \"\"\"Verify answers with Coder model.\"\"\"\n    if CURRENT_MODEL is None or CURRENT_MODEL_NAME != \"coder\":\n        return None\n    if time.time() >= CUTOFF_TIME:\n        return None\n    \n    model, tokenizer = CURRENT_MODEL\n    prompt = format_coder_prompt(question, answers)\n    \n    try:\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=4096,\n                temperature=0.7,\n                top_p=0.95,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id,\n            )\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return extract_boxed(response)\n    except Exception as e:\n        print(f\"    Coder error: {e}\", flush=True)\n        return None\n\ndef sample_deepseek(question: str, max_samples: int = 15) -> Tuple[List[int], float]:\n    \"\"\"Sample multiple solutions from DeepSeek.\"\"\"\n    answers = []\n    tasks = [(ptype, temp) for temp in TEMPERATURES for ptype in PROMPTS.keys()]\n    tasks = tasks[:max_samples]\n    \n    for i, (ptype, temp) in enumerate(tasks):\n        if time.time() >= CUTOFF_TIME:\n            break\n        ans = generate_deepseek(question, ptype, temp)\n        if ans is not None:\n            answers.append(ans)\n        \n        # Early consensus\n        if len(answers) >= 5:\n            counter = Counter(answers)\n            if counter.most_common(1)[0][1] >= 4:\n                break\n    \n    if answers:\n        final, conf = select_answer(answers)\n        return answers, conf\n    return [], 0.0\n\ndef sample_qwen(question: str, num_samples: int = 5) -> Tuple[List[int], float]:\n    \"\"\"Sample solutions from Qwen.\"\"\"\n    answers = []\n    temps = [0.7, 0.85, 1.0, 0.9, 0.8][:num_samples]\n    \n    for temp in temps:\n        if time.time() >= CUTOFF_TIME:\n            break\n        ans = generate_qwen(question, temp)\n        if ans is not None:\n            answers.append(ans)\n    \n    if answers:\n        final, conf = select_answer(answers)\n        return answers, conf\n    return [], 0.0\n\nprint(\"Generation functions ready (DeepSeek, Qwen, Coder)\", flush=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 8: THREE-PASS SOLVER\n# =============================================================================\n\n# Storage for results across passes\nRESULTS = {}  # {qid: {\"question\": str, \"deepseek\": (ans, conf, [answers]), \"qwen\": ..., \"final\": int}}\nPROBLEM_COUNT = 0\n\ndef solve_pass1_deepseek(qid: str, question: str, max_samples: int) -> Tuple[int, float, List[int]]:\n    \"\"\"Pass 1: DeepSeek-R1 on all problems.\"\"\"\n    answers, conf = sample_deepseek(question, max_samples)\n    if answers:\n        ans, conf = select_answer(answers)\n        return ans, conf, answers\n    return None, 0.0, []\n\ndef solve_pass2_qwen(qid: str, question: str, num_samples: int = 5) -> Tuple[int, float, List[int]]:\n    \"\"\"Pass 2: Qwen-Math on low-confidence problems.\"\"\"\n    answers, conf = sample_qwen(question, num_samples)\n    if answers:\n        ans, conf = select_answer(answers)\n        return ans, conf, answers\n    return None, 0.0, []\n\ndef solve_pass3_verify(qid: str, question: str, candidates: List[int]) -> Optional[int]:\n    \"\"\"Pass 3: Coder verification on disagreements.\"\"\"\n    if not candidates or len(set(candidates)) == 1:\n        return candidates[0] if candidates else None\n    return generate_coder_verify(question, list(set(candidates)))\n\ndef combine_results(qid: str) -> int:\n    \"\"\"Combine results from all passes.\"\"\"\n    r = RESULTS.get(qid, {})\n    \n    ds_ans, ds_conf, ds_all = r.get(\"deepseek\", (None, 0, []))\n    qw_ans, qw_conf, qw_all = r.get(\"qwen\", (None, 0, []))\n    coder_ans = r.get(\"coder\", None)\n    \n    # If coder verified, trust it\n    if coder_ans is not None:\n        return coder_ans\n    \n    # If both models agree, high confidence\n    if ds_ans is not None and qw_ans is not None and ds_ans == qw_ans:\n        return ds_ans\n    \n    # If only DeepSeek, use it\n    if qw_ans is None:\n        return ds_ans if ds_ans is not None else 12453\n    \n    # If DeepSeek high confidence, trust it\n    if ds_conf > 0.7:\n        return ds_ans\n    \n    # If Qwen high confidence, trust it\n    if qw_conf > 0.7:\n        return qw_ans\n    \n    # Combine all answers and vote\n    all_answers = ds_all + qw_all\n    if all_answers:\n        final, _ = select_answer(all_answers)\n        return final\n    \n    return ds_ans if ds_ans is not None else (qw_ans if qw_ans is not None else 12453)\n\ndef solve_single(qid: str, question: str) -> int:\n    \"\"\"Single-pass solver (called during Pass 1).\"\"\"\n    global PROBLEM_COUNT\n    PROBLEM_COUNT += 1\n    \n    time_left = CUTOFF_TIME - time.time()\n    problems_left = max(1, PROBLEMS_EXPECTED - PROBLEM_COUNT + 1)\n    time_per = time_left / problems_left\n    \n    # Adaptive samples\n    if time_per > 300:\n        max_samples = 20\n    elif time_per > 180:\n        max_samples = 15\n    elif time_per > 60:\n        max_samples = 10\n    else:\n        max_samples = 5\n    \n    print(f\"  Time/problem: {time_per:.0f}s, samples: {max_samples}\", flush=True)\n    \n    # Pass 1: DeepSeek\n    ans, conf, answers = solve_pass1_deepseek(qid, question, max_samples)\n    \n    # Store result\n    RESULTS[qid] = {\n        \"question\": question,\n        \"deepseek\": (ans, conf, answers),\n        \"qwen\": (None, 0, []),\n        \"coder\": None,\n        \"final\": ans if ans else 12453,\n    }\n    \n    print(f\"  DeepSeek: {ans} (conf: {conf:.2f}, n={len(answers)})\", flush=True)\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return ans if ans else 12453\n\ndef run_pass2():\n    \"\"\"Run Pass 2: Qwen on low-confidence problems.\"\"\"\n    if QWEN_PATH is None:\n        print(\"Qwen not available, skipping Pass 2\", flush=True)\n        return\n    \n    # Find problems needing Pass 2\n    needs_pass2 = []\n    for qid, r in RESULTS.items():\n        ds_ans, ds_conf, _ = r[\"deepseek\"]\n        if ds_conf < 0.70 or ds_ans is None:\n            needs_pass2.append(qid)\n    \n    if not needs_pass2:\n        print(\"All problems high confidence, skipping Pass 2\", flush=True)\n        return\n    \n    print(f\"\\n{'='*60}\", flush=True)\n    print(f\"PASS 2: Qwen on {len(needs_pass2)} low-confidence problems\", flush=True)\n    print(f\"{'='*60}\", flush=True)\n    \n    load_qwen()\n    \n    for i, qid in enumerate(needs_pass2):\n        if time.time() >= CUTOFF_TIME - 120:  # Reserve 2min for finalization\n            print(f\"Time limit approaching, stopping Pass 2\", flush=True)\n            break\n        \n        question = RESULTS[qid][\"question\"]\n        print(f\"\\nPass2 [{i+1}/{len(needs_pass2)}] {qid}\", flush=True)\n        \n        ans, conf, answers = solve_pass2_qwen(qid, question, num_samples=5)\n        RESULTS[qid][\"qwen\"] = (ans, conf, answers)\n        \n        print(f\"  Qwen: {ans} (conf: {conf:.2f})\", flush=True)\n    \n    unload_model()\n\ndef run_pass3():\n    \"\"\"Run Pass 3: Coder verification on disagreements.\"\"\"\n    if CODER_PATH is None:\n        print(\"Coder not available, skipping Pass 3\", flush=True)\n        return\n    \n    # Find disagreements\n    disagreements = []\n    for qid, r in RESULTS.items():\n        ds_ans, _, _ = r[\"deepseek\"]\n        qw_ans, _, _ = r[\"qwen\"]\n        if ds_ans is not None and qw_ans is not None and ds_ans != qw_ans:\n            disagreements.append(qid)\n    \n    if not disagreements:\n        print(\"No disagreements, skipping Pass 3\", flush=True)\n        return\n    \n    print(f\"\\n{'='*60}\", flush=True)\n    print(f\"PASS 3: Coder verification on {len(disagreements)} disagreements\", flush=True)\n    print(f\"{'='*60}\", flush=True)\n    \n    load_coder()\n    \n    for i, qid in enumerate(disagreements):\n        if time.time() >= CUTOFF_TIME - 60:\n            print(f\"Time limit approaching, stopping Pass 3\", flush=True)\n            break\n        \n        r = RESULTS[qid]\n        question = r[\"question\"]\n        ds_ans, _, _ = r[\"deepseek\"]\n        qw_ans, _, _ = r[\"qwen\"]\n        \n        print(f\"\\nPass3 [{i+1}/{len(disagreements)}] {qid}: DS={ds_ans} vs QW={qw_ans}\", flush=True)\n        \n        verified = solve_pass3_verify(qid, question, [ds_ans, qw_ans])\n        RESULTS[qid][\"coder\"] = verified\n        \n        print(f\"  Coder verified: {verified}\", flush=True)\n    \n    unload_model()\n\ndef finalize_results():\n    \"\"\"Compute final answers for all problems.\"\"\"\n    print(f\"\\n{'='*60}\", flush=True)\n    print(\"FINALIZING RESULTS\", flush=True)\n    print(f\"{'='*60}\", flush=True)\n    \n    for qid in RESULTS:\n        RESULTS[qid][\"final\"] = combine_results(qid)\n        ds_ans, ds_conf, _ = RESULTS[qid][\"deepseek\"]\n        qw_ans, qw_conf, _ = RESULTS[qid][\"qwen\"]\n        coder = RESULTS[qid][\"coder\"]\n        final = RESULTS[qid][\"final\"]\n        print(f\"{qid}: DS={ds_ans}({ds_conf:.2f}) QW={qw_ans}({qw_conf:.2f}) C={coder} -> {final}\", flush=True)\n\nprint(\"Three-pass solver ready\", flush=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 9: KAGGLE API (Three-Pass)\n# =============================================================================\n\n# Track all problems for multi-pass\nALL_PROBLEMS = []\nPASS1_COMPLETE = False\n\ndef predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame:\n    \"\"\"Called by Kaggle for each problem. During Pass 1, collect all & solve with DeepSeek.\"\"\"\n    global PASS1_COMPLETE\n    \n    qid = id_.item(0)\n    question = problem.item(0)\n    \n    time_left = (CUTOFF_TIME - time.time()) / 60\n    \n    print(f\"\\n{'='*60}\", flush=True)\n    print(f\"Problem {PROBLEM_COUNT + 1} | {qid} | {time_left:.1f}m left\", flush=True)\n    print(f\"Q: {question[:100]}...\", flush=True)\n    \n    # Store problem for later passes\n    ALL_PROBLEMS.append((qid, question))\n    \n    # Pass 1: Solve with DeepSeek\n    answer = solve_single(qid, question)\n    \n    print(f\"ANSWER (Pass1): {answer}\", flush=True)\n    print(f\"{'='*60}\", flush=True)\n    \n    return pl.DataFrame({\"id\": id_, \"answer\": answer})\n\ndef run_multi_pass():\n    \"\"\"Run Pass 2 and Pass 3 after all problems collected.\"\"\"\n    global PASS1_COMPLETE\n    \n    if PASS1_COMPLETE:\n        return\n    PASS1_COMPLETE = True\n    \n    print(f\"\\n{'#'*60}\", flush=True)\n    print(\"PASS 1 COMPLETE - Starting multi-pass refinement\", flush=True)\n    print(f\"{'#'*60}\", flush=True)\n    \n    # Pass 2: Qwen on low-confidence\n    run_pass2()\n    \n    # Pass 3: Coder verification\n    run_pass3()\n    \n    # Finalize\n    finalize_results()\n    \n    # Update any answers that changed\n    print(f\"\\n{'='*60}\", flush=True)\n    print(\"FINAL ANSWERS:\", flush=True)\n    for qid in RESULTS:\n        print(f\"  {qid}: {RESULTS[qid]['final']}\", flush=True)\n\nprint(\"Kaggle API ready (three-pass)\", flush=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 10: RUN (Three-Pass Strategy)\n# =============================================================================\n\nprint(\"=\"*60, flush=True)\nprint(\"RYANAIMO - AIMO3 (Three-Model Strategy)\", flush=True)\nprint(\"=\"*60, flush=True)\nprint(f\"Pass 1: DeepSeek-R1-70B @ {DEEPSEEK_PATH}\", flush=True)\nprint(f\"Pass 2: Qwen-72B-Math @ {QWEN_PATH}\", flush=True)\nprint(f\"Pass 3: DeepSeek-Coder @ {CODER_PATH}\", flush=True)\nprint(\"=\"*60, flush=True)\nprint(\"Strategy:\", flush=True)\nprint(\"  1. DeepSeek solves all 50 problems (5 prompts x 3 temps)\", flush=True)\nprint(\"  2. Qwen re-tackles conf < 70%\", flush=True)\nprint(\"  3. Coder verifies disagreements\", flush=True)\nprint(\"=\"*60, flush=True)\n\n# Load primary model (DeepSeek) for Pass 1\nprint(\"\\nLoading DeepSeek for Pass 1...\", flush=True)\nload_deepseek()\n\ntry:\n    import kaggle_evaluation.aimo_3_inference_server\n    server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n    \n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        print(\">>> COMPETITION MODE <<<\", flush=True)\n        server.serve()\n        # After all problems served, run Pass 2 and 3\n        run_multi_pass()\n    else:\n        print(\">>> LOCAL TEST <<<\", flush=True)\n        test_file = '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv'\n        if os.path.exists(test_file):\n            server.run_local_gateway((test_file,))\n            run_multi_pass()\n        else:\n            # Manual test with single problem\n            print(\"No test file, running manual test...\", flush=True)\n            sample = pl.DataFrame({\n                \"id\": [\"test\"], \n                \"problem\": [\"Find the remainder when 2^100 is divided by 127.\"]\n            })\n            result = predict(sample[\"id\"], sample[\"problem\"])\n            print(result)\n            run_multi_pass()\n\nexcept ImportError:\n    print(\">>> VALIDATION MODE (no kaggle_evaluation) <<<\", flush=True)\n    sample = pl.DataFrame({\n        \"id\": [\"test\"], \n        \"problem\": [\"Find the remainder when 2^100 is divided by 127.\"]\n    })\n    result = predict(sample[\"id\"], sample[\"problem\"])\n    print(result)\n    run_multi_pass()\n\nprint(f\"\\nTotal time: {(time.time() - START_TIME)/60:.1f}m\", flush=True)\nprint(\"Done!\", flush=True)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}