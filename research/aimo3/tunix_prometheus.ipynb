{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PROMETHEUS + Tunix: Teaching Gemma to Show Its Work\n",
    "\n",
    "## Novel Approach: CIC-Guided Reasoning Traces\n",
    "\n",
    "**Key Insights from PROMETHEUS Protocol:**\n",
    "1. **NCD on Traces** - Reward traces that compress similarly to known-good patterns\n",
    "2. **Basin Center Training** - Converge toward canonical reasoning per domain\n",
    "3. **Kolmogorov Regularization** - Shorter traces (given correctness) = higher reward\n",
    "4. **Multi-Domain Focus** - Creative, summarization, science (NOT just math)\n",
    "\n",
    "**Competition Requirements:**\n",
    "- Model: Gemma2 2B or Gemma3 1B\n",
    "- Output: `<reasoning>trace</reasoning><answer>answer</answer>`\n",
    "- Hardware: TPU v5e-8 (9hr session, 20hr/week)\n",
    "- Framework: Tunix (JAX-native GRPO)\n",
    "\n",
    "**Implementation based on:** windmaple's starter notebook (163 upvotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: SETUP & INSTALLATION\n",
    "# =============================================================================\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q kagglehub\n",
    "!pip install -q ipywidgets\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q transformers\n",
    "!pip install -q grain\n",
    "!pip install \"google-tunix[prod]==0.1.3\"\n",
    "!pip uninstall -q -y flax\n",
    "!pip install -U flax\n",
    "!pip install -q datasets\n",
    "\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: IMPORTS\n",
    "# =============================================================================\n",
    "import functools\n",
    "import gc\n",
    "import os\n",
    "from pprint import pprint\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "from pathlib import Path\n",
    "import qwix\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm.auto import tqdm\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma3 import params\n",
    "from tunix.models.gemma3 import model\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.sft import metrics_logger\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Config\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 3: HYPERPARAMETERS (\"Efficiency Build\" for TPU v5e-8)\n# =============================================================================\n\n# ====== Data ======\nTRAIN_DATA_DIR = \"./data/train\"\nTEST_DATA_DIR = \"./data/test\"\nTRAIN_FRACTION = 1.0\n\n# ====== LoRA ======\nRANK = 32  # Reduced from 64 for memory\nALPHA = 32.0  # Reduced from 64 for memory\n\n# ====== Sharding ======\nMESH = [(1, 4), (\"fsdp\", \"tp\")]\n\n# ====== GRPO Generation ======\nMAX_PROMPT_LENGTH = 256\nTOTAL_GENERATION_STEPS = 512\n\n# ====== TEMPERATURE ANNEALING (PROMETHEUS Insight) ======\n# Start HOT for exploration, cool down as we converge\n# \"pump the brakes harder and harder\"\nTEMP_START = 1.2       # Aggressive exploration initially\nTEMP_END = 0.5         # Stable exploitation at end\nTEMP_DECAY = \"cosine\"  # cosine, linear, or step\n\n# Current temp (will be updated during training)\nTEMPERATURE = TEMP_START\n\nTOP_P = 1.0\nTOP_K = 50\nNUM_GENERATIONS = 4  # G in GRPO algorithm\n\n# ====== GRPO Config ======\nNUM_ITERATIONS = 1\nBETA = 0.08  # KL divergence penalty\nEPSILON = 0.2  # Clipping for stable updates\n\n# ====== Training ======\nTRAIN_MICRO_BATCH_SIZE = 2  # Reduced from 4 for memory\nNUM_BATCHES = 3738\nNUM_TEST_BATCHES = 100\nEVAL_EVERY_N_STEPS = 10\nNUM_EPOCHS = 1\n\nMAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n\n# ====== AdamW + Warmup + Cosine Scheduler ======\nLEARNING_RATE = 3e-6\nB1 = 0.9\nB2 = 0.99\nWEIGHT_DECAY = 0.1\nWARMUP_STEPS = 0.1 * MAX_STEPS\nMAX_GRAD_NORM = 0.1  # Critical for KL stability\n\n# ====== Checkpointing ======\nINTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\nCKPT_DIR = \"/tmp/content/ckpts/\"\nSAVE_INTERVAL_STEPS = 500\nMAX_TO_KEEP = 4\n\n# ====== Inference ======\nGENERATION_CONFIGS = {\n    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n}\n\n\n# ====== TEMPERATURE SCHEDULE FUNCTION ======\ndef get_temperature(step, max_steps):\n    \"\"\"\n    Dynamic temperature annealing.\n    Start hot (exploration) -> cool down (exploitation)\n    \"\"\"\n    progress = step / max_steps\n    \n    if TEMP_DECAY == \"cosine\":\n        # Cosine decay: smooth transition\n        temp = TEMP_END + 0.5 * (TEMP_START - TEMP_END) * (1 + np.cos(np.pi * progress))\n    elif TEMP_DECAY == \"linear\":\n        # Linear decay\n        temp = TEMP_START - (TEMP_START - TEMP_END) * progress\n    elif TEMP_DECAY == \"step\":\n        # Step decay: drop at 33% and 66%\n        if progress < 0.33:\n            temp = TEMP_START\n        elif progress < 0.66:\n            temp = (TEMP_START + TEMP_END) / 2\n        else:\n            temp = TEMP_END\n    else:\n        temp = TEMP_START\n    \n    return max(TEMP_END, temp)\n\n\nprint(f\"Config: RANK={RANK}, ALPHA={ALPHA}, LR={LEARNING_RATE}\")\nprint(f\"Training: {MAX_STEPS} steps, batch={TRAIN_MICRO_BATCH_SIZE}\")\nprint(f\"\\nðŸ”¥ TEMPERATURE ANNEALING:\")\nprint(f\"   Start: {TEMP_START} (HOT - exploration)\")\nprint(f\"   End:   {TEMP_END} (COOL - exploitation)\")\nprint(f\"   Decay: {TEMP_DECAY}\")\nprint(f\"\\n   Step 0:    temp={get_temperature(0, MAX_STEPS):.2f}\")\nprint(f\"   Step 50%:  temp={get_temperature(MAX_STEPS//2, MAX_STEPS):.2f}\")\nprint(f\"   Step 100%: temp={get_temperature(MAX_STEPS, MAX_STEPS):.2f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: TEMPLATE & SPECIAL TOKENS (Competition Format)\n",
    "# =============================================================================\n",
    "\n",
    "# Required format tags (locked by competition)\n",
    "reasoning_start = \"<reasoning>\"\n",
    "reasoning_end = \"</reasoning>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\"\n",
    "\n",
    "# PROMETHEUS Insight: Multi-domain system prompt\n",
    "# NOT just math - creative, summarization, science\n",
    "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\n",
    "provide your reasoning. Place it between {reasoning_start} and \\\n",
    "{reasoning_end}. Then, provide the final answer between {solution_start} \\\n",
    "and {solution_end}.\"\"\"\n",
    "\n",
    "# Gemma3 chat template\n",
    "TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{system_prompt}\n",
    "\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "\n",
    "print(\"Template configured.\")\n",
    "print(f\"Format: {reasoning_start}...{reasoning_end} then {solution_start}...{solution_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 5: DATA LOADING (Real User Tasks - PROMETHEUS Insight)\n# =============================================================================\n# Key insight: Train on what users ACTUALLY ask LLMs\n# NOT academic benchmarks - real conversations\n#\n# Datasets:\n# - OpenAssistant (oasst1) - Human-written conversations\n# - WildChat - Organic ChatGPT queries\n# - ShareGPT/Vicuna - Real user interactions\n# - Dolly - Databricks instruction dataset\n\ndef get_real_user_dataset(max_samples=10000) -> grain.MapDataset:\n    \"\"\"Load real user conversation datasets.\"\"\"\n    \n    all_questions = []\n    \n    # 1. OpenAssistant (high quality human conversations)\n    print(\"Loading OpenAssistant...\")\n    try:\n        oasst = load_dataset(\"OpenAssistant/oasst1\", split=\"train\")\n        # Filter for initial user messages (prompter turns)\n        for item in oasst:\n            if item.get(\"role\") == \"prompter\" and item.get(\"parent_id\") is None:\n                text = item.get(\"text\", \"\").strip()\n                if 20 < len(text) < 500:  # Reasonable length\n                    all_questions.append({\n                        \"question\": text,\n                        \"source\": \"oasst\",\n                        \"answer\": None  # No ground truth for open-ended\n                    })\n        print(f\"  -> {len([q for q in all_questions if q['source']=='oasst'])} from OpenAssistant\")\n    except Exception as e:\n        print(f\"  -> OpenAssistant failed: {e}\")\n    \n    # 2. Dolly (Databricks instructions)\n    print(\"Loading Dolly...\")\n    try:\n        dolly = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n        for item in dolly:\n            instruction = item.get(\"instruction\", \"\").strip()\n            context = item.get(\"context\", \"\").strip()\n            if context:\n                question = f\"{instruction}\\n\\nContext: {context}\"\n            else:\n                question = instruction\n            if 20 < len(question) < 500:\n                all_questions.append({\n                    \"question\": question,\n                    \"source\": \"dolly\",\n                    \"answer\": None\n                })\n        print(f\"  -> {len([q for q in all_questions if q['source']=='dolly'])} from Dolly\")\n    except Exception as e:\n        print(f\"  -> Dolly failed: {e}\")\n    \n    # 3. Alpaca (Stanford instruction dataset)\n    print(\"Loading Alpaca...\")\n    try:\n        alpaca = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n        for item in alpaca:\n            instruction = item.get(\"instruction\", \"\").strip()\n            inp = item.get(\"input\", \"\").strip()\n            if inp:\n                question = f\"{instruction}\\n\\nInput: {inp}\"\n            else:\n                question = instruction\n            if 20 < len(question) < 500:\n                all_questions.append({\n                    \"question\": question,\n                    \"source\": \"alpaca\",\n                    \"answer\": None\n                })\n        print(f\"  -> {len([q for q in all_questions if q['source']=='alpaca'])} from Alpaca\")\n    except Exception as e:\n        print(f\"  -> Alpaca failed: {e}\")\n    \n    # 4. FLAN (diverse tasks)\n    print(\"Loading FLAN subset...\")\n    try:\n        flan = load_dataset(\"Muennighoff/flan\", split=\"train\", streaming=True)\n        flan_count = 0\n        for item in flan:\n            if flan_count >= 5000:  # Limit FLAN samples\n                break\n            question = item.get(\"inputs\", \"\").strip()\n            if 20 < len(question) < 500:\n                all_questions.append({\n                    \"question\": question,\n                    \"source\": \"flan\",\n                    \"answer\": None\n                })\n                flan_count += 1\n        print(f\"  -> {flan_count} from FLAN\")\n    except Exception as e:\n        print(f\"  -> FLAN failed: {e}\")\n    \n    # Shuffle and limit\n    random.shuffle(all_questions)\n    all_questions = all_questions[:max_samples]\n    \n    print(f\"\\nTotal: {len(all_questions)} real user questions\")\n    \n    # Show distribution\n    sources = {}\n    for q in all_questions:\n        src = q[\"source\"]\n        sources[src] = sources.get(src, 0) + 1\n    print(f\"Distribution: {sources}\")\n    \n    # Convert to grain dataset\n    dataset = (\n        grain.MapDataset.source(all_questions)\n        .shuffle(seed=42)\n        .map(\n            lambda x: {\n                \"prompts\": TEMPLATE.format(\n                    system_prompt=SYSTEM_PROMPT,\n                    question=x[\"question\"],\n                ),\n                \"question\": x[\"question\"],\n                \"answer\": x[\"answer\"],  # None for open-ended\n                \"source\": x[\"source\"],\n            }\n        )\n    )\n    return dataset\n\n\n# Load datasets\nprint(\"Loading REAL USER datasets...\")\nprint(\"=\" * 50)\n\nfull_dataset = get_real_user_dataset(max_samples=NUM_BATCHES * TRAIN_MICRO_BATCH_SIZE * 2)\n\n# Split into train/test\ndataset = full_dataset.batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_BATCHES]\n\nif TRAIN_FRACTION == 1.0:\n    train_dataset = dataset.repeat(NUM_EPOCHS)\n    val_dataset = None\nelse:\n    train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)]\n    train_dataset = train_dataset.repeat(NUM_EPOCHS)\n    val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n\n# Use last 10% as test\ntest_start = int(len(dataset) * 0.9)\ntest_dataset = full_dataset.batch(TRAIN_MICRO_BATCH_SIZE)[test_start:test_start + NUM_TEST_BATCHES]\n\nprint(f\"\\nTrain batches: {len(train_dataset)}\")\nprint(f\"Test batches: {len(test_dataset)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: LOAD MODEL (Gemma3 1B-IT)\n",
    "# =============================================================================\n",
    "\n",
    "def show_hbm_usage():\n",
    "    \"\"\"Display memory usage per device.\"\"\"\n",
    "    fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
    "    for d in jax.local_devices():\n",
    "        stats = d.memory_stats()\n",
    "        used = stats[\"bytes_in_use\"]\n",
    "        limit = stats[\"bytes_limit\"]\n",
    "        print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")\n",
    "\n",
    "\n",
    "# Kaggle authentication\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "def auto_login():\n",
    "    try:\n",
    "        user_secrets = UserSecretsClient()\n",
    "        username = user_secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "        key = user_secrets.get_secret(\"KAGGLE_KEY\")\n",
    "        if username and key:\n",
    "            os.environ[\"KAGGLE_USERNAME\"] = username\n",
    "            os.environ[\"KAGGLE_KEY\"] = key\n",
    "            print(\"âœ… Authenticated via Secrets\")\n",
    "            return\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    if \"KAGGLE_USERNAME\" in os.environ and \"KAGGLE_KEY\" in os.environ:\n",
    "        print(\"âœ… Environment variables already set\")\n",
    "        return\n",
    "    \n",
    "    print(\"âš ï¸ Manual login required...\")\n",
    "    kagglehub.login()\n",
    "\n",
    "auto_login()\n",
    "\n",
    "# Clean up old checkpoints\n",
    "!rm /tmp/content/intermediate_ckpt/* -rf\n",
    "!rm /tmp/content/ckpts/* -rf\n",
    "\n",
    "# Load Gemma3 1B-IT\n",
    "print(\"Loading Gemma3 1B-IT...\")\n",
    "MODEL_CP_PATH = params.GEMMA3_1B_IT\n",
    "config = model.ModelConfig.gemma3_1b()\n",
    "gemma = params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n",
    "tokenizer = params.create_tokenizer()\n",
    "\n",
    "# Save intermediate checkpoint for LoRA loading\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "_, state = nnx.split(gemma)\n",
    "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
    "checkpointer.wait_until_finished()\n",
    "\n",
    "# Clean up to save memory\n",
    "del gemma\n",
    "del state\n",
    "gc.collect()\n",
    "\n",
    "print(\"Model checkpoint saved.\")\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: LORA MODEL SETUP\n",
    "# =============================================================================\n",
    "\n",
    "def get_gemma_ref_model(ckpt_path):\n",
    "    \"\"\"Load reference model from checkpoint with sharding.\"\"\"\n",
    "    mesh = jax.make_mesh(*MESH)\n",
    "    model_config = model.ModelConfig.gemma3_1b()\n",
    "    \n",
    "    abs_gemma = nnx.eval_shape(\n",
    "        lambda: params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n",
    "    )\n",
    "\n",
    "    abs_state = nnx.state(abs_gemma)\n",
    "    abs_state = jax.tree.map(\n",
    "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "        abs_state,\n",
    "        nnx.get_named_sharding(abs_state, mesh),\n",
    "    )\n",
    "    \n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
    "\n",
    "    graph_def, _ = nnx.split(abs_gemma)\n",
    "    gemma = nnx.merge(graph_def, restored_params)\n",
    "    return gemma, mesh, model_config\n",
    "\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    \"\"\"Apply LoRA layers to base model.\"\"\"\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        module_path=(\n",
    "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "            \".*attn_vec_einsum\"\n",
    "        ),\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "    )\n",
    "\n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, **model_input\n",
    "    )\n",
    "\n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "\n",
    "    return lora_model\n",
    "\n",
    "\n",
    "# Load models\n",
    "print(\"Loading reference model...\")\n",
    "ref_model, mesh, model_config = get_gemma_ref_model(\n",
    "    ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
    ")\n",
    "\n",
    "print(\"Applying LoRA...\")\n",
    "lora_policy = get_lora_model(ref_model, mesh=mesh)\n",
    "\n",
    "print(f\"LoRA model ready: RANK={RANK}, ALPHA={ALPHA}\")\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 8: PROMETHEUS REWARD FUNCTIONS (Open-Ended Tasks)\n# =============================================================================\n# For real user tasks, we can't check \"correctness\" - instead reward:\n# 1. Format compliance (exact and approximate)\n# 2. Reasoning trace quality (coherence, structure, relevance)\n# 3. Answer completeness (non-empty, reasonable length)\n# 4. Compression bonus (Kolmogorov: concise but complete)\n\n# Format matching regex\nmatch_format = re.compile(\n    rf\"^[\\s]{{0,}}\"\n    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n    rf\"{solution_start}(.+?){solution_end}\"\n    rf\"[\\s]{{0,}}$\",\n    flags=re.MULTILINE | re.DOTALL,\n)\n\n\ndef match_format_exactly(prompts, completions, **kwargs):\n    \"\"\"Reward 3.0 if format matches exactly.\"\"\"\n    return [\n        0 if match_format.search(response) is None else 3.0\n        for response in completions\n    ]\n\n\ndef match_format_approximately(prompts, completions, **kwargs):\n    \"\"\"Reward partial format compliance.\"\"\"\n    scores = []\n    for completion in completions:\n        score = 0\n        response = completion\n        # Reward each tag appearing exactly once\n        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n        score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n        score += 0.5 if response.count(solution_start) == 1 else -0.5\n        score += 0.5 if response.count(solution_end) == 1 else -0.5\n        scores.append(score)\n    return scores\n\n\ndef reasoning_coherence(prompts, completions, **kwargs):\n    \"\"\"Reward coherent, structured reasoning traces.\"\"\"\n    scores = []\n    \n    # Logical connectors that indicate good reasoning\n    connectors = [\n        'first', 'second', 'third', 'finally',\n        'because', 'since', 'therefore', 'thus', 'so',\n        'however', 'although', 'but', 'yet',\n        'for example', 'specifically', 'in particular',\n        'this means', 'which means', 'as a result',\n        'let me', 'i need to', 'to do this',\n        'step', '1.', '2.', '3.',\n    ]\n    \n    for completion in completions:\n        # Extract reasoning\n        match = re.search(\n            rf\"{reasoning_start}(.+?){reasoning_end}\",\n            completion,\n            re.DOTALL\n        )\n        if not match:\n            scores.append(0)\n            continue\n        \n        reasoning = match.group(1).lower()\n        score = 0\n        \n        # Count logical connectors (max 2.0 points)\n        connector_count = sum(1 for c in connectors if c in reasoning)\n        score += min(2.0, connector_count * 0.3)\n        \n        # Has multiple sentences (max 1.0 point)\n        sentences = [s.strip() for s in reasoning.split('.') if len(s.strip()) > 10]\n        if len(sentences) >= 3:\n            score += 1.0\n        elif len(sentences) >= 2:\n            score += 0.5\n        \n        scores.append(score)\n    \n    return scores\n\n\ndef answer_completeness(prompts, completions, **kwargs):\n    \"\"\"Reward non-empty, substantive answers.\"\"\"\n    scores = []\n    \n    for completion in completions:\n        # Extract answer\n        match = re.search(\n            rf\"{solution_start}(.+?){solution_end}\",\n            completion,\n            re.DOTALL\n        )\n        if not match:\n            scores.append(0)\n            continue\n        \n        answer = match.group(1).strip()\n        word_count = len(answer.split())\n        \n        # Reward based on answer length (not too short, not too long)\n        if 5 <= word_count <= 100:\n            scores.append(2.0)\n        elif 3 <= word_count < 5 or 100 < word_count <= 200:\n            scores.append(1.0)\n        elif 1 <= word_count < 3:\n            scores.append(0.5)\n        else:\n            scores.append(0)\n    \n    return scores\n\n\ndef question_relevance(prompts, completions, **kwargs):\n    \"\"\"Reward reasoning that references the question.\"\"\"\n    questions = kwargs.get(\"question\", [\"\"])\n    scores = []\n    \n    for completion, question in zip(completions, questions):\n        # Extract reasoning\n        match = re.search(\n            rf\"{reasoning_start}(.+?){reasoning_end}\",\n            completion,\n            re.DOTALL\n        )\n        if not match:\n            scores.append(0)\n            continue\n        \n        reasoning = match.group(1).lower()\n        question_lower = question.lower()\n        \n        # Extract key words from question (nouns, verbs - simple heuristic)\n        question_words = set(\n            w for w in re.findall(r'\\b\\w{4,}\\b', question_lower)\n            if w not in {'what', 'when', 'where', 'which', 'that', 'this', 'have', 'does', 'will', 'would', 'could', 'should'}\n        )\n        \n        if not question_words:\n            scores.append(1.0)  # Can't check, give benefit of doubt\n            continue\n        \n        # Count how many question words appear in reasoning\n        matches = sum(1 for w in question_words if w in reasoning)\n        relevance = matches / len(question_words)\n        \n        scores.append(min(2.0, relevance * 2.5))\n    \n    return scores\n\n\ndef trace_compression_bonus(prompts, completions, **kwargs):\n    \"\"\"Kolmogorov bonus: reward concise but complete reasoning.\"\"\"\n    scores = []\n    \n    for completion in completions:\n        # Extract reasoning\n        match = re.search(\n            rf\"{reasoning_start}(.+?){reasoning_end}\",\n            completion,\n            re.DOTALL\n        )\n        if not match:\n            scores.append(0)\n            continue\n        \n        reasoning = match.group(1)\n        word_count = len(reasoning.split())\n        \n        # Optimal range: 30-100 words (concise but substantive)\n        if 30 <= word_count <= 100:\n            scores.append(1.5)\n        elif 20 <= word_count < 30 or 100 < word_count <= 150:\n            scores.append(1.0)\n        elif 15 <= word_count < 20 or 150 < word_count <= 200:\n            scores.append(0.5)\n        else:\n            scores.append(0)  # Too short or too long\n    \n    return scores\n\n\n# Debug helper\ndef debug_sample(prompts, completions, **kwargs):\n    \"\"\"Print first sample for debugging (returns 0 reward).\"\"\"\n    questions = kwargs.get(\"question\", [\"\"])\n    if completions:\n        q = questions[0][:60] if questions else \"?\"\n        c = completions[0][:200] if completions[0] else \"(empty)\"\n        print(f\"Q: {q}...\")\n        print(f\"R: {c}...\")\n    return [0] * len(completions)\n\n\nprint(\"PROMETHEUS reward functions for OPEN-ENDED tasks:\")\nprint(\"  - match_format_exactly (3.0)\")\nprint(\"  - match_format_approximately (+/-0.5 per tag)\")\nprint(\"  - reasoning_coherence (connectors, structure)\")\nprint(\"  - answer_completeness (length, substance)\")\nprint(\"  - question_relevance (keywords from question)\")\nprint(\"  - trace_compression_bonus (Kolmogorov)\")\nprint(\"\\nNo 'correctness' check - we reward PROCESS not ANSWER\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 9: EVALUATION HELPERS (Open-Ended Tasks)\n# =============================================================================\n\ndef generate(question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None):\n    \"\"\"Generate response for a question.\"\"\"\n    if isinstance(question, str):\n        input_batch = [\n            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question),\n        ]\n    else:\n        input_batch = [\n            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q)\n            for q in question\n        ]\n\n    out_data = sampler(\n        input_strings=input_batch,\n        max_generation_steps=768,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        echo=False,\n        seed=seed,\n        eos_tokens=[1, 106],\n    )\n\n    output = out_data.text\n    return output[0] if isinstance(question, str) else output\n\n\ndef evaluate_open_ended(dataset, sampler, temperature=0.7, top_k=50, top_p=0.95, num_passes=1):\n    \"\"\"\n    Evaluate on open-ended tasks.\n    Since no ground truth, measure:\n    - Format compliance\n    - Reasoning quality (coherence, length)\n    - Answer presence\n    \"\"\"\n    total = 0\n    format_exact = 0\n    has_reasoning = 0\n    has_answer = 0\n    avg_reasoning_len = []\n    avg_answer_len = []\n    coherence_scores = []\n    \n    for batch in tqdm(dataset):\n        questions = batch[\"question\"]\n        \n        responses = generate(questions, sampler, temperature, top_k, top_p, seed=0)\n        \n        for response in responses:\n            total += 1\n            \n            # Check format\n            if match_format.search(response) is not None:\n                format_exact += 1\n            \n            # Check reasoning\n            r_match = re.search(\n                rf\"{reasoning_start}(.+?){reasoning_end}\",\n                response,\n                re.DOTALL\n            )\n            if r_match:\n                has_reasoning += 1\n                reasoning = r_match.group(1)\n                avg_reasoning_len.append(len(reasoning.split()))\n                \n                # Simple coherence: count logical connectors\n                connectors = ['because', 'therefore', 'first', 'then', 'so', 'thus']\n                coherence = sum(1 for c in connectors if c in reasoning.lower())\n                coherence_scores.append(coherence)\n            \n            # Check answer\n            a_match = re.search(\n                rf\"{solution_start}(.+?){solution_end}\",\n                response,\n                re.DOTALL\n            )\n            if a_match:\n                has_answer += 1\n                answer = a_match.group(1)\n                avg_answer_len.append(len(answer.split()))\n    \n    return {\n        \"total\": total,\n        \"format_exact_pct\": format_exact / total * 100 if total else 0,\n        \"has_reasoning_pct\": has_reasoning / total * 100 if total else 0,\n        \"has_answer_pct\": has_answer / total * 100 if total else 0,\n        \"avg_reasoning_words\": np.mean(avg_reasoning_len) if avg_reasoning_len else 0,\n        \"avg_answer_words\": np.mean(avg_answer_len) if avg_answer_len else 0,\n        \"avg_coherence\": np.mean(coherence_scores) if coherence_scores else 0,\n    }\n\n\ndef print_eval_results(results, label=\"\"):\n    \"\"\"Pretty print evaluation results.\"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"{label} RESULTS\")\n    print(f\"{'='*50}\")\n    print(f\"  Samples evaluated: {results['total']}\")\n    print(f\"\\n  FORMAT:\")\n    print(f\"    Exact format match: {results['format_exact_pct']:.1f}%\")\n    print(f\"    Has <reasoning>:    {results['has_reasoning_pct']:.1f}%\")\n    print(f\"    Has <answer>:       {results['has_answer_pct']:.1f}%\")\n    print(f\"\\n  QUALITY:\")\n    print(f\"    Avg reasoning words: {results['avg_reasoning_words']:.1f}\")\n    print(f\"    Avg answer words:    {results['avg_answer_words']:.1f}\")\n    print(f\"    Avg coherence score: {results['avg_coherence']:.2f}\")\n\n\nprint(\"Open-ended evaluation helpers defined.\")\nprint(\"Metrics: format compliance, reasoning presence, quality indicators\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 10: PRE-TRAINING EVALUATION\n# =============================================================================\n\nprint(\"Creating sampler for evaluation...\")\nsampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)\n\nprint(\"\\nEvaluating BASE model (before training)...\")\nbase_results = evaluate_open_ended(\n    test_dataset,\n    sampler,\n    **GENERATION_CONFIGS[\"greedy\"],\n)\nprint_eval_results(base_results, \"BASE MODEL\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: GRPO TRAINING SETUP\n",
    "# =============================================================================\n",
    "\n",
    "# Checkpoint options\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
    ")\n",
    "\n",
    "# Optimizer: AdamW with warmup cosine decay\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        decay_steps=MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=B1,\n",
    "    b2=B2,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "# Gradient clipping (critical for KL stability)\n",
    "if MAX_GRAD_NORM is not None:\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
    "        optimizer,\n",
    "    )\n",
    "\n",
    "# Training cluster config\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh={\n",
    "        rl_cluster_lib.Role.ACTOR: mesh,\n",
    "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
    "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
    "    },\n",
    "    rollout_engine='vanilla',\n",
    "    offload_to_cpu=False,\n",
    "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
    "        actor_optimizer=optimizer,\n",
    "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "        max_steps=MAX_STEPS,\n",
    "        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        metrics_logging_options=None,  # Disable wandb\n",
    "        checkpoint_root_directory=CKPT_DIR,\n",
    "        checkpointing_options=checkpointing_options,\n",
    "    ),\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
    "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        top_k=TOP_K,\n",
    "        eos_tokens=[1, 106],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# GRPO config\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    beta=BETA,\n",
    "    epsilon=EPSILON,\n",
    ")\n",
    "\n",
    "print(\"GRPO training config ready.\")\n",
    "print(f\"  Steps: {MAX_STEPS}\")\n",
    "print(f\"  LR: {LEARNING_RATE} (warmup {int(WARMUP_STEPS)} steps)\")\n",
    "print(f\"  GRPO: G={NUM_GENERATIONS}, Î²={BETA}, Îµ={EPSILON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 12: INITIALIZE GRPO TRAINER\n# =============================================================================\n\n# Create RL cluster\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,\n    reference=ref_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n\n# Create GRPO trainer with PROMETHEUS reward functions for OPEN-ENDED tasks\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[\n        match_format_exactly,        # 3.0 for correct format\n        match_format_approximately,  # +/-0.5 per tag\n        reasoning_coherence,         # Up to 3.0 for logical structure\n        answer_completeness,         # Up to 2.0 for substantive answer\n        question_relevance,          # Up to 2.0 for staying on topic\n        trace_compression_bonus,     # Up to 1.5 for concise reasoning\n    ],\n    grpo_config=grpo_config,\n)\n\nprint(\"GRPO trainer initialized with 6 OPEN-ENDED reward functions:\")\nprint(\"  Format: exact (3.0) + approximate (+/-2.0)\")\nprint(\"  Quality: coherence (3.0) + completeness (2.0)\")  \nprint(\"  Relevance: keywords (2.0) + compression (1.5)\")\nprint(f\"\\nMax possible reward: ~14.0\")\nprint(\"Ready to train on REAL USER TASKS!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: TRAIN!\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"Starting GRPO training for {MAX_STEPS} steps...\")\n",
    "print(f\"This may take several hours on TPU v5e-8.\")\n",
    "print()\n",
    "\n",
    "with mesh:\n",
    "    grpo_trainer.train(train_dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 14: LOAD BEST CHECKPOINT & EVALUATE\n# =============================================================================\n\n# Find latest checkpoint\nactor_ckpt_dir = os.path.join(CKPT_DIR, \"actor\")\n\nlatest_step = -1\nif os.path.exists(actor_ckpt_dir):\n    for item in os.listdir(actor_ckpt_dir):\n        if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r'^\\d+$', item):\n            step = int(item)\n            if step > latest_step:\n                latest_step = step\n\nif latest_step == -1:\n    raise FileNotFoundError(f\"No checkpoints found in {actor_ckpt_dir}\")\n\nprint(f\"Loading checkpoint from step {latest_step}...\")\n\ntrained_ckpt_path = os.path.join(\n    CKPT_DIR, \"actor\", str(latest_step), \"model_params\"\n)\n\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(lora_policy, nnx.LoRAParam),\n)\ncheckpointer = ocp.StandardCheckpointer()\ntrained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n\nnnx.update(\n    lora_policy,\n    jax.tree.map(\n        lambda a, b: b,\n        nnx.state(lora_policy, nnx.LoRAParam),\n        trained_lora_params,\n    ),\n)\n\n# Create sampler with trained model\nsampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)\n\nprint(\"\\nEvaluating TRAINED model...\")\ntrained_results = evaluate_open_ended(\n    test_dataset,\n    sampler,\n    **GENERATION_CONFIGS[\"greedy\"],\n)\nprint_eval_results(trained_results, \"TRAINED MODEL\")\n\n# Compare\nprint(\"\\n\" + \"=\"*50)\nprint(\"IMPROVEMENT SUMMARY\")\nprint(\"=\"*50)\nprint(f\"  Format: {base_results['format_exact_pct']:.1f}% -> {trained_results['format_exact_pct']:.1f}%\")\nprint(f\"  Reasoning: {base_results['has_reasoning_pct']:.1f}% -> {trained_results['has_reasoning_pct']:.1f}%\")\nprint(f\"  Coherence: {base_results['avg_coherence']:.2f} -> {trained_results['avg_coherence']:.2f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 15: INFERENCE EXAMPLES\n# =============================================================================\n\n# Test on various domains (PROMETHEUS: multi-domain focus)\ntest_questions = [\n    # Math (low weight but test anyway)\n    \"Janet has 3 times as many marbles as Tom. Tom has 12 marbles. How many marbles does Janet have?\",\n    # Science\n    \"Why does ice float on water?\",\n    # Creative\n    \"Come up with 3 creative uses for a broken umbrella.\",\n    # Logic\n    \"If all cats are mammals and some mammals can swim, can we conclude that some cats can swim?\",\n]\n\nprint(\"=\"*60)\nprint(\"INFERENCE EXAMPLES\")\nprint(\"=\"*60)\n\n# Use \"standard\" config for coherent outputs (greedy with temp=1e-4 produces garbage)\nfor q in test_questions:\n    print(f\"\\nQ: {q}\")\n    print(\"-\"*40)\n    response = generate(q, sampler, **GENERATION_CONFIGS[\"standard\"])\n    print(response)\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CELL 16: SAVE FOR MULTI-SESSION MODE\n# =============================================================================\n\n# For single-session mode: Model is reproduced from this notebook\n# For multi-session mode: Upload to Kaggle Models\n\n# MUST use absolute path for Orbax checkpointer\nSAVE_PATH = \"/kaggle/working/prometheus_gemma_reasoning\"\n\nprint(f\"Saving trained LoRA weights to {SAVE_PATH}...\")\n\n# Save LoRA params\nos.makedirs(SAVE_PATH, exist_ok=True)\ncheckpointer = ocp.StandardCheckpointer()\ncheckpointer.save(\n    os.path.join(SAVE_PATH, \"lora_params\"),\n    nnx.state(lora_policy, nnx.LoRAParam)\n)\ncheckpointer.wait_until_finished()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"PROMETHEUS + TUNIX TRAINING COMPLETE\")\nprint(\"=\"*60)\nprint(f\"\\nModel saved to: {SAVE_PATH}\")\nprint(\"\\nFor multi-session evaluation:\")\nprint(\"  1. Upload to Kaggle Models\")\nprint(\"  2. Specify KAGGLE_MODEL_ID in submission notebook\")\nprint(\"\\nKey settings used:\")\nprint(f\"  - Model: Gemma3 1B-IT\")\nprint(f\"  - LoRA: RANK={RANK}, ALPHA={ALPHA}\")\nprint(f\"  - GRPO: G={NUM_GENERATIONS}, Î²={BETA}, Îµ={EPSILON}\")\nprint(f\"  - Training: {MAX_STEPS} steps, LR={LEARNING_RATE}\")"
  },
  {
   "cell_type": "code",
   "id": "3fx5fnx7n1u",
   "source": "# =============================================================================\n# CELL 17: INFERENCE SERVER FOR KAGGLE SUBMISSION\n# =============================================================================\n# This cell sets up the inference server required for Kaggle automated evaluation.\n# The server is only activated during competition rerun (KAGGLE_IS_COMPETITION_RERUN=True)\n\nimport os\nimport sys\n\n# Check if we're in competition rerun mode\nIS_COMPETITION_RERUN = bool(os.getenv('KAGGLE_IS_COMPETITION_RERUN'))\n\nprint(f\"Competition rerun mode: {IS_COMPETITION_RERUN}\")\n\n# Define the predict function\ndef predict(question: str) -> str:\n    \"\"\"\n    Generate a response with reasoning trace for the given question.\n    Returns format: <reasoning>...</reasoning><answer>...</answer>\n    \"\"\"\n    response = generate(question, sampler, **GENERATION_CONFIGS[\"standard\"])\n    return response\n\n\n# Try to find and use the inference server\nif IS_COMPETITION_RERUN:\n    print(\"Setting up inference server for submission...\")\n    \n    # List available inputs to find the inference server module\n    input_dir = \"/kaggle/input\"\n    if os.path.exists(input_dir):\n        print(f\"Available inputs: {os.listdir(input_dir)}\")\n        \n        # Look for kaggle_evaluation in competition data\n        for item in os.listdir(input_dir):\n            item_path = os.path.join(input_dir, item)\n            if os.path.isdir(item_path):\n                # Check for kaggle_evaluation subdirectory\n                eval_path = os.path.join(item_path, \"kaggle_evaluation\")\n                if os.path.exists(eval_path):\n                    print(f\"Found kaggle_evaluation at: {eval_path}\")\n                    sys.path.insert(0, item_path)\n                    break\n    \n    # Try to import the inference server\n    try:\n        # First, try Tunix-specific inference server (if it exists)\n        try:\n            from kaggle_evaluation.tunix_inference_server import TunixInferenceServer\n            inference_server = TunixInferenceServer(predict)\n            print(\"Using TunixInferenceServer\")\n        except ImportError:\n            # Try generic inference server\n            try:\n                from kaggle_evaluation import make_env\n                inference_server = make_env(predict)\n                print(\"Using generic make_env\")\n            except ImportError:\n                # Try other common patterns\n                import kaggle_evaluation\n                # List what's available\n                print(f\"kaggle_evaluation contents: {dir(kaggle_evaluation)}\")\n                \n                # Try to find any inference server class\n                for name in dir(kaggle_evaluation):\n                    if 'inference' in name.lower() or 'server' in name.lower():\n                        print(f\"Found: {name}\")\n                        cls = getattr(kaggle_evaluation, name)\n                        if callable(cls):\n                            inference_server = cls(predict)\n                            break\n        \n        # Start serving\n        print(\"Starting inference server...\")\n        inference_server.serve()\n        \n    except Exception as e:\n        print(f\"ERROR: Could not set up inference server: {e}\")\n        print(\"Available modules in kaggle_evaluation:\")\n        try:\n            import kaggle_evaluation\n            print(dir(kaggle_evaluation))\n        except ImportError as ie:\n            print(f\"kaggle_evaluation not found: {ie}\")\n        \n        # Provide debugging info\n        print(\"\\nTo fix this, check the competition data for the correct inference server module.\")\n        print(\"Run this in a Kaggle notebook:\")\n        print(\"  import os\")\n        print(\"  for root, dirs, files in os.walk('/kaggle/input'):\")\n        print(\"      for f in files:\")\n        print(\"          if 'inference' in f.lower() or 'server' in f.lower():\")\n        print(\"              print(os.path.join(root, f))\")\n\nelse:\n    print(\"Not in competition mode - skipping inference server setup.\")\n    print(\"Local testing: use generate() or predict() directly.\")\n    \n    # Quick test of predict function\n    test_q = \"What is 2 + 2?\"\n    print(f\"\\nTest predict('{test_q}'):\")\n    print(predict(test_q)[:200] + \"...\" if len(predict(test_q)) > 200 else predict(test_q))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}