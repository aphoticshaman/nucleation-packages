{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df91832",
   "metadata": {
    "_cell_guid": "8c62453f-96e3-4690-b446-66cc36944430",
    "_uuid": "2723e7ea-8f54-4ded-bc4d-654016305214",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003814,
     "end_time": "2025-12-10T01:22:19.241014",
     "exception": false,
     "start_time": "2025-12-10T01:22:19.237200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "References\n",
    "- https://www.kaggle.com/code/huikang/arc-agi-2-code-approach\n",
    "- https://www.kaggle.com/code/huikang/r1-distill-qwen-tir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0cb638",
   "metadata": {
    "_cell_guid": "fdf223c2-f8ef-467a-9dbb-cc5484786047",
    "_kg_hide-output": true,
    "_uuid": "3da124f9-ff58-4c99-8f7a-fe224f1fa647",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:22:19.248283Z",
     "iopub.status.busy": "2025-12-10T01:22:19.248099Z",
     "iopub.status.idle": "2025-12-10T01:22:49.369593Z",
     "shell.execute_reply": "2025-12-10T01:22:49.369170Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 30.12927,
     "end_time": "2025-12-10T01:22:49.373538",
     "exception": false,
     "start_time": "2025-12-10T01:22:19.244268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.18.0\n",
      "Uninstalling tensorflow-2.18.0:\n",
      "  Successfully uninstalled tensorflow-2.18.0\n",
      "Found existing installation: matplotlib 3.7.2\n",
      "Uninstalling matplotlib-3.7.2:\n",
      "  Successfully uninstalled matplotlib-3.7.2\n",
      "Found existing installation: keras 3.8.0\n",
      "Uninstalling keras-3.8.0:\n",
      "  Successfully uninstalled keras-3.8.0\n",
      "Found existing installation: scikit-learn 1.2.2\n",
      "Uninstalling scikit-learn-1.2.2:\n",
      "  Successfully uninstalled scikit-learn-1.2.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip', 'uninstall', '--yes', 'tensorflow', 'matplotlib', 'keras', 'scikit-learn'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [\"pip\", \"uninstall\", \"--yes\", \"tensorflow\", \"matplotlib\", \"keras\", \"scikit-learn\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb288ea",
   "metadata": {
    "_cell_guid": "4fcbcdb6-696b-4b48-826f-c9bd42031243",
    "_uuid": "92a580b1-f0fe-46c6-a97a-9059012c30a5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:22:49.380990Z",
     "iopub.status.busy": "2025-12-10T01:22:49.380839Z",
     "iopub.status.idle": "2025-12-10T01:23:01.928553Z",
     "shell.execute_reply": "2025-12-10T01:23:01.928049Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 12.552652,
     "end_time": "2025-12-10T01:23:01.929760",
     "exception": false,
     "start_time": "2025-12-10T01:22:49.377108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def is_on_kaggle_commit() -> bool:\n",
    "    return os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Batch\" and not bool(\n",
    "        os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n",
    "    )\n",
    "\n",
    "\n",
    "def is_on_kaggle_interactive() -> bool:\n",
    "    return os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not bool(\n",
    "        os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n",
    "    )\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "final_cutoff_time = start_time + (4 * 60 + 45) * 60  # 4.75 hours from start time\n",
    "cutoff_times = [\n",
    "    int(x) for x in np.linspace(final_cutoff_time, start_time + 12 * 60, 50 + 1)\n",
    "]  # 5 minutes loading time at the start\n",
    "cutoff_times.pop()\n",
    "\n",
    "os.makedirs(\"solutions\", exist_ok=True)\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "assert torch.cuda.device_count() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae76bdff",
   "metadata": {
    "_cell_guid": "5c120214-e16b-45ea-a2d1-b77777bedb72",
    "_uuid": "1d6623d2-ce24-4d27-973c-142828fe4e1a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003232,
     "end_time": "2025-12-10T01:23:01.936876",
     "exception": false,
     "start_time": "2025-12-10T01:23:01.933644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Serve vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0643a24",
   "metadata": {
    "_cell_guid": "8783f9ec-c1d9-4110-9aaa-39d6a92b4a54",
    "_uuid": "aec8a8d8-b6af-4748-ab79-d7da2298e33d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:01.944528Z",
     "iopub.status.busy": "2025-12-10T01:23:01.944310Z",
     "iopub.status.idle": "2025-12-10T01:23:01.959757Z",
     "shell.execute_reply": "2025-12-10T01:23:01.959384Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.020075,
     "end_time": "2025-12-10T01:23:01.960390",
     "exception": false,
     "start_time": "2025-12-10T01:23:01.940315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl100k_base.tiktoken\n",
      "o200k_base.tiktoken\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ls', '/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"ls\", \"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825e9887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:01.967941Z",
     "iopub.status.busy": "2025-12-10T01:23:01.967798Z",
     "iopub.status.idle": "2025-12-10T01:23:01.970346Z",
     "shell.execute_reply": "2025-12-10T01:23:01.969970Z"
    },
    "papermill": {
     "duration": 0.007003,
     "end_time": "2025-12-10T01:23:01.970936",
     "exception": false,
     "start_time": "2025-12-10T01:23:01.963933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"a-vllm.log\", \"w\") as f:\n",
    "    f.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f6ed2b",
   "metadata": {
    "_cell_guid": "7f17cd32-442e-460a-982e-7ca73668b69f",
    "_uuid": "53e3ad10-1b6f-47d8-b5de-19f556c4b7a8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:01.978513Z",
     "iopub.status.busy": "2025-12-10T01:23:01.978369Z",
     "iopub.status.idle": "2025-12-10T01:23:01.983108Z",
     "shell.execute_reply": "2025-12-10T01:23:01.982691Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009004,
     "end_time": "2025-12-10T01:23:01.983652",
     "exception": false,
     "start_time": "2025-12-10T01:23:01.974648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs: /kaggle/working/a-vllm.log\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def start_vllm_server() -> subprocess.Popen[bytes]:\n",
    "    \"\"\"Start vLLM server in the background\"\"\"\n",
    "    os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "    os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "    os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"TRITON_ATTN\"\n",
    "    os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    # https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#troubleshooting\n",
    "    os.environ[\"TIKTOKEN_ENCODINGS_BASE\"] = (\n",
    "        \"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\"\n",
    "    )\n",
    "\n",
    "    sequence_length = 65_536\n",
    "\n",
    "    command: list[str] = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\",\n",
    "        \"/kaggle/input/gpt-oss-120b/transformers/default/1\",\n",
    "        \"--served-model-name\",\n",
    "        \"vllm-model\",\n",
    "        \"--tensor-parallel-size\",\n",
    "        \"1\",\n",
    "        \"--max-num-seqs\",\n",
    "        \"4\",\n",
    "        \"--gpu-memory-utilization\",\n",
    "        \"0.96\",  # any higher may not have enough for graph capture\n",
    "        \"--host\",\n",
    "        \"0.0.0.0\",\n",
    "        \"--port\",\n",
    "        \"8000\",\n",
    "        \"--dtype\",\n",
    "        \"auto\",\n",
    "        \"--max-model-len\",\n",
    "        f\"{sequence_length}\",\n",
    "    ]\n",
    "\n",
    "    # Start the process in the background\n",
    "    with open(\"/kaggle/working/a-vllm.log\", \"w\") as logfile:\n",
    "        process: subprocess.Popen[bytes] = subprocess.Popen(\n",
    "            command, stdout=logfile, stderr=subprocess.STDOUT, start_new_session=True\n",
    "        )\n",
    "\n",
    "    print(\"Logs: /kaggle/working/a-vllm.log\")\n",
    "    return process\n",
    "\n",
    "# Start the server\n",
    "vllm_process: subprocess.Popen[bytes] = start_vllm_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb82818b",
   "metadata": {
    "_cell_guid": "bf725ef0-626c-4b0b-93d1-e74549a152a7",
    "_uuid": "1759af22-a458-484e-84b1-64010680e36b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:01.991417Z",
     "iopub.status.busy": "2025-12-10T01:23:01.991275Z",
     "iopub.status.idle": "2025-12-10T01:23:07.407476Z",
     "shell.execute_reply": "2025-12-10T01:23:07.407004Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.421483,
     "end_time": "2025-12-10T01:23:07.408724",
     "exception": false,
     "start_time": "2025-12-10T01:23:01.987241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI, Stream\n",
    "from openai.types.chat import ChatCompletion, ChatCompletionMessageParam\n",
    "from openai.types.chat.chat_completion_chunk import ChatCompletionChunk\n",
    "\n",
    "# Point the client to your local vLLM server\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://127.0.0.1:8000/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-local\"  # any non-empty string\n",
    "\n",
    "client: OpenAI = OpenAI(\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "\n",
    "# https://github.com/vllm-project/vllm/issues/27243\n",
    "# Unexpected token 2000?? while expecting start token 200006\n",
    "stop_token_ids: list[int] = [\n",
    "    token_id\n",
    "    for token_id in range(200_000, 201_088)\n",
    "    if token_id not in [200005, 200006, 200007, 200008]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39dbc2bb",
   "metadata": {
    "_cell_guid": "7506d16a-f083-47f7-a68f-c8cf2602c432",
    "_uuid": "7ffb104f-e307-4f91-926c-15177cd12f0a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.416742Z",
     "iopub.status.busy": "2025-12-10T01:23:07.416346Z",
     "iopub.status.idle": "2025-12-10T01:23:07.419443Z",
     "shell.execute_reply": "2025-12-10T01:23:07.419074Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007521,
     "end_time": "2025-12-10T01:23:07.419984",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.412463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def await_client(printing: bool = False):\n",
    "    for _ in range(20 * 60):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            model_list = client.models.list()\n",
    "            if printing:\n",
    "                print(model_list)\n",
    "        except NameError:\n",
    "            raise  # maybe you did not run the cell initializing client\n",
    "        except Exception:\n",
    "            continue\n",
    "        break\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "if is_on_kaggle_interactive():\n",
    "    await_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9d84f5e",
   "metadata": {
    "_cell_guid": "7726741c-26cf-4297-bb09-950ae9e812bd",
    "_uuid": "4bae670b-305a-4956-ace4-c439a613da21",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.427259Z",
     "iopub.status.busy": "2025-12-10T01:23:07.427129Z",
     "iopub.status.idle": "2025-12-10T01:23:07.448118Z",
     "shell.execute_reply": "2025-12-10T01:23:07.447700Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025322,
     "end_time": "2025-12-10T01:23:07.448677",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.423355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cachetools import cached, TTLCache\n",
    "from typing import Generator\n",
    "import time\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def reversed_lines(path: str, block_size: int = 4096) -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Iterate over the lines of a file in reverse order (last line first),\n",
    "    without loading the entire file into memory.\n",
    "\n",
    "    Yields lines as strings (including the trailing newline if present).\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        f.seek(0, os.SEEK_END)\n",
    "        file_end = f.tell()\n",
    "\n",
    "        buffer = b\"\"\n",
    "        pos = file_end\n",
    "\n",
    "        while pos > 0:\n",
    "            # Read a block from the end going backwards\n",
    "            read_size = min(block_size, pos)\n",
    "            pos -= read_size\n",
    "            f.seek(pos, os.SEEK_SET)\n",
    "            data = f.read(read_size)\n",
    "\n",
    "            buffer = data + buffer\n",
    "            # Split into lines\n",
    "            lines = buffer.split(b\"\\n\")\n",
    "            # Keep the first (possibly incomplete) part in buffer\n",
    "            buffer = lines[0]\n",
    "            # The rest (from the end backwards) are full lines\n",
    "            for line in reversed(lines[1:]):\n",
    "                yield line.decode(\"utf-8\", errors=\"replace\") + \"\\n\"\n",
    "\n",
    "        # Finally, yield the very first line (if any)\n",
    "        if buffer:\n",
    "            yield buffer.decode(\"utf-8\", errors=\"replace\") + \"\\n\"\n",
    "\n",
    "\n",
    "@cached(cache=TTLCache(maxsize=50, ttl=10))\n",
    "def get_gpu_kv_cache_usage(question_id: str | None = None) -> float:\n",
    "    for line in reversed_lines(\"a-vllm.log\"):\n",
    "        pattern = r\"GPU KV cache usage: ([\\d.]+)%\"\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            gpu_cache_usage = float(match.group(1))\n",
    "            return gpu_cache_usage\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb3b7ea3",
   "metadata": {
    "_cell_guid": "77510a63-d235-4a5c-b182-18a4ade735e3",
    "_uuid": "d0bcd210-101b-4f9a-9cd3-d9c656dd76c8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.455740Z",
     "iopub.status.busy": "2025-12-10T01:23:07.455609Z",
     "iopub.status.idle": "2025-12-10T01:23:07.458329Z",
     "shell.execute_reply": "2025-12-10T01:23:07.457973Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006946,
     "end_time": "2025-12-10T01:23:07.458854",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.451908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_on_kaggle_interactive():\n",
    "    resp: ChatCompletion = client.chat.completions.create(\n",
    "        model=\"vllm-model\",  # use your served name; if not set, the model path/name vLLM shows in logs\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Reply your answer in \\\\boxed{}\"},\n",
    "            {\"role\": \"user\", \"content\": \"How many r are there in strawberry?\"},\n",
    "        ],\n",
    "        max_tokens=1024,\n",
    "        temperature=1.0,\n",
    "        extra_body=dict(min_p=0.02, stop_token_ids=stop_token_ids, chat_template_kwargs=dict(enable_thinking=True)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f02c2eb",
   "metadata": {
    "_cell_guid": "3080497b-33ed-4c1d-a5fb-45ae4b8ad5d5",
    "_uuid": "07968db3-c86c-4c1d-93ca-40a8a020bd68",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.466142Z",
     "iopub.status.busy": "2025-12-10T01:23:07.466021Z",
     "iopub.status.idle": "2025-12-10T01:23:07.468180Z",
     "shell.execute_reply": "2025-12-10T01:23:07.467811Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006525,
     "end_time": "2025-12-10T01:23:07.468753",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.462228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_on_kaggle_interactive():\n",
    "    print(resp.choices[0].message.reasoning_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57f1cbc9",
   "metadata": {
    "_cell_guid": "2cb79916-5b27-4cb9-af64-d056ccb6bca4",
    "_uuid": "70b6ddcb-bea0-4a1c-9d66-5470a464a25e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.476247Z",
     "iopub.status.busy": "2025-12-10T01:23:07.476128Z",
     "iopub.status.idle": "2025-12-10T01:23:07.478181Z",
     "shell.execute_reply": "2025-12-10T01:23:07.477842Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006429,
     "end_time": "2025-12-10T01:23:07.478715",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.472286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_on_kaggle_interactive():\n",
    "    print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f7a1ed",
   "metadata": {
    "_cell_guid": "de5a9274-0b86-43c8-8715-f677550b7170",
    "_uuid": "7da500ca-4bf4-4af5-91d0-01a1b7611756",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003279,
     "end_time": "2025-12-10T01:23:07.485419",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.482140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "163505f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.493247Z",
     "iopub.status.busy": "2025-12-10T01:23:07.492844Z",
     "iopub.status.idle": "2025-12-10T01:23:07.495374Z",
     "shell.execute_reply": "2025-12-10T01:23:07.495037Z"
    },
    "papermill": {
     "duration": 0.007238,
     "end_time": "2025-12-10T01:23:07.495941",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.488703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPTS = [\n",
    "    \"You are solving a national/international-level mathematics olympiad problem. You must rigorously define all variables, explore multiple solution strategies before committing, perform full case analysis where required, justify every nontrivial step, explicitly check boundary cases and hidden assumptions, and verify the final result using at least one independent method. Return only the final numerical answer inside \\\\boxed{}. The answer must be an integer in [0, 99999]. Never guess.\",\n",
    "\n",
    "    \"Solve the problem with full rigor. After obtaining a candidate solution, actively attempt to refute your own answer by searching for counterexamples, re-running the logic from a different viewpoint, and stress-testing edge cases. Only after the answer survives refutation, return it in \\\\boxed{}. The answer must be an integer in [0, 99999]. Never guess.\",\n",
    "\n",
    "    \"Solve this problem as if under IMO-level time pressure: identify the key invariant, symmetry, or extremal principle early, avoid brute force unless strictly justified, compress reasoning without sacrificing correctness, and perform at least one final arithmetic verification pass. Return only the final integer answer in \\\\boxed{}, with 0 ≤ answer ≤ 99999. Never guess.\",\n",
    "\n",
    "    \"You must attempt at least two fundamentally different solution approaches (e.g., algebraic vs geometric, combinatorial vs number-theoretic). Proceed with the more rigorous one and use the other as a verification tool. Return only the verified final answer in \\\\boxed{}, where the answer is an integer in [0, 99999]. Never guess.\",\n",
    "\n",
    "    \"Solve the problem rigorously. If at any point a step relies on an unproven assumption, a jump in logic is detected, or the computation becomes inconsistent, you must restart the solution from first principles. Return only the final verified integer answer inside \\\\boxed{}, with 0 ≤ answer ≤ 99999. Never guess.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7937a8e",
   "metadata": {
    "_cell_guid": "9f2711cc-d0ad-4a94-94a1-512f91eee2f9",
    "_uuid": "a84de285-e493-427b-9fd6-bbab7f89f761",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.503395Z",
     "iopub.status.busy": "2025-12-10T01:23:07.503271Z",
     "iopub.status.idle": "2025-12-10T01:23:07.506442Z",
     "shell.execute_reply": "2025-12-10T01:23:07.506059Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007663,
     "end_time": "2025-12-10T01:23:07.507044",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.499381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_boxed_text(text: str) -> str:\n",
    "    \"\"\"Extract text inside \\\\boxed{} from LaTeX-formatted text\"\"\"\n",
    "    import re\n",
    "\n",
    "    pattern: str = r\"oxed{(.*?)}\"\n",
    "    matches: list[str] = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def is_valid_answer_string(text: str) -> bool:\n",
    "    try:\n",
    "        if int(text) == float(text):\n",
    "            if 0 <= int(text) <= 99_999:\n",
    "                # now AIMO answers no longer need modulo\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93520bf0",
   "metadata": {
    "_cell_guid": "07a38eab-85e1-4fe3-a0b1-c4ff0ac3ac1b",
    "_uuid": "b9a8c369-0e80-4421-b0f2-dab28170e33d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.514541Z",
     "iopub.status.busy": "2025-12-10T01:23:07.514422Z",
     "iopub.status.idle": "2025-12-10T01:23:07.519057Z",
     "shell.execute_reply": "2025-12-10T01:23:07.518650Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009002,
     "end_time": "2025-12-10T01:23:07.519624",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.510622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "completed_question_ids: set[str] = set()\n",
    "question_id_to_counter: dict[str, Counter] = {\"\": Counter()}\n",
    "\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def vote_answer(question_id: str, force_answer: bool = False) -> int | None:\n",
    "    # reads counter from global\n",
    "    counter = question_id_to_counter[question_id]\n",
    "    if force_answer and not counter:\n",
    "        print(f\"Current GPU usage {get_gpu_kv_cache_usage()}\")\n",
    "        print(\"force_answer=True but no answer recorded\")\n",
    "        completed_question_ids.add(question_id)\n",
    "        return 12453\n",
    "\n",
    "    # voting mechanism\n",
    "    modified_counter = Counter()\n",
    "    for value, count in counter.items():\n",
    "        # re-weighted because smaller answers seems to be wrong\n",
    "        # \"1.25 +\" because log(1) = 0\n",
    "        modified_counter[value] += math.log(1.25 + abs(value)) * count\n",
    "\n",
    "    total_score = sum(modified_counter.values())\n",
    "    score_list = sorted(\n",
    "        (score, counter[value], value) for value, score in modified_counter.items()\n",
    "    )\n",
    "    if force_answer:\n",
    "        print(f\"score_list | {total_score:8.1f} over {sum(counter.values())} attempts\")\n",
    "        print(f\"Current GPU usage {get_gpu_kv_cache_usage()}\")\n",
    "        for score, count, value in score_list[::-1]:\n",
    "            print(f\"{value:10}   {score:8.1f} {count:8d}\")\n",
    "        return score_list[-1][-1]\n",
    "    if score_list[-1][0] > max(3, total_score / (2 + math.log(1 + total_score))):\n",
    "        if len(score_list) == 1:\n",
    "            completed_question_ids.add(question_id)\n",
    "        else:\n",
    "            if score_list[-1][0] - score_list[-2][0] > 1:\n",
    "                # win by a certain number of points at least\n",
    "                completed_question_ids.add(question_id)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0357c4f",
   "metadata": {
    "_cell_guid": "9ad61568-0a06-40c8-9fbd-02e7c5355f9d",
    "_uuid": "8da03edd-ee18-442f-a459-8b6d0308bacc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.527206Z",
     "iopub.status.busy": "2025-12-10T01:23:07.527090Z",
     "iopub.status.idle": "2025-12-10T01:23:07.534105Z",
     "shell.execute_reply": "2025-12-10T01:23:07.533727Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.011527,
     "end_time": "2025-12-10T01:23:07.534622",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.523095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def generate_solution(\n",
    "    question_text: str, question_id: str = \"\", solution_index: int = 0, system_prompt: str = \"\"\n",
    ") -> str:\n",
    "    if question_id in completed_question_ids:\n",
    "        return \"\"\n",
    "    if time.time() >= cutoff_times[-1]:\n",
    "        return \"\"\n",
    "\n",
    "    if not system_prompt:\n",
    "        system_prompt = SYSTEM_PROMPTS[0]\n",
    "    \n",
    "    messages: list[ChatCompletionMessageParam] = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": question_text},\n",
    "    ]\n",
    "\n",
    "    text_response_to_save = \"\"\n",
    "    generation_idx = 0\n",
    "    for iteration in range(2):\n",
    "        text_response = \"\"\n",
    "        breaking = False\n",
    "\n",
    "        stream: Stream[ChatCompletionChunk] = client.chat.completions.create(\n",
    "            model=\"vllm-model\",  # use your served name; if not set, the model path/name vLLM shows in logs\n",
    "            messages=messages,\n",
    "            temperature=1.0,\n",
    "            stream=True,\n",
    "            extra_body=dict(min_p=0.02, stop_token_ids=stop_token_ids),\n",
    "            reasoning_effort=\"high\",\n",
    "        )\n",
    "\n",
    "        for chunk in stream:\n",
    "            generation_idx += 1\n",
    "            chunk_text = (\n",
    "                chunk.choices[0].delta.reasoning_content\n",
    "                if chunk.choices[0].delta.reasoning_content is not None\n",
    "                else chunk.choices[0].delta.content\n",
    "            )\n",
    "            if chunk_text:\n",
    "                text_response += chunk_text\n",
    "            if question_id in completed_question_ids:\n",
    "                # stop generating if we have finalized on an answer\n",
    "                breaking = True\n",
    "            if time.time() >= cutoff_times[-1]:\n",
    "                breaking = True\n",
    "            if generation_idx > 60_000:\n",
    "                breaking = True\n",
    "            if breaking:\n",
    "                break\n",
    "            # instead of breaking = True, so we want to inject instructions for these conditions\n",
    "            if \"}\" in chunk_text and is_valid_answer_string(extract_boxed_text(text_response)):\n",
    "                break\n",
    "            if iteration == 0 and generation_idx > 50_000:\n",
    "                break\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": text_response})\n",
    "        text_response_to_save += text_response\n",
    "        stream.close()\n",
    "\n",
    "        if breaking:\n",
    "            break\n",
    "\n",
    "        boxed_text = extract_boxed_text(text_response)\n",
    "        if not is_valid_answer_string(extract_boxed_text(text_response)) and iteration == 0 and generation_idx > 50_000:\n",
    "            print(\"follow-up - guess answer\")\n",
    "            user_follow_up = \"The answer is expected to be an integer between 0 and 99999 inclusive. Make an educated guess (e.g. lower bound, upper bound, ...) on your final answer and put in \\\\boxed{}.\"\n",
    "            messages.append({\"role\": \"user\", \"content\": user_follow_up})\n",
    "            text_response_to_save += \"\\n===\\n\" + user_follow_up + \"\\n===\\n\"\n",
    "        elif not is_valid_answer_string(boxed_text):\n",
    "            print(\"follow-up - boxed answer\")\n",
    "            user_follow_up = \"The answer is expected to be an integer between 0 and 99999 inclusive. Place your final answer in \\\\boxed{}. Do not guess the answer.\"\n",
    "            messages.append({\"role\": \"user\", \"content\": user_follow_up})\n",
    "            text_response_to_save += \"\\n===\\n\" + user_follow_up + \"\\n===\\n\"\n",
    "        elif int(boxed_text) <= 10:\n",
    "            print(\"follow-up - are you sure\")\n",
    "            user_follow_up = \"Are you sure that is the answer? Do not guess the answer.\"\n",
    "            messages.append({\"role\": \"user\", \"content\": user_follow_up})\n",
    "            text_response_to_save += \"\\n===\\n\" + user_follow_up + \"\\n===\\n\"\n",
    "        elif iteration == 0 and get_gpu_kv_cache_usage(question_id) < 10:\n",
    "            print(\"follow-up - have you verified\")\n",
    "            user_follow_up = \"Have you verified your answer?\"\n",
    "            messages.append({\"role\": \"user\", \"content\": user_follow_up})\n",
    "            text_response_to_save += \"\\n===\\n\" + user_follow_up + \"\\n===\\n\"\n",
    "        else:\n",
    "            # answer found, no issues detected, proceed to answering\n",
    "            break\n",
    "\n",
    "    boxed_text = extract_boxed_text(\n",
    "        text_response_to_save\n",
    "    )  # expected to use the full conversation        \n",
    "\n",
    "    if question_id and text_response_to_save:\n",
    "        answer_suffix = \"\"\n",
    "        if is_valid_answer_string(boxed_text):\n",
    "            answer_suffix = f\"-{boxed_text}\"\n",
    "        with open(f\"solutions/{question_id}/{solution_index:04d}-{generation_idx}{answer_suffix}.txt\", \"w\") as f:\n",
    "            f.write(text_response_to_save)\n",
    "\n",
    "    if is_valid_answer_string(boxed_text):\n",
    "        question_id_to_counter[question_id][int(boxed_text)] += 1\n",
    "        vote_answer(question_id)\n",
    "\n",
    "    return boxed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2d25646",
   "metadata": {
    "_cell_guid": "5a1125d9-7c4a-4bd0-9d82-d7d3305c794e",
    "_uuid": "5390f9d4-9e1e-4cae-b989-731e721daa36",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.542190Z",
     "iopub.status.busy": "2025-12-10T01:23:07.542068Z",
     "iopub.status.idle": "2025-12-10T01:23:07.544149Z",
     "shell.execute_reply": "2025-12-10T01:23:07.543764Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006597,
     "end_time": "2025-12-10T01:23:07.544675",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.538078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_on_kaggle_interactive():\n",
    "    generate_solution(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b737826",
   "metadata": {
    "_cell_guid": "118f9b34-c37a-457b-8bf0-d5c0d3fa3338",
    "_uuid": "5ff403c4-6680-43cd-93a7-7859bacf3995",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.552751Z",
     "iopub.status.busy": "2025-12-10T01:23:07.552633Z",
     "iopub.status.idle": "2025-12-10T01:23:07.556240Z",
     "shell.execute_reply": "2025-12-10T01:23:07.555858Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.008269,
     "end_time": "2025-12-10T01:23:07.556760",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.548491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def solve(question_text: str, question_id: str = \"\") -> int:\n",
    "    await_client()\n",
    "    print(f\"processing {question_id}\")\n",
    "    os.makedirs(f\"solutions/{question_id}\", exist_ok=True)\n",
    "    question_id_to_counter[question_id] = Counter()\n",
    "    completed_question_ids.discard(question_id)  # just in case question_id collides\n",
    "\n",
    "    if question_id and time.time() > cutoff_times[-1]:\n",
    "        print(\"timeout did not solve\")\n",
    "        return 12314\n",
    "\n",
    "    num_generations = 4\n",
    "    get_gpu_kv_cache_usage(question_id)  # run once to prevent running in the first batch of execution\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # run in parallel with different system prompts\n",
    "        results = executor.map(\n",
    "            generate_solution,\n",
    "            [question_text] * num_generations,\n",
    "            [question_id] * num_generations,\n",
    "            list(range(num_generations)),\n",
    "            SYSTEM_PROMPTS,\n",
    "        )\n",
    "        list(results)\n",
    "\n",
    "    final_answer = vote_answer(question_id, force_answer=True)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba3d691f",
   "metadata": {
    "_cell_guid": "80a428f9-c68e-4d1a-8142-4a25ec90d1f2",
    "_uuid": "876c6b69-be5a-4055-91d4-5a1c6bf3f9e4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.564909Z",
     "iopub.status.busy": "2025-12-10T01:23:07.564774Z",
     "iopub.status.idle": "2025-12-10T01:23:07.566937Z",
     "shell.execute_reply": "2025-12-10T01:23:07.566551Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006881,
     "end_time": "2025-12-10T01:23:07.567518",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.560637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_on_kaggle_interactive():\n",
    "    solve(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e853eea7",
   "metadata": {
    "_cell_guid": "285433b8-11c7-4cb2-9b39-f934c2486581",
    "_uuid": "c52ec099-6ff1-43ab-a120-680aa0739024",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003775,
     "end_time": "2025-12-10T01:23:07.575163",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.571388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0528c3d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:07.583489Z",
     "iopub.status.busy": "2025-12-10T01:23:07.583195Z",
     "iopub.status.idle": "2025-12-10T01:23:12.987034Z",
     "shell.execute_reply": "2025-12-10T01:23:12.986522Z"
    },
    "papermill": {
     "duration": 5.409606,
     "end_time": "2025-12-10T01:23:12.988531",
     "exception": false,
     "start_time": "2025-12-10T01:23:07.578925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\"\n",
    ")\n",
    "\n",
    "id_to_answer: dict[str, str] = dict(zip(df[\"id\"], df[\"answer\"]))\n",
    "df.drop(\"answer\", axis=1).to_csv(\"reference.csv\", index=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67fd4f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:12.996866Z",
     "iopub.status.busy": "2025-12-10T01:23:12.996721Z",
     "iopub.status.idle": "2025-12-10T01:23:13.000393Z",
     "shell.execute_reply": "2025-12-10T01:23:13.000001Z"
    },
    "papermill": {
     "duration": 0.008406,
     "end_time": "2025-12-10T01:23:13.000955",
     "exception": false,
     "start_time": "2025-12-10T01:23:12.992549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    global id_to_answer\n",
    "    global correct\n",
    "    global total\n",
    "\n",
    "    # Unpack values\n",
    "    question_id: str = id_.item(0)\n",
    "    question_text: str = problem.item(0)\n",
    "\n",
    "    # Generate prediction\n",
    "    prediction = solve(question_text, question_id=question_id)\n",
    "    completed_question_ids.add(question_id)\n",
    "    cutoff_times.pop()\n",
    "\n",
    "    # ------------------------ SCORING ------------------------\n",
    "    try:\n",
    "        true_answer = int(id_to_answer.get(question_id, -1))\n",
    "    except:\n",
    "        true_answer = -1\n",
    "\n",
    "    total += 1\n",
    "    if prediction == true_answer and true_answer != -1:\n",
    "        correct += 1\n",
    "        print(f\"[debug] correct | score={correct}/{total}\")\n",
    "    else:\n",
    "        print(f\"[debug] WRONG: predicted {prediction}, but actual answer {true_answer} | score={correct}/{total}\")\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    return pl.DataFrame({\"id\": id_, \"answer\": prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6df89fa6",
   "metadata": {
    "_cell_guid": "43eb0762-f519-43d3-ac74-9a56eacdb3bc",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "a77e0235-3654-4b17-8731-e9201473fbad",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-10T01:23:13.008837Z",
     "iopub.status.busy": "2025-12-10T01:23:13.008575Z",
     "iopub.status.idle": "2025-12-10T02:14:43.312085Z",
     "shell.execute_reply": "2025-12-10T02:14:43.311563Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3090.308519,
     "end_time": "2025-12-10T02:14:43.313033",
     "exception": false,
     "start_time": "2025-12-10T01:23:13.004514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dd7f5e\n",
      "score_list |      5.1 over 1 attempts\n",
      "Current GPU usage 24.2\n",
      "       160        5.1        1\n",
      "[debug] correct | score=1/1\n",
      "processing 424e18\n",
      "score_list |     11.0 over 1 attempts\n",
      "Current GPU usage 12.7\n",
      "     62140       11.0        1\n",
      "[debug] WRONG: predicted 62140, but actual answer 21818 | score=1/2\n",
      "processing 641659\n",
      "follow-up - guess answer\n",
      "follow-up - guess answer\n",
      "follow-up - guess answer\n",
      "follow-up - guess answer\n",
      "Current GPU usage 92.0\n",
      "force_answer=True but no answer recorded\n",
      "[debug] WRONG: predicted 12453, but actual answer 57447 | score=1/3\n",
      "processing a295e9\n",
      "score_list |      6.3 over 1 attempts\n",
      "Current GPU usage 45.2\n",
      "       520        6.3        1\n",
      "[debug] correct | score=2/4\n",
      "processing 92ba6a\n",
      "follow-up - have you verified\n",
      "follow-up - have you verified\n",
      "follow-up - have you verified\n",
      "follow-up - have you verified\n",
      "score_list |     15.7 over 4 attempts\n",
      "Current GPU usage 6.9\n",
      "        50       15.7        4\n",
      "[debug] correct | score=3/5\n",
      "processing 86e8e5\n",
      "follow-up - guess answer\n",
      "follow-up - guess answer\n",
      "follow-up - guess answer\n",
      "follow-up - guess answer\n",
      "Current GPU usage 17.9\n",
      "force_answer=True but no answer recorded\n",
      "[debug] WRONG: predicted 12453, but actual answer 8687 | score=3/6\n",
      "processing 9c1c5f\n",
      "score_list |      6.4 over 1 attempts\n",
      "Current GPU usage 21.0\n",
      "       580        6.4        1\n",
      "[debug] correct | score=4/7\n",
      "processing 0e644e\n",
      "score_list |      5.8 over 1 attempts\n",
      "Current GPU usage 17.3\n",
      "       336        5.8        1\n",
      "[debug] correct | score=5/8\n",
      "processing 42d360\n",
      "score_list |     10.4 over 1 attempts\n",
      "Current GPU usage 15.0\n",
      "     32193       10.4        1\n",
      "[debug] correct | score=6/9\n",
      "processing 26de63\n",
      "follow-up - have you verified\n",
      "follow-up - have you verified\n",
      "score_list |     31.2 over 3 attempts\n",
      "Current GPU usage 13.9\n",
      "     32951       31.2        3\n",
      "[debug] correct | score=7/10\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(\n",
    "    predict\n",
    ")\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway((\"reference.csv\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e890c9",
   "metadata": {
    "_cell_guid": "1e7bdc73-4aa2-47ae-a7df-52442398b911",
    "_kg_hide-input": false,
    "_uuid": "800882f9-51ae-4527-9ecc-8b7ddc41e1f9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004805,
     "end_time": "2025-12-10T02:14:43.322788",
     "exception": false,
     "start_time": "2025-12-10T02:14:43.317983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "sourceId": 281315401,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 396608,
     "sourceId": 499291,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 422384,
     "modelInstanceId": 404485,
     "sourceId": 510391,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3147.251691,
   "end_time": "2025-12-10T02:14:44.043522",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-10T01:22:16.791831",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
