{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMO3 - GPT-OSS-120B + Reflexion\n",
    "\n",
    "**Model**: GPT-OSS-120B (OpenAI's open-weight model)\n",
    "**Attach**: `danielhanchen/gpt-oss-120b` or `gpt-oss-120b`\n",
    "\n",
    "Key techniques from top 38/50 solution:\n",
    "- vLLM OpenAI-compatible server\n",
    "- 5 diverse system prompts\n",
    "- Reflexion follow-ups\n",
    "- Log-weighted voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Purge conflicts\n",
    "subprocess.run([\"pip\", \"uninstall\", \"--yes\", \"tensorflow\", \"matplotlib\", \"keras\", \"scikit-learn\"])\n",
    "\n",
    "# Pre-cache model files for faster loading\n",
    "print(\"Caching model files...\")\n",
    "subprocess.run([\"find\", \"/kaggle/usr/lib\", \"-type\", \"f\", \"-print0\"], capture_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import polars as pl\n",
    "\n",
    "# Timing\n",
    "START_TIME = time.time()\n",
    "TOTAL_BUDGET = (4 * 60 + 45) * 60  # 4h45m\n",
    "CUTOFF_TIME = START_TIME + TOTAL_BUDGET\n",
    "\n",
    "ANSWER_MIN, ANSWER_MAX = 0, 99999\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"Budget: {TOTAL_BUDGET//3600}h {(TOTAL_BUDGET%3600)//60}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find GPT-OSS-120B model\n",
    "import glob\n",
    "\n",
    "MODEL_PATHS = [\n",
    "    \"/kaggle/input/gpt-oss-120b/transformers/default/1\",\n",
    "    \"/kaggle/input/gpt-oss-120b\",\n",
    "    \"/kaggle/input/danielhanchen/gpt-oss-120b\",\n",
    "    \"/kaggle/input/d/danielhanchen/gpt-oss-120b\",\n",
    "]\n",
    "\n",
    "MODEL_PATH = None\n",
    "for p in MODEL_PATHS:\n",
    "    if os.path.exists(p):\n",
    "        configs = glob.glob(f\"{p}/**/config.json\", recursive=True)\n",
    "        if configs:\n",
    "            MODEL_PATH = os.path.dirname(configs[0])\n",
    "            break\n",
    "        elif os.path.exists(os.path.join(p, \"config.json\")):\n",
    "            MODEL_PATH = p\n",
    "            break\n",
    "\n",
    "if MODEL_PATH is None:\n",
    "    # Search all inputs\n",
    "    for root, dirs, files in os.walk(\"/kaggle/input\"):\n",
    "        if \"config.json\" in files and \"gpt\" in root.lower():\n",
    "            MODEL_PATH = root\n",
    "            break\n",
    "\n",
    "print(f\"Model path: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache model weights into memory\n",
    "def cache_model(path, num_workers=16, chunk_mb=1024):\n",
    "    import multiprocessing\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    \n",
    "    exts = (\".bin\", \".pt\", \".safetensors\")\n",
    "    \n",
    "    def warmup_file(fpath):\n",
    "        chunk_size = chunk_mb * 1024 * 1024\n",
    "        total = 0\n",
    "        with open(fpath, \"rb\") as f:\n",
    "            while True:\n",
    "                data = f.read(chunk_size)\n",
    "                if not data:\n",
    "                    break\n",
    "                total += len(data)\n",
    "        return fpath, total\n",
    "    \n",
    "    files = [\n",
    "        os.path.join(root, name)\n",
    "        for root, _, names in os.walk(path)\n",
    "        for name in names\n",
    "        if name.endswith(exts)\n",
    "    ]\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No model files found in {path}\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"Caching {len(files)} files...\")\n",
    "    t0 = time.time()\n",
    "    total_bytes = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as pool:\n",
    "        futures = {pool.submit(warmup_file, f): f for f in files}\n",
    "        for i, fut in enumerate(as_completed(futures), 1):\n",
    "            fpath, n = fut.result()\n",
    "            total_bytes += n\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  [{i}/{len(files)}] cached\")\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    gb = total_bytes / 1024**3\n",
    "    print(f\"Cached {gb:.2f} GB in {elapsed:.1f}s ({gb/elapsed:.2f} GB/s)\")\n",
    "    return total_bytes\n",
    "\n",
    "if MODEL_PATH:\n",
    "    cache_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start vLLM server\n",
    "import subprocess\n",
    "\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"TRITON_ATTN\"\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "SEQUENCE_LENGTH = 65536\n",
    "\n",
    "command = [\n",
    "    \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--model\", MODEL_PATH,\n",
    "    \"--served-model-name\", \"vllm-model\",\n",
    "    \"--tensor-parallel-size\", \"1\",\n",
    "    \"--max-num-seqs\", \"4\",\n",
    "    \"--gpu-memory-utilization\", \"0.96\",\n",
    "    \"--host\", \"0.0.0.0\",\n",
    "    \"--port\", \"8000\",\n",
    "    \"--dtype\", \"auto\",\n",
    "    \"--max-model-len\", str(SEQUENCE_LENGTH),\n",
    "]\n",
    "\n",
    "with open(\"/kaggle/working/vllm.log\", \"w\") as logfile:\n",
    "    vllm_process = subprocess.Popen(\n",
    "        command, stdout=logfile, stderr=subprocess.STDOUT, start_new_session=True\n",
    "    )\n",
    "\n",
    "print(f\"vLLM server starting (PID: {vllm_process.pid})\")\n",
    "print(\"Logs: /kaggle/working/vllm.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI client\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://127.0.0.1:8000/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-local\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:8000/v1\",\n",
    "    api_key=\"sk-local\",\n",
    ")\n",
    "\n",
    "# GPT-OSS stop tokens\n",
    "STOP_TOKEN_IDS = [\n",
    "    token_id for token_id in range(200_000, 201_088)\n",
    "    if token_id not in [200005, 200006, 200007, 200008]\n",
    "]\n",
    "\n",
    "# Wait for server\n",
    "def await_server(max_wait=900):\n",
    "    for _ in range(max_wait):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            client.models.list()\n",
    "            print(\"Server ready!\")\n",
    "            return True\n",
    "        except:\n",
    "            continue\n",
    "    print(\"Server timeout!\")\n",
    "    return False\n",
    "\n",
    "await_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompts (from top solutions)\n",
    "SYSTEM_PROMPTS = [\n",
    "    \"\"\"You are solving a national/international-level mathematics olympiad problem. \n",
    "Rigorously define all variables, explore multiple solution strategies, \n",
    "perform full case analysis, justify every step, check boundary cases, \n",
    "and verify using at least one independent method. \n",
    "Return only the final numerical answer inside \\\\boxed{}. \n",
    "Answer must be an integer in [0, 99999]. Never guess.\"\"\",\n",
    "\n",
    "    \"\"\"Solve with full rigor. After obtaining a candidate solution, \n",
    "actively attempt to refute it by searching for counterexamples, \n",
    "re-running logic from a different viewpoint, and stress-testing edge cases. \n",
    "Only after the answer survives refutation, return it in \\\\boxed{}. \n",
    "Answer must be an integer in [0, 99999]. Never guess.\"\"\",\n",
    "\n",
    "    \"\"\"Solve as if under IMO time pressure: identify the key invariant, \n",
    "symmetry, or extremal principle early. Avoid brute force unless justified. \n",
    "Compress reasoning without sacrificing correctness. \n",
    "Perform at least one arithmetic verification pass. \n",
    "Return only the final integer in \\\\boxed{}, 0 ≤ answer ≤ 99999. Never guess.\"\"\",\n",
    "\n",
    "    \"\"\"Attempt at least two fundamentally different solution approaches \n",
    "(algebraic vs geometric, combinatorial vs number-theoretic). \n",
    "Proceed with the more rigorous one and use the other for verification. \n",
    "Return only the verified answer in \\\\boxed{}, integer in [0, 99999]. Never guess.\"\"\",\n",
    "\n",
    "    \"\"\"Solve rigorously. If any step relies on an unproven assumption, \n",
    "has a logic jump, or becomes inconsistent, restart from first principles. \n",
    "Return only the verified integer in \\\\boxed{}, 0 ≤ answer ≤ 99999. Never guess.\"\"\"\n",
    "]\n",
    "\n",
    "print(f\"{len(SYSTEM_PROMPTS)} system prompts loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer extraction\n",
    "def extract_boxed(text: str) -> Optional[str]:\n",
    "    pattern = r\"oxed{(.*?)}\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    for m in reversed(matches):\n",
    "        if m.strip():\n",
    "            return m.strip()\n",
    "    return None\n",
    "\n",
    "def is_valid_answer(text: str) -> bool:\n",
    "    try:\n",
    "        val = int(text)\n",
    "        return 0 <= val <= 99999\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Log-weighted voting (from top 27/50)\n",
    "def vote_answer(counter: Counter, force: bool = False) -> Tuple[Optional[int], bool]:\n",
    "    if not counter:\n",
    "        return (12453 if force else None, False)\n",
    "    \n",
    "    scores = {v: math.log(1.25 + abs(v)) * c for v, c in counter.items()}\n",
    "    total = sum(scores.values())\n",
    "    ranked = sorted(scores.items(), key=lambda x: -x[1])\n",
    "    \n",
    "    best_val, best_score = ranked[0]\n",
    "    threshold = total / (2 + math.log(1 + total))\n",
    "    confident = best_score > max(3, threshold)\n",
    "    \n",
    "    if len(ranked) == 1 or (len(ranked) > 1 and best_score - ranked[1][1] > 1):\n",
    "        confident = True\n",
    "    \n",
    "    if force:\n",
    "        print(f\"  Vote: {ranked[:5]}\")\n",
    "    \n",
    "    return (best_val, confident)\n",
    "\n",
    "print(\"Utilities loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation with reflexion\n",
    "COMPLETED_IDS = set()\n",
    "\n",
    "def generate_solution(question: str, question_id: str, idx: int, system_prompt: str, counter: Counter) -> Optional[int]:\n",
    "    if question_id in COMPLETED_IDS or time.time() >= CUTOFF_TIME:\n",
    "        return None\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    for iteration in range(2):\n",
    "        if question_id in COMPLETED_IDS or time.time() >= CUTOFF_TIME:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"vllm-model\",\n",
    "                messages=messages,\n",
    "                max_tokens=4096,\n",
    "                temperature=0.7 if iteration == 0 else 0.5,\n",
    "                extra_body=dict(\n",
    "                    min_p=0.02,\n",
    "                    stop_token_ids=STOP_TOKEN_IDS,\n",
    "                    chat_template_kwargs=dict(enable_thinking=True)\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            response = resp.choices[0].message.content or \"\"\n",
    "            if hasattr(resp.choices[0].message, 'reasoning_content') and resp.choices[0].message.reasoning_content:\n",
    "                response = resp.choices[0].message.reasoning_content + \"\\n\" + response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Gen error: {e}\")\n",
    "            break\n",
    "        \n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        full_response += response\n",
    "        \n",
    "        boxed = extract_boxed(response)\n",
    "        \n",
    "        # Reflexion logic\n",
    "        if not boxed or not is_valid_answer(boxed):\n",
    "            messages.append({\"role\": \"user\", \"content\": \"Place your final integer answer in \\\\boxed{}. Answer must be 0-99999.\"})\n",
    "        elif int(boxed) <= 10:\n",
    "            messages.append({\"role\": \"user\", \"content\": \"Are you sure? Double-check your work.\"})\n",
    "        elif iteration == 0 and len(response) < 1000:\n",
    "            messages.append({\"role\": \"user\", \"content\": \"Have you verified your answer?\"})\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    boxed = extract_boxed(full_response)\n",
    "    if boxed and is_valid_answer(boxed):\n",
    "        ans = int(boxed)\n",
    "        counter[ans] += 1\n",
    "        return ans\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"Generation with reflexion loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main solver\n",
    "def solve(question: str, question_id: str) -> int:\n",
    "    global COMPLETED_IDS\n",
    "    \n",
    "    if time.time() >= CUTOFF_TIME:\n",
    "        return 12453\n",
    "    \n",
    "    counter = Counter()\n",
    "    num_gens = 4\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                generate_solution,\n",
    "                question, question_id, i,\n",
    "                SYSTEM_PROMPTS[i % len(SYSTEM_PROMPTS)],\n",
    "                counter\n",
    "            )\n",
    "            for i in range(num_gens)\n",
    "        ]\n",
    "        \n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                fut.result()\n",
    "                ans, confident = vote_answer(counter)\n",
    "                if confident and ans is not None:\n",
    "                    COMPLETED_IDS.add(question_id)\n",
    "                    print(f\"  Early stop: {ans}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "    \n",
    "    answer, _ = vote_answer(counter, force=True)\n",
    "    \n",
    "    if answer is None:\n",
    "        nums = [int(x) for x in re.findall(r'\\b\\d+\\b', question) if 0 < int(x) < 100000]\n",
    "        answer = nums[0] if nums else 12453\n",
    "    \n",
    "    return max(0, min(99999, int(answer)))\n",
    "\n",
    "print(\"Solver loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API\n",
    "SOLVED = 0\n",
    "\n",
    "def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame:\n",
    "    global SOLVED\n",
    "    SOLVED += 1\n",
    "    \n",
    "    qid = id_.item(0)\n",
    "    question = problem.item(0)\n",
    "    \n",
    "    time_left = (CUTOFF_TIME - time.time()) / 60\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Problem {SOLVED} | {qid} | {time_left:.1f}m left\")\n",
    "    print(f\"Q: {question[:60]}...\")\n",
    "    \n",
    "    answer = solve(question, qid)\n",
    "    COMPLETED_IDS.add(qid)\n",
    "    \n",
    "    print(f\"ANSWER: {answer}\")\n",
    "    return pl.DataFrame({\"id\": id_, \"answer\": answer})\n",
    "\n",
    "print(\"API ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AIMO3 - GPT-OSS-120B + Reflexion\")\n",
    "print(f\"Model: {MODEL_PATH}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    server.serve()\n",
    "else:\n",
    "    server.run_local_gateway(\n",
    "        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n",
    "    )\n",
    "\n",
    "print(f\"\\nDone in {(time.time()-START_TIME)/60:.1f}m | Solved: {SOLVED}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
