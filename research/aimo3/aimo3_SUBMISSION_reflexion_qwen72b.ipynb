{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROMETHEUS v2 + Reflexion Prompting\n",
    "\n",
    "## Key Improvements from Top 27/50 Notebook:\n",
    "1. **5 Different System Prompts** - Diverse reasoning strategies\n",
    "2. **Reflexion Prompting** - Follow-up verification (\"Are you sure?\")\n",
    "3. **Log-Weighted Voting** - Penalize trivial small answers\n",
    "4. **Early Stopping** - Stop when confidence is high\n",
    "5. **Streaming + Early Exit** - Stop generation when \\boxed{} found\n",
    "\n",
    "**Model**: Qwen-72B-Math-INT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import gc\n",
    "\n",
    "# Purge TensorFlow conflicts\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \n",
    "                \"tensorflow\", \"keras\", \"scikit-learn\", \"matplotlib\"], capture_output=True)\n",
    "\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Install vLLM from wheels\n",
    "WHEEL_DIR = \"/kaggle/input/vllm-085-wheels\"\n",
    "AUX_DIR = \"/kaggle/input/aimo3-offline-wheels\"\n",
    "\n",
    "if os.path.exists(WHEEL_DIR):\n",
    "    result = subprocess.run([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \"vllm\",\n",
    "        \"--no-index\", f\"--find-links={WHEEL_DIR}\", f\"--find-links={AUX_DIR}\"\n",
    "    ], capture_output=True)\n",
    "    print(\"vLLM installed\" if result.returncode == 0 else f\"vLLM failed: {result.stderr[-200:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: IMPORTS & TIMING\n",
    "# =============================================================================\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import tempfile\n",
    "import statistics\n",
    "from typing import Optional, List, Tuple, Dict, Any\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# Seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Timing (AIMO3 Rule: GPU <= 5 hours)\n",
    "START_TIME = time.time()\n",
    "TOTAL_BUDGET = (4 * 60 + 40) * 60  # 4h40m safety\n",
    "CUTOFF_TIME = START_TIME + TOTAL_BUDGET\n",
    "\n",
    "# Answer bounds\n",
    "ANSWER_MIN, ANSWER_MAX = 0, 99999\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"Budget: {TOTAL_BUDGET//3600}h {(TOTAL_BUDGET%3600)//60}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: MODEL LOADING (Qwen-72B-Math-INT4)\n",
    "# =============================================================================\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "MODEL_PATHS = [\n",
    "    \"/kaggle/input/qwen-72b-math-int4\",\n",
    "    \"/kaggle/input/d/ryancardwell/qwen-72b-math-int4\",\n",
    "    \"/kaggle/input/qwen-72b-math-nf4\",\n",
    "]\n",
    "\n",
    "def find_model():\n",
    "    import glob\n",
    "    for p in MODEL_PATHS:\n",
    "        if os.path.exists(p) and os.path.exists(os.path.join(p, \"config.json\")):\n",
    "            return p\n",
    "        configs = glob.glob(f\"{p}/**/config.json\", recursive=True)\n",
    "        if configs:\n",
    "            return os.path.dirname(configs[0])\n",
    "    return None\n",
    "\n",
    "MODEL_PATH = find_model()\n",
    "print(f\"Model: {MODEL_PATH}\")\n",
    "\n",
    "LLM_MODEL = None\n",
    "if MODEL_PATH:\n",
    "    LLM_MODEL = LLM(\n",
    "        model=MODEL_PATH,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.92,\n",
    "        trust_remote_code=True,\n",
    "        max_model_len=8192,\n",
    "        enforce_eager=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: REFLEXION SYSTEM PROMPTS (from top 27/50 notebook)\n",
    "# =============================================================================\n",
    "\n",
    "SYSTEM_PROMPTS = [\n",
    "    # Rigorous multi-method\n",
    "    \"\"\"You are solving a national/international-level mathematics olympiad problem. \n",
    "You must rigorously define all variables, explore multiple solution strategies before committing, \n",
    "perform full case analysis where required, justify every nontrivial step, \n",
    "explicitly check boundary cases and hidden assumptions, \n",
    "and verify the final result using at least one independent method. \n",
    "Return only the final numerical answer inside \\\\boxed{}. \n",
    "The answer must be an integer in [0, 99999]. Never guess.\"\"\",\n",
    "\n",
    "    # Self-refutation\n",
    "    \"\"\"Solve the problem with full rigor. After obtaining a candidate solution, \n",
    "actively attempt to refute your own answer by searching for counterexamples, \n",
    "re-running the logic from a different viewpoint, and stress-testing edge cases. \n",
    "Only after the answer survives refutation, return it in \\\\boxed{}. \n",
    "The answer must be an integer in [0, 99999]. Never guess.\"\"\",\n",
    "\n",
    "    # IMO-style efficiency\n",
    "    \"\"\"Solve this problem as if under IMO-level time pressure: \n",
    "identify the key invariant, symmetry, or extremal principle early, \n",
    "avoid brute force unless strictly justified, compress reasoning without sacrificing correctness, \n",
    "and perform at least one final arithmetic verification pass. \n",
    "Return only the final integer answer in \\\\boxed{}, with 0 ≤ answer ≤ 99999. Never guess.\"\"\",\n",
    "\n",
    "    # Dual approach\n",
    "    \"\"\"You must attempt at least two fundamentally different solution approaches \n",
    "(e.g., algebraic vs geometric, combinatorial vs number-theoretic). \n",
    "Proceed with the more rigorous one and use the other as a verification tool. \n",
    "Return only the verified final answer in \\\\boxed{}, where the answer is an integer in [0, 99999]. Never guess.\"\"\",\n",
    "\n",
    "    # Restart on inconsistency\n",
    "    \"\"\"Solve the problem rigorously. If at any point a step relies on an unproven assumption, \n",
    "a jump in logic is detected, or the computation becomes inconsistent, \n",
    "you must restart the solution from first principles. \n",
    "Return only the final verified integer answer inside \\\\boxed{}, with 0 ≤ answer ≤ 99999. Never guess.\"\"\"\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(SYSTEM_PROMPTS)} system prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: ANSWER EXTRACTION & VALIDATION\n",
    "# =============================================================================\n",
    "\n",
    "def extract_boxed(text: str) -> Optional[str]:\n",
    "    \"\"\"Extract text inside \\\\boxed{}.\"\"\"\n",
    "    pattern = r\"oxed{(.*?)}\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return None\n",
    "    # Return last non-empty match\n",
    "    for m in reversed(matches):\n",
    "        if m.strip():\n",
    "            return m.strip()\n",
    "    return None\n",
    "\n",
    "def is_valid_answer(text: str) -> bool:\n",
    "    \"\"\"Check if answer is valid integer in [0, 99999].\"\"\"\n",
    "    try:\n",
    "        val = int(text)\n",
    "        return 0 <= val <= 99999\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def normalize_answer(raw: Any) -> Optional[int]:\n",
    "    \"\"\"Convert raw output to integer answer.\"\"\"\n",
    "    if raw is None:\n",
    "        return None\n",
    "    try:\n",
    "        s = str(raw).replace(',', '').strip()\n",
    "        s = re.sub(r'[^0-9.-]', '', s)\n",
    "        if not s:\n",
    "            return None\n",
    "        val = int(float(s))\n",
    "        if 0 <= val <= 99999:\n",
    "            return val\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "print(\"Answer utilities loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: LOG-WEIGHTED VOTING (from top 27/50 notebook)\n",
    "# =============================================================================\n",
    "\n",
    "def vote_answer(counter: Counter, force_answer: bool = False) -> Tuple[Optional[int], bool]:\n",
    "    \"\"\"\n",
    "    Log-weighted voting from top 27/50 notebook.\n",
    "    \n",
    "    Key insight: Small answers (0, 1, 2) are often wrong guesses.\n",
    "    Weight by log(1.25 + |value|) to penalize trivial answers.\n",
    "    \n",
    "    Returns: (answer, is_confident)\n",
    "    \"\"\"\n",
    "    if not counter:\n",
    "        return (12453 if force_answer else None, False)\n",
    "    \n",
    "    # Compute log-weighted scores\n",
    "    modified_counter = Counter()\n",
    "    for value, count in counter.items():\n",
    "        # log(1.25 + |value|) penalizes small answers\n",
    "        weight = math.log(1.25 + abs(value)) * count\n",
    "        modified_counter[value] = weight\n",
    "    \n",
    "    total_score = sum(modified_counter.values())\n",
    "    score_list = sorted(\n",
    "        [(score, counter[value], value) for value, score in modified_counter.items()],\n",
    "        key=lambda x: -x[0]\n",
    "    )\n",
    "    \n",
    "    best_score, best_count, best_value = score_list[0]\n",
    "    \n",
    "    # Confidence check from top notebook:\n",
    "    # Stop when best > total / (2 + log(1 + total))\n",
    "    threshold = total_score / (2 + math.log(1 + total_score))\n",
    "    is_confident = best_score > max(3, threshold)\n",
    "    \n",
    "    # Additional confidence: single answer or clear winner\n",
    "    if len(score_list) == 1:\n",
    "        is_confident = True\n",
    "    elif len(score_list) > 1 and best_score - score_list[1][0] > 1:\n",
    "        is_confident = True\n",
    "    \n",
    "    if force_answer:\n",
    "        print(f\"  Vote: {[(v, s, c) for s, c, v in score_list[:5]]}\")\n",
    "    \n",
    "    return (best_value, is_confident)\n",
    "\n",
    "print(\"Log-weighted voting loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: GENERATION WITH REFLEXION\n",
    "# =============================================================================\n",
    "\n",
    "STOP_TOKENS = [\"```output\", \"```\\nOutput\", \"\\n\\n\\n\", \"<|im_end|>\"]\n",
    "\n",
    "def generate_with_reflexion(\n",
    "    question: str,\n",
    "    system_prompt: str,\n",
    "    counter: Counter,\n",
    "    completed: set,\n",
    "    question_id: str = \"\",\n",
    "    max_iterations: int = 2\n",
    ") -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Generate answer with reflexion prompting.\n",
    "    \n",
    "    Reflexion: After initial answer, follow up with:\n",
    "    - \"Are you sure?\" for small answers\n",
    "    - \"Have you verified?\" for quick answers\n",
    "    - \"Put in \\\\boxed{}\" if no boxed answer\n",
    "    \"\"\"\n",
    "    if question_id in completed:\n",
    "        return None\n",
    "    if time.time() >= CUTOFF_TIME:\n",
    "        return None\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Generate\n",
    "        prompt = format_messages(messages)\n",
    "        params = SamplingParams(\n",
    "            temperature=0.7 if iteration == 0 else 0.5,\n",
    "            top_p=0.9,\n",
    "            max_tokens=4096,\n",
    "            stop=STOP_TOKENS,\n",
    "        )\n",
    "        \n",
    "        outputs = LLM_MODEL.generate([prompt], sampling_params=params)\n",
    "        response = outputs[0].outputs[0].text\n",
    "        \n",
    "        # Check for early stop\n",
    "        if question_id in completed:\n",
    "            break\n",
    "        if time.time() >= CUTOFF_TIME:\n",
    "            break\n",
    "        \n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        # Extract answer\n",
    "        boxed = extract_boxed(response)\n",
    "        \n",
    "        # Reflexion logic\n",
    "        if not boxed or not is_valid_answer(boxed):\n",
    "            # No valid answer - ask for boxed\n",
    "            follow_up = \"The answer is expected to be an integer between 0 and 99999 inclusive. Place your final answer in \\\\boxed{}. Do not guess.\"\n",
    "            messages.append({\"role\": \"user\", \"content\": follow_up})\n",
    "        elif int(boxed) <= 10:\n",
    "            # Small answer - ask for verification\n",
    "            follow_up = \"Are you sure that is the answer? Double-check your work.\"\n",
    "            messages.append({\"role\": \"user\", \"content\": follow_up})\n",
    "        elif iteration == 0 and len(response) < 1000:\n",
    "            # Quick answer - ask for verification\n",
    "            follow_up = \"Have you verified your answer using an alternative method?\"\n",
    "            messages.append({\"role\": \"user\", \"content\": follow_up})\n",
    "        else:\n",
    "            # Good answer - break\n",
    "            break\n",
    "    \n",
    "    # Final extraction\n",
    "    full_text = \"\\n\".join([m[\"content\"] for m in messages if m[\"role\"] == \"assistant\"])\n",
    "    boxed = extract_boxed(full_text)\n",
    "    \n",
    "    if boxed and is_valid_answer(boxed):\n",
    "        answer = int(boxed)\n",
    "        counter[answer] += 1\n",
    "        return answer\n",
    "    \n",
    "    return None\n",
    "\n",
    "def format_messages(messages: List[Dict]) -> str:\n",
    "    \"\"\"Format messages for Qwen chat template.\"\"\"\n",
    "    parts = []\n",
    "    for m in messages:\n",
    "        role = m[\"role\"]\n",
    "        content = m[\"content\"]\n",
    "        parts.append(f\"<|im_start|>{role}\\n{content}<|im_end|>\")\n",
    "    parts.append(\"<|im_start|>assistant\\n\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "print(\"Reflexion generation loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: PARALLEL SOLVER WITH EARLY STOPPING\n",
    "# =============================================================================\n",
    "\n",
    "PROBLEM_COUNTER = Counter()\n",
    "COMPLETED_IDS = set()\n",
    "\n",
    "def solve(question: str, question_id: str = \"\") -> int:\n",
    "    \"\"\"\n",
    "    Solve with parallel generation + early stopping.\n",
    "    \n",
    "    1. Launch 4 parallel generations with different system prompts\n",
    "    2. Stop early when confidence is high\n",
    "    3. Vote on final answer with log-weighting\n",
    "    \"\"\"\n",
    "    if not LLM_MODEL:\n",
    "        print(\"  No model - returning default\")\n",
    "        return 12453\n",
    "    \n",
    "    if time.time() >= CUTOFF_TIME:\n",
    "        print(\"  Timeout - returning default\")\n",
    "        return 12453\n",
    "    \n",
    "    counter = Counter()\n",
    "    completed = set()\n",
    "    \n",
    "    num_generations = min(4, len(SYSTEM_PROMPTS))\n",
    "    \n",
    "    # Parallel generation with different system prompts\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                generate_with_reflexion,\n",
    "                question,\n",
    "                SYSTEM_PROMPTS[i % len(SYSTEM_PROMPTS)],\n",
    "                counter,\n",
    "                completed,\n",
    "                question_id\n",
    "            )\n",
    "            for i in range(num_generations)\n",
    "        ]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    # Check for early stopping\n",
    "                    answer, confident = vote_answer(counter)\n",
    "                    if confident:\n",
    "                        completed.add(question_id)\n",
    "                        print(f\"  Early stop: {answer} (confident)\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Generation error: {e}\")\n",
    "    \n",
    "    # Final vote\n",
    "    answer, _ = vote_answer(counter, force_answer=True)\n",
    "    \n",
    "    if answer is None:\n",
    "        # Fallback: extract numbers from question\n",
    "        nums = [int(x) for x in re.findall(r'\\b\\d+\\b', question) if 0 < int(x) < 100000]\n",
    "        answer = nums[0] if nums else 12453\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    answer = max(ANSWER_MIN, min(ANSWER_MAX, int(answer)))\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "print(\"Parallel solver loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: KAGGLE API INTERFACE\n",
    "# =============================================================================\n",
    "\n",
    "SOLVED = 0\n",
    "TOTAL = 0\n",
    "\n",
    "def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame:\n",
    "    \"\"\"AIMO3 API predict function.\"\"\"\n",
    "    global SOLVED, TOTAL\n",
    "    TOTAL += 1\n",
    "    \n",
    "    question_id = id_.item(0)\n",
    "    question = problem.item(0)\n",
    "    \n",
    "    time_left = (CUTOFF_TIME - time.time()) / 60\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Problem {TOTAL} | ID: {question_id} | Time: {time_left:.1f}m\")\n",
    "    print(f\"Q: {question[:80]}...\")\n",
    "    \n",
    "    answer = solve(question, question_id)\n",
    "    \n",
    "    print(f\"ANSWER: {answer}\")\n",
    "    SOLVED += 1\n",
    "    \n",
    "    return pl.DataFrame({\"id\": id_, \"answer\": answer})\n",
    "\n",
    "print(\"API interface ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: RUN\n",
    "# =============================================================================\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROMETHEUS v2 + Reflexion\")\n",
    "print(f\"Model: {MODEL_PATH}\")\n",
    "print(f\"Budget: {TOTAL_BUDGET//3600}h {(TOTAL_BUDGET%3600)//60}m\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n",
    "    )\n",
    "\n",
    "print(f\"\\nDone in {(time.time() - START_TIME)/60:.1f}m | Solved: {SOLVED}/{TOTAL}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
