{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PROMETHEUS v2.0 + Tunix: CIC-Guided Reasoning Training\n",
    "\n",
    "## Synthesized with 20+ Novel Insights\n",
    "\n",
    "**Key Innovations from PROMETHEUS SYNTHESIS ENCYCLOPEDIA:**\n",
    "1. **NCD Basin Regularization** - Reward traces close to canonical patterns\n",
    "2. **ToM-Informed Reward Shaping** - Optimize for human collaboration\n",
    "3. **CIC Functional Scoring** - F[T] = Φ(T) - λH(T|X) + γC(T)\n",
    "4. **Causal Emergence Rewards** - Macro explains micro\n",
    "5. **Coherence Graph Scoring** - Logical flow, not just keywords\n",
    "6. **Temperature-κ Coupling** - Adaptive exploration\n",
    "7. **Trace Diversity via NCD** - Prevent mode collapse\n",
    "8. **Kolmogorov Regularization** - Reward concise reasoning\n",
    "\n",
    "**From Riedl & Weidmann (2025):**\n",
    "- ToM predicts AI collaboration (ρs=0.17, p<0.001)\n",
    "- Solo ability has ZERO correlation (β=-0.00)\n",
    "- Optimize for κ (collaborative ability), not θ (solo)\n",
    "\n",
    "**Competition Requirements:**\n",
    "- Model: Gemma2 2B or Gemma3 1B\n",
    "- Output: `<reasoning>trace</reasoning><answer>answer</answer>`\n",
    "- Hardware: TPU v5e-8 (9hr session, 20hr/week)\n",
    "- Framework: Tunix (JAX-native GRPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: SETUP & INSTALLATION\n",
    "# =============================================================================\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q kagglehub\n",
    "!pip install -q ipywidgets\n",
    "!pip install -q tensorflow\n",
    "!pip install -q tensorflow_datasets\n",
    "!pip install -q tensorboardX\n",
    "!pip install -q transformers\n",
    "!pip install -q grain\n",
    "!pip install \"google-tunix[prod]==0.1.3\"\n",
    "!pip uninstall -q -y flax\n",
    "!pip install -U flax\n",
    "!pip install -q datasets\n",
    "!pip install -q scipy  # For NCD clustering\n",
    "\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: IMPORTS\n",
    "# =============================================================================\n",
    "import functools\n",
    "import gc\n",
    "import os\n",
    "import zlib  # For NCD computation\n",
    "from pprint import pprint\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "from pathlib import Path\n",
    "import qwix\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm.auto import tqdm\n",
    "from tunix.generate import sampler as sampler_lib\n",
    "from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "from tunix.models.gemma3 import params\n",
    "from tunix.models.gemma3 import model\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.sft import metrics_logger\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Config\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: HYPERPARAMETERS (\"Efficiency Build\" for TPU v5e-8)\n",
    "# =============================================================================\n",
    "\n",
    "# ====== Data ======\n",
    "TRAIN_FRACTION = 1.0\n",
    "\n",
    "# ====== LoRA ======\n",
    "RANK = 32\n",
    "ALPHA = 32.0\n",
    "\n",
    "# ====== Sharding ======\n",
    "MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
    "\n",
    "# ====== GRPO Generation ======\n",
    "MAX_PROMPT_LENGTH = 256\n",
    "TOTAL_GENERATION_STEPS = 512\n",
    "\n",
    "# ====== ADAPTIVE TEMPERATURE (PROMETHEUS v2.0) ======\n",
    "# Now coupled to training dynamics via κ proxy\n",
    "TEMP_START = 1.2\n",
    "TEMP_END = 0.5\n",
    "TEMP_MIN = 0.3\n",
    "TEMP_MAX = 1.5\n",
    "\n",
    "TOP_P = 1.0\n",
    "TOP_K = 50\n",
    "NUM_GENERATIONS = 4\n",
    "\n",
    "# ====== GRPO Config ======\n",
    "NUM_ITERATIONS = 1\n",
    "BETA = 0.08\n",
    "EPSILON = 0.2\n",
    "\n",
    "# ====== Training ======\n",
    "TRAIN_MICRO_BATCH_SIZE = 2\n",
    "NUM_BATCHES = 3738\n",
    "NUM_TEST_BATCHES = 100\n",
    "EVAL_EVERY_N_STEPS = 10\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
    "\n",
    "# ====== AdamW + Warmup + Cosine Scheduler ======\n",
    "LEARNING_RATE = 3e-6\n",
    "B1 = 0.9\n",
    "B2 = 0.99\n",
    "WEIGHT_DECAY = 0.1\n",
    "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
    "MAX_GRAD_NORM = 0.1\n",
    "\n",
    "# ====== Checkpointing ======\n",
    "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
    "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
    "SAVE_INTERVAL_STEPS = 500\n",
    "MAX_TO_KEEP = 4\n",
    "\n",
    "# ====== Inference ======\n",
    "GENERATION_CONFIGS = {\n",
    "    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n",
    "    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n",
    "}\n",
    "\n",
    "print(f\"Config: RANK={RANK}, ALPHA={ALPHA}, LR={LEARNING_RATE}\")\n",
    "print(f\"Training: {MAX_STEPS} steps, batch={TRAIN_MICRO_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: NCD PRIMITIVES (Core of PROMETHEUS v2.0)\n",
    "# =============================================================================\n",
    "# From Li et al. (2004) \"The Similarity Metric\"\n",
    "# =============================================================================\n",
    "\n",
    "def ncd(x: str, y: str) -> float:\n",
    "    \"\"\"Normalized Compression Distance using zlib.\"\"\"\n",
    "    if not x or not y:\n",
    "        return 1.0\n",
    "    cx = len(zlib.compress(x.encode(), level=9))\n",
    "    cy = len(zlib.compress(y.encode(), level=9))\n",
    "    cxy = len(zlib.compress((x + y).encode(), level=9))\n",
    "    return (cxy - min(cx, cy)) / max(cx, cy)\n",
    "\n",
    "\n",
    "def kolmogorov_proxy(text: str) -> float:\n",
    "    \"\"\"Approximate Kolmogorov complexity via compression ratio.\"\"\"\n",
    "    if not text:\n",
    "        return 1.0\n",
    "    original = len(text.encode())\n",
    "    compressed = len(zlib.compress(text.encode(), level=9))\n",
    "    return compressed / original\n",
    "\n",
    "\n",
    "print(\"NCD primitives loaded.\")\n",
    "print(f\"  Test: ncd('hello', 'hello') = {ncd('hello', 'hello'):.3f}\")\n",
    "print(f\"  Test: ncd('hello', 'world') = {ncd('hello', 'world'):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: TEMPLATE & BASIN CENTERS\n",
    "# =============================================================================\n",
    "\n",
    "reasoning_start = \"<reasoning>\"\n",
    "reasoning_end = \"</reasoning>\"\n",
    "solution_start = \"<answer>\"\n",
    "solution_end = \"</answer>\"\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\n",
    "provide your reasoning. Place it between {reasoning_start} and \\\n",
    "{reasoning_end}. Then, provide the final answer between {solution_start} \\\n",
    "and {solution_end}.\"\"\"\n",
    "\n",
    "TEMPLATE = \"\"\"<start_of_turn>user\n",
    "{system_prompt}\n",
    "\n",
    "{question}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# BASIN CENTERS - Canonical reasoning patterns for training\n",
    "# Reward traces that compress well with these patterns\n",
    "# =============================================================================\n",
    "CANONICAL_TRACES = [\n",
    "    # Step-by-step pattern\n",
    "    \"\"\"Let me break this down step by step.\n",
    "First, I identify the key elements of the problem.\n",
    "Next, I consider the relationships between them.\n",
    "Then, I apply the relevant principle or formula.\n",
    "Finally, I combine these to reach the conclusion.\"\"\",\n",
    "    \n",
    "    # Hypothesis-test pattern\n",
    "    \"\"\"I'll approach this by forming a hypothesis.\n",
    "Hypothesis: The answer has property X.\n",
    "Testing: If this is true, then we should see Y.\n",
    "Verification: Checking this condition...\n",
    "The hypothesis holds, so the conclusion follows.\"\"\",\n",
    "    \n",
    "    # Case analysis pattern\n",
    "    \"\"\"This problem has several cases to consider.\n",
    "Case 1: When the first condition applies.\n",
    "Case 2: When the second condition applies.\n",
    "Combining the cases gives the complete answer.\"\"\",\n",
    "    \n",
    "    # Deductive pattern\n",
    "    \"\"\"Given the premises, I can deduce the following.\n",
    "From A, it follows that B.\n",
    "Combined with C, this implies D.\n",
    "Therefore, the answer is E.\"\"\",\n",
    "]\n",
    "\n",
    "print(\"Template and basin centers configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: DATA LOADING (Real User Tasks)\n",
    "# =============================================================================\n",
    "\n",
    "def get_real_user_dataset(max_samples=10000) -> grain.MapDataset:\n",
    "    \"\"\"Load real user conversation datasets.\"\"\"\n",
    "    all_questions = []\n",
    "    \n",
    "    # OpenAssistant\n",
    "    print(\"Loading OpenAssistant...\")\n",
    "    try:\n",
    "        oasst = load_dataset(\"OpenAssistant/oasst1\", split=\"train\")\n",
    "        for item in oasst:\n",
    "            if item.get(\"role\") == \"prompter\" and item.get(\"parent_id\") is None:\n",
    "                text = item.get(\"text\", \"\").strip()\n",
    "                if 20 < len(text) < 500:\n",
    "                    all_questions.append({\"question\": text, \"source\": \"oasst\", \"answer\": None})\n",
    "        print(f\"  -> {len([q for q in all_questions if q['source']=='oasst'])} from OpenAssistant\")\n",
    "    except Exception as e:\n",
    "        print(f\"  -> OpenAssistant failed: {e}\")\n",
    "    \n",
    "    # Dolly\n",
    "    print(\"Loading Dolly...\")\n",
    "    try:\n",
    "        dolly = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "        for item in dolly:\n",
    "            instruction = item.get(\"instruction\", \"\").strip()\n",
    "            context = item.get(\"context\", \"\").strip()\n",
    "            question = f\"{instruction}\\n\\nContext: {context}\" if context else instruction\n",
    "            if 20 < len(question) < 500:\n",
    "                all_questions.append({\"question\": question, \"source\": \"dolly\", \"answer\": None})\n",
    "        print(f\"  -> {len([q for q in all_questions if q['source']=='dolly'])} from Dolly\")\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Dolly failed: {e}\")\n",
    "    \n",
    "    # Alpaca\n",
    "    print(\"Loading Alpaca...\")\n",
    "    try:\n",
    "        alpaca = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "        for item in alpaca:\n",
    "            instruction = item.get(\"instruction\", \"\").strip()\n",
    "            inp = item.get(\"input\", \"\").strip()\n",
    "            question = f\"{instruction}\\n\\nInput: {inp}\" if inp else instruction\n",
    "            if 20 < len(question) < 500:\n",
    "                all_questions.append({\"question\": question, \"source\": \"alpaca\", \"answer\": None})\n",
    "        print(f\"  -> {len([q for q in all_questions if q['source']=='alpaca'])} from Alpaca\")\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Alpaca failed: {e}\")\n",
    "    \n",
    "    random.shuffle(all_questions)\n",
    "    all_questions = all_questions[:max_samples]\n",
    "    print(f\"\\nTotal: {len(all_questions)} real user questions\")\n",
    "    \n",
    "    dataset = (\n",
    "        grain.MapDataset.source(all_questions)\n",
    "        .shuffle(seed=42)\n",
    "        .map(\n",
    "            lambda x: {\n",
    "                \"prompts\": TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=x[\"question\"]),\n",
    "                \"question\": x[\"question\"],\n",
    "                \"answer\": x[\"answer\"],\n",
    "                \"source\": x[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "full_dataset = get_real_user_dataset(max_samples=NUM_BATCHES * TRAIN_MICRO_BATCH_SIZE * 2)\n",
    "dataset = full_dataset.batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_BATCHES]\n",
    "train_dataset = dataset.repeat(NUM_EPOCHS)\n",
    "test_start = int(len(dataset) * 0.9)\n",
    "test_dataset = full_dataset.batch(TRAIN_MICRO_BATCH_SIZE)[test_start:test_start + NUM_TEST_BATCHES]\n",
    "print(f\"Train batches: {len(train_dataset)}\")\n",
    "print(f\"Test batches: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: MODEL LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def show_hbm_usage():\n",
    "    fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
    "    for d in jax.local_devices():\n",
    "        stats = d.memory_stats()\n",
    "        used = stats[\"bytes_in_use\"]\n",
    "        limit = stats[\"bytes_limit\"]\n",
    "        print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "def auto_login():\n",
    "    try:\n",
    "        user_secrets = UserSecretsClient()\n",
    "        username = user_secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "        key = user_secrets.get_secret(\"KAGGLE_KEY\")\n",
    "        if username and key:\n",
    "            os.environ[\"KAGGLE_USERNAME\"] = username\n",
    "            os.environ[\"KAGGLE_KEY\"] = key\n",
    "            print(\"✅ Authenticated\")\n",
    "            return\n",
    "    except: pass\n",
    "    kagglehub.login()\n",
    "\n",
    "auto_login()\n",
    "\n",
    "!rm /tmp/content/intermediate_ckpt/* -rf\n",
    "!rm /tmp/content/ckpts/* -rf\n",
    "\n",
    "print(\"Loading Gemma3 1B-IT...\")\n",
    "MODEL_CP_PATH = params.GEMMA3_1B_IT\n",
    "config = model.ModelConfig.gemma3_1b()\n",
    "gemma = params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n",
    "tokenizer = params.create_tokenizer()\n",
    "\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "_, state = nnx.split(gemma)\n",
    "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
    "checkpointer.wait_until_finished()\n",
    "\n",
    "del gemma, state\n",
    "gc.collect()\n",
    "print(\"Model checkpoint saved.\")\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: LORA MODEL SETUP\n",
    "# =============================================================================\n",
    "\n",
    "def get_gemma_ref_model(ckpt_path):\n",
    "    mesh = jax.make_mesh(*MESH)\n",
    "    model_config = model.ModelConfig.gemma3_1b()\n",
    "    abs_gemma = nnx.eval_shape(\n",
    "        lambda: params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n",
    "    )\n",
    "    abs_state = nnx.state(abs_gemma)\n",
    "    abs_state = jax.tree.map(\n",
    "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "        abs_state,\n",
    "        nnx.get_named_sharding(abs_state, mesh),\n",
    "    )\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
    "    graph_def, _ = nnx.split(abs_gemma)\n",
    "    gemma = nnx.merge(graph_def, restored_params)\n",
    "    return gemma, mesh, model_config\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|.*attn_vec_einsum\",\n",
    "        rank=RANK,\n",
    "        alpha=ALPHA,\n",
    "    )\n",
    "    model_input = base_model.get_model_input()\n",
    "    lora_model = qwix.apply_lora_to_model(base_model, lora_provider, **model_input)\n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "    return lora_model\n",
    "\n",
    "print(\"Loading reference model...\")\n",
    "ref_model, mesh, model_config = get_gemma_ref_model(\n",
    "    ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
    ")\n",
    "\n",
    "print(\"Applying LoRA...\")\n",
    "lora_policy = get_lora_model(ref_model, mesh=mesh)\n",
    "print(f\"LoRA model ready: RANK={RANK}, ALPHA={ALPHA}\")\n",
    "show_hbm_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: PROMETHEUS v2.0 REWARD FUNCTIONS (CIC-Guided)\n",
    "# =============================================================================\n",
    "# Synthesized from PROMETHEUS SYNTHESIS ENCYCLOPEDIA\n",
    "# Implements 8 novel reward signals\n",
    "# =============================================================================\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags=re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# REWARD 1: Format Compliance (from v1)\n",
    "# =============================================================================\n",
    "def match_format_exactly(prompts, completions, **kwargs):\n",
    "    return [0 if match_format.search(r) is None else 3.0 for r in completions]\n",
    "\n",
    "\n",
    "def match_format_approximately(prompts, completions, **kwargs):\n",
    "    scores = []\n",
    "    for c in completions:\n",
    "        score = 0\n",
    "        score += 0.5 if c.count(reasoning_start) == 1 else -0.5\n",
    "        score += 0.5 if c.count(reasoning_end) == 1 else -0.5\n",
    "        score += 0.5 if c.count(solution_start) == 1 else -0.5\n",
    "        score += 0.5 if c.count(solution_end) == 1 else -0.5\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# REWARD 2: NCD Basin Regularization (INSIGHT C2)\n",
    "# Reward traces close to canonical patterns\n",
    "# =============================================================================\n",
    "def basin_regularization_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward traces close to canonical reasoning patterns.\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        match = re.search(rf\"{reasoning_start}(.+?){reasoning_end}\", completion, re.DOTALL)\n",
    "        if not match:\n",
    "            scores.append(-1.0)\n",
    "            continue\n",
    "        \n",
    "        trace = match.group(1)\n",
    "        distances = [ncd(trace, canonical) for canonical in CANONICAL_TRACES]\n",
    "        min_distance = min(distances)\n",
    "        \n",
    "        # Score: inverse of distance (closer = higher)\n",
    "        score = 2.0 * (1 - min_distance)\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# REWARD 3: Causal Emergence (INSIGHT B7/C1)\n",
    "# Macro-level statements explaining micro-level details\n",
    "# =============================================================================\n",
    "MACRO_PATTERNS = [\n",
    "    r'by (symmetry|induction|contradiction|construction)',\n",
    "    r'(therefore|thus|hence|so)\\s+(all|every|the)',\n",
    "    r'in general',\n",
    "    r'for (all|any|every)',\n",
    "    r'this (implies|means|shows)',\n",
    "    r'the key (insight|observation)',\n",
    "]\n",
    "\n",
    "MICRO_PATTERNS = [\n",
    "    r'\\d+\\s*[\\+\\-\\*/]\\s*\\d+\\s*=\\s*\\d+',\n",
    "    r'step\\s+\\d+',\n",
    "    r'case\\s+\\d+',\n",
    "]\n",
    "\n",
    "def causal_emergence_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward traces with good macro/micro balance.\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        match = re.search(rf\"{reasoning_start}(.+?){reasoning_end}\", completion, re.DOTALL)\n",
    "        if not match:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        trace = match.group(1).lower()\n",
    "        \n",
    "        macro_count = sum(len(re.findall(p, trace)) for p in MACRO_PATTERNS)\n",
    "        micro_count = sum(len(re.findall(p, trace)) for p in MICRO_PATTERNS)\n",
    "        \n",
    "        if micro_count == 0:\n",
    "            scores.append(0.5)\n",
    "            continue\n",
    "        \n",
    "        # Ideal: ~1 macro per 3 micros\n",
    "        ratio = macro_count / micro_count if micro_count > 0 else 0\n",
    "        ideal = 1/3\n",
    "        psi = 1 - min(1, abs(ratio - ideal) / ideal)\n",
    "        scores.append(psi * 1.5)  # Scale to 1.5 max\n",
    "    return scores\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# REWARD 4: ToM-Informed Properties (INSIGHT C1)\n",
    "# Outputs that help humans understand and verify\n",
    "# =============================================================================\n",
    "def tom_informed_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward ToM-friendly outputs.\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    uncertainty_markers = [\n",
    "        'i think', 'likely', 'probably', 'might',\n",
    "        'assuming', 'if we', 'could be',\n",
    "    ]\n",
    "    checkable_patterns = [\n",
    "        r'therefore.*=',\n",
    "        r'which gives',\n",
    "        r'substituting',\n",
    "        r'to verify',\n",
    "    ]\n",
    "    \n",
    "    for completion in completions:\n",
    "        lower = completion.lower()\n",
    "        score = 0\n",
    "        \n",
    "        # Uncertainty markers (calibration)\n",
    "        unc_count = sum(1 for m in uncertainty_markers if m in lower)\n",
    "        score += min(0.5, unc_count * 0.15)\n",
    "        \n",
    "        # Checkable steps\n",
    "        check_count = sum(len(re.findall(p, lower)) for p in checkable_patterns)\n",
    "        score += min(0.5, check_count * 0.2)\n",
    "        \n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# REWARD 5: Coherence Graph (INSIGHT C5)\n",
    "# Logical connectors indicating flow\n",
    "# =============================================================================\n",
    "def coherence_graph_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward traces with coherent logical structure.\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    forward = ['therefore', 'thus', 'so', 'hence', 'this means', 'which implies']\n",
    "    backward = ['because', 'since', 'as', 'given that']\n",
    "    \n",
    "    for completion in completions:\n",
    "        match = re.search(rf\"{reasoning_start}(.+?){reasoning_end}\", completion, re.DOTALL)\n",
    "        if not match:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        trace = match.group(1).lower()\n",
    "        sentences = [s.strip() for s in re.split(r'[.!?]+', trace) if s.strip()]\n",
    "        n_sentences = len(sentences)\n",
    "        \n",
    "        if n_sentences < 2:\n",
    "            scores.append(0.5)\n",
    "            continue\n",
    "        \n",
    "        n_forward = sum(1 for c in forward if c in trace)\n",
    "        n_backward = sum(1 for c in backward if c in trace)\n",
    "        \n",
    "        # Ideal: ~1 connector per 2-3 sentences\n",
    "        expected = n_sentences / 2.5\n",
    "        actual = n_forward + n_backward\n",
    "        ratio = min(1.5, actual / expected) if expected > 0 else 0\n",
    "        \n",
    "        # Balance bonus\n",
    "        balance = 0.3 if n_forward > 0 and n_backward > 0 else 0\n",
    "        \n",
    "        scores.append(min(2.0, ratio + balance))\n",
    "    return scores\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# REWARD 6: Kolmogorov Compression (INSIGHT C6)\n",
    "# Reward concise but complete reasoning\n",
    "# =============================================================================\n",
    "def kolmogorov_compression_reward(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward efficient reasoning traces.\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        match = re.search(rf\"{reasoning_start}(.+?){reasoning_end}\", completion, re.DOTALL)\n",
    "        if not match:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        trace = match.group(1)\n",
    "        k_ratio = kolmogorov_proxy(trace)\n",
    "        \n",
    "        # Lower ratio = more compressible = more structured\n",
    "        # Ideal range: 0.3-0.6 (not too repetitive, not too random)\n",
    "        if 0.3 <= k_ratio <= 0.6:\n",
    "            scores.append(1.5)\n",
    "        elif 0.2 <= k_ratio < 0.3 or 0.6 < k_ratio <= 0.7:\n",
    "            scores.append(1.0)\n",
    "        else:\n",
    "            scores.append(0.5)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# REWARD 7: NCD Diversity Bonus (INSIGHT C4)\n",
    "# Reward diverse traces within batch (prevent mode collapse)\n",
    "# =============================================================================\n",
    "def trace_diversity_bonus(prompts, completions, **kwargs):\n",
    "    \"\"\"Reward diverse reasoning traces.\"\"\"\n",
    "    if len(completions) < 2:\n",
    "        return [0] * len(completions)\n",
    "    \n",
    "    traces = []\n",
    "    for c in completions:\n",
    "        match = re.search(rf\"{reasoning_start}(.+?){reasoning_end}\", c, re.DOTALL)\n",
    "        traces.append(match.group(1) if match else \"\")\n",
    "    \n",
    "    scores = []\n",
    "    for i, trace in enumerate(traces):\n",
    "        if not trace:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        other_dists = [ncd(trace, t) for j, t in enumerate(traces) if j != i and t]\n",
    "        if other_dists:\n",
    "            avg_dist = np.mean(other_dists)\n",
    "            # Higher distance = more diverse = bonus\n",
    "            scores.append(max(0, (avg_dist - 0.5) * 2))\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# REWARD 8: Answer Completeness (from v1, enhanced)\n",
    "# =============================================================================\n",
    "def answer_completeness(prompts, completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        match = re.search(rf\"{solution_start}(.+?){solution_end}\", completion, re.DOTALL)\n",
    "        if not match:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        answer = match.group(1).strip()\n",
    "        word_count = len(answer.split())\n",
    "        \n",
    "        if 3 <= word_count <= 100:\n",
    "            scores.append(2.0)\n",
    "        elif 1 <= word_count < 3 or 100 < word_count <= 200:\n",
    "            scores.append(1.0)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "    return scores\n",
    "\n",
    "\n",
    "print(\"PROMETHEUS v2.0 Reward Functions loaded:\")\n",
    "print(\"  1. Format exact (3.0)\")\n",
    "print(\"  2. Format approximate (+/-2.0)\")\n",
    "print(\"  3. Basin regularization (2.0) [NCD]\")\n",
    "print(\"  4. Causal emergence (1.5) [Ψ]\")\n",
    "print(\"  5. ToM-informed (1.0)\")\n",
    "print(\"  6. Coherence graph (2.0)\")\n",
    "print(\"  7. Kolmogorov compression (1.5)\")\n",
    "print(\"  8. Trace diversity (1.0) [NCD]\")\n",
    "print(\"  9. Answer completeness (2.0)\")\n",
    "print(f\"\\nMax possible reward: ~16.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: ADAPTIVE TEMPERATURE SCHEDULER (INSIGHT C3)\n",
    "# =============================================================================\n",
    "\n",
    "class AdaptiveTemperatureScheduler:\n",
    "    \"\"\"Temperature coupled to training dynamics.\"\"\"\n",
    "    \n",
    "    def __init__(self, temp_start=1.2, temp_end=0.5, ema_alpha=0.1):\n",
    "        self.temp_start = temp_start\n",
    "        self.temp_end = temp_end\n",
    "        self.ema_alpha = ema_alpha\n",
    "        self.reward_variance_ema = None\n",
    "        self.baseline_variance = None\n",
    "        self.step = 0\n",
    "    \n",
    "    def update(self, batch_rewards):\n",
    "        self.step += 1\n",
    "        batch_var = np.var(batch_rewards) if len(batch_rewards) > 1 else 0\n",
    "        \n",
    "        if self.reward_variance_ema is None:\n",
    "            self.reward_variance_ema = batch_var\n",
    "            self.baseline_variance = batch_var\n",
    "        else:\n",
    "            self.reward_variance_ema = (\n",
    "                self.ema_alpha * batch_var +\n",
    "                (1 - self.ema_alpha) * self.reward_variance_ema\n",
    "            )\n",
    "    \n",
    "    def get_temperature(self, max_steps):\n",
    "        progress = self.step / max_steps\n",
    "        \n",
    "        # Base: cosine decay\n",
    "        base = self.temp_end + 0.5 * (self.temp_start - self.temp_end) * (\n",
    "            1 + np.cos(np.pi * progress)\n",
    "        )\n",
    "        \n",
    "        # Variance adjustment\n",
    "        if self.baseline_variance and self.baseline_variance > 0:\n",
    "            ratio = self.reward_variance_ema / self.baseline_variance\n",
    "            adjustment = 0.2 * (ratio - 1)\n",
    "        else:\n",
    "            adjustment = 0\n",
    "        \n",
    "        return np.clip(base + adjustment, TEMP_MIN, TEMP_MAX)\n",
    "\n",
    "\n",
    "temp_scheduler = AdaptiveTemperatureScheduler()\n",
    "print(\"Adaptive Temperature Scheduler loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: EVALUATION HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "def generate(question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None):\n",
    "    if isinstance(question, str):\n",
    "        input_batch = [TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question)]\n",
    "    else:\n",
    "        input_batch = [TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q) for q in question]\n",
    "    \n",
    "    out = sampler(\n",
    "        input_strings=input_batch,\n",
    "        max_generation_steps=768,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        echo=False,\n",
    "        seed=seed,\n",
    "        eos_tokens=[1, 106],\n",
    "    )\n",
    "    return out.text[0] if isinstance(question, str) else out.text\n",
    "\n",
    "\n",
    "def evaluate_open_ended(dataset, sampler, **config):\n",
    "    total = format_exact = has_reasoning = has_answer = 0\n",
    "    coherence_scores = []\n",
    "    \n",
    "    for batch in tqdm(dataset):\n",
    "        responses = generate(batch[\"question\"], sampler, **config, seed=0)\n",
    "        for r in responses:\n",
    "            total += 1\n",
    "            if match_format.search(r): format_exact += 1\n",
    "            r_match = re.search(rf\"{reasoning_start}(.+?){reasoning_end}\", r, re.DOTALL)\n",
    "            if r_match:\n",
    "                has_reasoning += 1\n",
    "                coherence = sum(1 for c in ['because', 'therefore', 'so'] if c in r_match.group(1).lower())\n",
    "                coherence_scores.append(coherence)\n",
    "            if re.search(rf\"{solution_start}(.+?){solution_end}\", r, re.DOTALL):\n",
    "                has_answer += 1\n",
    "    \n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"format_exact_pct\": format_exact / total * 100 if total else 0,\n",
    "        \"has_reasoning_pct\": has_reasoning / total * 100 if total else 0,\n",
    "        \"has_answer_pct\": has_answer / total * 100 if total else 0,\n",
    "        \"avg_coherence\": np.mean(coherence_scores) if coherence_scores else 0,\n",
    "    }\n",
    "\n",
    "print(\"Evaluation helpers loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: PRE-TRAINING EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Creating sampler...\")\n",
    "sampler = sampler_lib.Sampler(\n",
    "    transformer=lora_policy,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating BASE model...\")\n",
    "base_results = evaluate_open_ended(test_dataset, sampler, **GENERATION_CONFIGS[\"greedy\"])\n",
    "print(f\"Format: {base_results['format_exact_pct']:.1f}%\")\n",
    "print(f\"Reasoning: {base_results['has_reasoning_pct']:.1f}%\")\n",
    "print(f\"Coherence: {base_results['avg_coherence']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: GRPO TRAINING SETUP\n",
    "# =============================================================================\n",
    "\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n",
    ")\n",
    "\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        decay_steps=MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=B1, b2=B2, weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "if MAX_GRAD_NORM:\n",
    "    optimizer = optax.chain(optax.clip_by_global_norm(MAX_GRAD_NORM), optimizer)\n",
    "\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh={\n",
    "        rl_cluster_lib.Role.ACTOR: mesh,\n",
    "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
    "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
    "    },\n",
    "    rollout_engine='vanilla',\n",
    "    offload_to_cpu=False,\n",
    "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
    "        actor_optimizer=optimizer,\n",
    "        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
    "        max_steps=MAX_STEPS,\n",
    "        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
    "        metrics_logging_options=None,\n",
    "        checkpoint_root_directory=CKPT_DIR,\n",
    "        checkpointing_options=checkpointing_options,\n",
    "    ),\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
    "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
    "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "        temperature=TEMP_START,\n",
    "        top_p=TOP_P,\n",
    "        top_k=TOP_K,\n",
    "        eos_tokens=[1, 106],\n",
    "    ),\n",
    ")\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=NUM_GENERATIONS,\n",
    "    num_iterations=NUM_ITERATIONS,\n",
    "    beta=BETA,\n",
    "    epsilon=EPSILON,\n",
    ")\n",
    "\n",
    "print(\"GRPO config ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: INITIALIZE GRPO TRAINER WITH v2.0 REWARDS\n",
    "# =============================================================================\n",
    "\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    actor=lora_policy,\n",
    "    reference=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    cluster_config=cluster_config,\n",
    ")\n",
    "\n",
    "# PROMETHEUS v2.0 Reward Stack\n",
    "grpo_trainer = GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=[\n",
    "        match_format_exactly,          # 3.0\n",
    "        match_format_approximately,    # +/-2.0\n",
    "        basin_regularization_reward,   # 2.0 [NCD]\n",
    "        causal_emergence_reward,       # 1.5 [Ψ]\n",
    "        tom_informed_reward,           # 1.0\n",
    "        coherence_graph_reward,        # 2.0\n",
    "        kolmogorov_compression_reward, # 1.5\n",
    "        trace_diversity_bonus,         # 1.0 [NCD]\n",
    "        answer_completeness,           # 2.0\n",
    "    ],\n",
    "    grpo_config=grpo_config,\n",
    ")\n",
    "\n",
    "print(\"GRPO trainer initialized with PROMETHEUS v2.0 rewards.\")\n",
    "print(f\"9 reward functions, max ~16.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: TRAIN!\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"Starting GRPO training for {MAX_STEPS} steps...\")\n",
    "print(f\"This may take several hours on TPU v5e-8.\")\n",
    "print()\n",
    "\n",
    "with mesh:\n",
    "    grpo_trainer.train(train_dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROMETHEUS v2.0 TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 16: LOAD BEST CHECKPOINT & EVALUATE\n",
    "# =============================================================================\n",
    "\n",
    "actor_ckpt_dir = os.path.join(CKPT_DIR, \"actor\")\n",
    "latest_step = -1\n",
    "if os.path.exists(actor_ckpt_dir):\n",
    "    for item in os.listdir(actor_ckpt_dir):\n",
    "        if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r'^\\d+$', item):\n",
    "            step = int(item)\n",
    "            if step > latest_step:\n",
    "                latest_step = step\n",
    "\n",
    "if latest_step == -1:\n",
    "    raise FileNotFoundError(f\"No checkpoints found\")\n",
    "\n",
    "print(f\"Loading checkpoint from step {latest_step}...\")\n",
    "\n",
    "trained_ckpt_path = os.path.join(CKPT_DIR, \"actor\", str(latest_step), \"model_params\")\n",
    "abs_params = jax.tree.map(\n",
    "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    nnx.state(lora_policy, nnx.LoRAParam),\n",
    ")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n",
    "\n",
    "nnx.update(\n",
    "    lora_policy,\n",
    "    jax.tree.map(lambda a, b: b, nnx.state(lora_policy, nnx.LoRAParam), trained_lora_params),\n",
    ")\n",
    "\n",
    "sampler = sampler_lib.Sampler(\n",
    "    transformer=lora_policy, tokenizer=tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating TRAINED model...\")\n",
    "trained_results = evaluate_open_ended(test_dataset, sampler, **GENERATION_CONFIGS[\"greedy\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"IMPROVEMENT SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Format: {base_results['format_exact_pct']:.1f}% -> {trained_results['format_exact_pct']:.1f}%\")\n",
    "print(f\"Reasoning: {base_results['has_reasoning_pct']:.1f}% -> {trained_results['has_reasoning_pct']:.1f}%\")\n",
    "print(f\"Coherence: {base_results['avg_coherence']:.2f} -> {trained_results['avg_coherence']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 17: SAVE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "SAVE_PATH = \"/kaggle/working/prometheus_v2_gemma_reasoning\"\n",
    "\n",
    "print(f\"Saving trained LoRA weights to {SAVE_PATH}...\")\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "checkpointer.save(\n",
    "    os.path.join(SAVE_PATH, \"lora_params\"),\n",
    "    nnx.state(lora_policy, nnx.LoRAParam)\n",
    ")\n",
    "checkpointer.wait_until_finished()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROMETHEUS v2.0 + TUNIX TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model saved to: {SAVE_PATH}\")\n",
    "print(f\"\\nKey innovations applied:\")\n",
    "print(f\"  - NCD Basin Regularization\")\n",
    "print(f\"  - Causal Emergence (Ψ) Rewards\")\n",
    "print(f\"  - ToM-Informed Output Shaping\")\n",
    "print(f\"  - Coherence Graph Scoring\")\n",
    "print(f\"  - Kolmogorov Compression\")\n",
    "print(f\"  - NCD Trace Diversity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
