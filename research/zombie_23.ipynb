{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87bb6b82",
   "metadata": {
    "_cell_guid": "8c62453f-96e3-4690-b446-66cc36944430",
    "_uuid": "2723e7ea-8f54-4ded-bc4d-654016305214",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003698,
     "end_time": "2025-12-13T02:31:45.597690",
     "exception": false,
     "start_time": "2025-12-13T02:31:45.593992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "References\n",
    "- https://www.kaggle.com/code/huikang/arc-agi-2-code-approach\n",
    "- https://www.kaggle.com/code/huikang/r1-distill-qwen-tir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc148cdf",
   "metadata": {
    "_cell_guid": "fdf223c2-f8ef-467a-9dbb-cc5484786047",
    "_kg_hide-output": true,
    "_uuid": "3da124f9-ff58-4c99-8f7a-fe224f1fa647",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:31:45.605155Z",
     "iopub.status.busy": "2025-12-13T02:31:45.604708Z",
     "iopub.status.idle": "2025-12-13T02:32:53.055496Z",
     "shell.execute_reply": "2025-12-13T02:32:53.055076Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 67.459009,
     "end_time": "2025-12-13T02:32:53.059678",
     "exception": false,
     "start_time": "2025-12-13T02:31:45.600669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.18.0\n",
      "Uninstalling tensorflow-2.18.0:\n",
      "  Successfully uninstalled tensorflow-2.18.0\n",
      "Found existing installation: matplotlib 3.7.2\n",
      "Uninstalling matplotlib-3.7.2:\n",
      "  Successfully uninstalled matplotlib-3.7.2\n",
      "Found existing installation: keras 3.8.0\n",
      "Uninstalling keras-3.8.0:\n",
      "  Successfully uninstalled keras-3.8.0\n",
      "Found existing installation: scikit-learn 1.2.2\n",
      "Uninstalling scikit-learn-1.2.2:\n",
      "  Successfully uninstalled scikit-learn-1.2.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip', 'uninstall', '--yes', 'tensorflow', 'matplotlib', 'keras', 'scikit-learn'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [\"pip\", \"uninstall\", \"--yes\", \"tensorflow\", \"matplotlib\", \"keras\", \"scikit-learn\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683763f3",
   "metadata": {
    "_cell_guid": "4fcbcdb6-696b-4b48-826f-c9bd42031243",
    "_uuid": "92a580b1-f0fe-46c6-a97a-9059012c30a5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:32:53.067882Z",
     "iopub.status.busy": "2025-12-13T02:32:53.067711Z",
     "iopub.status.idle": "2025-12-13T02:33:12.040730Z",
     "shell.execute_reply": "2025-12-13T02:33:12.040274Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 18.978452,
     "end_time": "2025-12-13T02:33:12.041784",
     "exception": false,
     "start_time": "2025-12-13T02:32:53.063332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def is_on_kaggle_commit() -> bool:\n",
    "    return os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Batch\" and not bool(\n",
    "        os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n",
    "    )\n",
    "\n",
    "\n",
    "def is_on_kaggle_interactive() -> bool:\n",
    "    return os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not bool(\n",
    "        os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n",
    "    )\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "final_cutoff_time = start_time + (4 * 60 + 47) * 60  # 4.75 hours from start time\n",
    "cutoff_times = [\n",
    "    int(x) for x in np.linspace(final_cutoff_time, start_time + 12 * 60, 50 + 1)\n",
    "]  # 5 minutes loading time at the start\n",
    "cutoff_times.pop()\n",
    "\n",
    "os.makedirs(\"solutions\", exist_ok=True)\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "assert torch.cuda.device_count() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65f708",
   "metadata": {
    "_cell_guid": "5c120214-e16b-45ea-a2d1-b77777bedb72",
    "_uuid": "1d6623d2-ce24-4d27-973c-142828fe4e1a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003401,
     "end_time": "2025-12-13T02:33:12.048875",
     "exception": false,
     "start_time": "2025-12-13T02:33:12.045474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Serve vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a763b5e",
   "metadata": {
    "_cell_guid": "8783f9ec-c1d9-4110-9aaa-39d6a92b4a54",
    "_uuid": "aec8a8d8-b6af-4748-ab79-d7da2298e33d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:12.056445Z",
     "iopub.status.busy": "2025-12-13T02:33:12.056229Z",
     "iopub.status.idle": "2025-12-13T02:33:12.076149Z",
     "shell.execute_reply": "2025-12-13T02:33:12.075763Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.024562,
     "end_time": "2025-12-13T02:33:12.076832",
     "exception": false,
     "start_time": "2025-12-13T02:33:12.052270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl100k_base.tiktoken\n",
      "o200k_base.tiktoken\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ls', '/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"ls\", \"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8667bf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:12.085033Z",
     "iopub.status.busy": "2025-12-13T02:33:12.084896Z",
     "iopub.status.idle": "2025-12-13T02:33:12.087403Z",
     "shell.execute_reply": "2025-12-13T02:33:12.087006Z"
    },
    "papermill": {
     "duration": 0.007585,
     "end_time": "2025-12-13T02:33:12.088004",
     "exception": false,
     "start_time": "2025-12-13T02:33:12.080419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"a-vllm.log\", \"w\") as f:\n",
    "    f.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e9cd20",
   "metadata": {
    "_cell_guid": "7f17cd32-442e-460a-982e-7ca73668b69f",
    "_uuid": "53e3ad10-1b6f-47d8-b5de-19f556c4b7a8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:12.095617Z",
     "iopub.status.busy": "2025-12-13T02:33:12.095472Z",
     "iopub.status.idle": "2025-12-13T02:33:12.100284Z",
     "shell.execute_reply": "2025-12-13T02:33:12.099862Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009341,
     "end_time": "2025-12-13T02:33:12.100914",
     "exception": false,
     "start_time": "2025-12-13T02:33:12.091573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs: /kaggle/working/a-vllm.log\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def start_vllm_server() -> subprocess.Popen[bytes]:\n",
    "    \"\"\"Start vLLM server in the background\"\"\"\n",
    "    os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "    os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "    os.environ[\"VLLM_ATTENTION_BACKEND\"] = \"TRITON_ATTN\"\n",
    "    os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    # https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#troubleshooting\n",
    "    os.environ[\"TIKTOKEN_ENCODINGS_BASE\"] = (\n",
    "        \"/kaggle/usr/lib/pip_install_aimo3_1/tiktoken_encodings\"\n",
    "    )\n",
    "\n",
    "    sequence_length = 65_536\n",
    "\n",
    "    command: list[str] = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\",\n",
    "        \"/kaggle/input/gpt-oss-120b/transformers/default/1\",\n",
    "        \"--served-model-name\",\n",
    "        \"vllm-model\",\n",
    "        \"--tensor-parallel-size\",\n",
    "        \"1\",\n",
    "        \"--max-num-seqs\",\n",
    "        \"4\",\n",
    "        \"--gpu-memory-utilization\",\n",
    "        \"0.96\",  # any higher may not have enough for graph capture\n",
    "        \"--host\",\n",
    "        \"0.0.0.0\",\n",
    "        \"--port\",\n",
    "        \"8000\",\n",
    "        \"--dtype\",\n",
    "        \"auto\",\n",
    "        \"--max-model-len\",\n",
    "        f\"{sequence_length}\",\n",
    "    ]\n",
    "\n",
    "    # Start the process in the background\n",
    "    with open(\"/kaggle/working/a-vllm.log\", \"w\") as logfile:\n",
    "        process: subprocess.Popen[bytes] = subprocess.Popen(\n",
    "            command, stdout=logfile, stderr=subprocess.STDOUT, start_new_session=True\n",
    "        )\n",
    "\n",
    "    print(\"Logs: /kaggle/working/a-vllm.log\")\n",
    "    return process\n",
    "\n",
    "# Start the server\n",
    "vllm_process: subprocess.Popen[bytes] = start_vllm_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9191e44d",
   "metadata": {
    "_cell_guid": "bf725ef0-626c-4b0b-93d1-e74549a152a7",
    "_uuid": "1759af22-a458-484e-84b1-64010680e36b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:12.109179Z",
     "iopub.status.busy": "2025-12-13T02:33:12.109027Z",
     "iopub.status.idle": "2025-12-13T02:33:21.235289Z",
     "shell.execute_reply": "2025-12-13T02:33:21.234804Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 9.131807,
     "end_time": "2025-12-13T02:33:21.236478",
     "exception": false,
     "start_time": "2025-12-13T02:33:12.104671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI, Stream\n",
    "from openai.types.chat import ChatCompletion, ChatCompletionMessageParam\n",
    "from openai.types.chat.chat_completion_chunk import ChatCompletionChunk\n",
    "\n",
    "# Point the client to your local vLLM server\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://127.0.0.1:8000/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-local\"  # any non-empty string\n",
    "\n",
    "client: OpenAI = OpenAI(\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "\n",
    "# https://github.com/vllm-project/vllm/issues/27243\n",
    "# Unexpected token 2000?? while expecting start token 200006\n",
    "stop_token_ids: list[int] = [\n",
    "    token_id\n",
    "    for token_id in range(200_000, 201_088)\n",
    "    if token_id not in [200005, 200006, 200007, 200008]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0424e270",
   "metadata": {
    "_cell_guid": "7506d16a-f083-47f7-a68f-c8cf2602c432",
    "_uuid": "7ffb104f-e307-4f91-926c-15177cd12f0a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.245021Z",
     "iopub.status.busy": "2025-12-13T02:33:21.244865Z",
     "iopub.status.idle": "2025-12-13T02:33:21.248000Z",
     "shell.execute_reply": "2025-12-13T02:33:21.247610Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007899,
     "end_time": "2025-12-13T02:33:21.248580",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.240681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def await_client(printing: bool = False):\n",
    "    for _ in range(15 * 60):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            model_list = client.models.list()\n",
    "            if printing:\n",
    "                print(model_list)\n",
    "        except NameError:\n",
    "            raise  # maybe you did not run the cell initializing client\n",
    "        except Exception:\n",
    "            continue\n",
    "        break\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "if is_on_kaggle_interactive():\n",
    "    await_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27eede65",
   "metadata": {
    "_cell_guid": "7726741c-26cf-4297-bb09-950ae9e812bd",
    "_uuid": "4bae670b-305a-4956-ace4-c439a613da21",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.257074Z",
     "iopub.status.busy": "2025-12-13T02:33:21.256666Z",
     "iopub.status.idle": "2025-12-13T02:33:21.301438Z",
     "shell.execute_reply": "2025-12-13T02:33:21.301039Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.04987,
     "end_time": "2025-12-13T02:33:21.302167",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.252297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cachetools import cached, TTLCache\n",
    "from typing import Generator\n",
    "import time\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def reversed_lines(path: str, block_size: int = 4096) -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Iterate over the lines of a file in reverse order (last line first),\n",
    "    without loading the entire file into memory.\n",
    "\n",
    "    Yields lines as strings (including the trailing newline if present).\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        f.seek(0, os.SEEK_END)\n",
    "        file_end = f.tell()\n",
    "\n",
    "        buffer = b\"\"\n",
    "        pos = file_end\n",
    "\n",
    "        while pos > 0:\n",
    "            # Read a block from the end going backwards\n",
    "            read_size = min(block_size, pos)\n",
    "            pos -= read_size\n",
    "            f.seek(pos, os.SEEK_SET)\n",
    "            data = f.read(read_size)\n",
    "\n",
    "            buffer = data + buffer\n",
    "            # Split into lines\n",
    "            lines = buffer.split(b\"\\n\")\n",
    "            # Keep the first (possibly incomplete) part in buffer\n",
    "            buffer = lines[0]\n",
    "            # The rest (from the end backwards) are full lines\n",
    "            for line in reversed(lines[1:]):\n",
    "                yield line.decode(\"utf-8\", errors=\"replace\") + \"\\n\"\n",
    "\n",
    "        # Finally, yield the very first line (if any)\n",
    "        if buffer:\n",
    "            yield buffer.decode(\"utf-8\", errors=\"replace\") + \"\\n\"\n",
    "\n",
    "\n",
    "@cached(cache=TTLCache(maxsize=50, ttl=10))\n",
    "def get_gpu_kv_cache_usage(question_id: str | None = None) -> float:\n",
    "    for line in reversed_lines(\"a-vllm.log\"):\n",
    "        pattern = r\"GPU KV cache usage: ([\\d.]+)%\"\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            gpu_cache_usage = float(match.group(1))\n",
    "            return gpu_cache_usage\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d8a6526",
   "metadata": {
    "_cell_guid": "77510a63-d235-4a5c-b182-18a4ade735e3",
    "_uuid": "d0bcd210-101b-4f9a-9cd3-d9c656dd76c8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.310162Z",
     "iopub.status.busy": "2025-12-13T02:33:21.310040Z",
     "iopub.status.idle": "2025-12-13T02:33:21.312777Z",
     "shell.execute_reply": "2025-12-13T02:33:21.312426Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00726,
     "end_time": "2025-12-13T02:33:21.313341",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.306081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_on_kaggle_interactive():\n",
    "    resp: ChatCompletion = client.chat.completions.create(\n",
    "        model=\"vllm-model\",  # use your served name; if not set, the model path/name vLLM shows in logs\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Reply with your answer in \\\\boxed{}\"},\n",
    "            {\"role\": \"user\", \"content\": \"How many letter r are there in strawberry?\"},\n",
    "        ],\n",
    "        mmax_tokens=annealed_max_tokens(iteration, max_iterations),\n",
    "        temperature=entropy_floored_temperature(iteration, max_iterations),\n",
    "        extra_body=dict(min_p=0.02, stop_token_ids=stop_token_ids, chat_template_kwargs=dict(enable_thinking=True)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce06713a",
   "metadata": {
    "_cell_guid": "3080497b-33ed-4c1d-a5fb-45ae4b8ad5d5",
    "_uuid": "07968db3-c86c-4c1d-93ca-40a8a020bd68",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.320654Z",
     "iopub.status.busy": "2025-12-13T02:33:21.320529Z",
     "iopub.status.idle": "2025-12-13T02:33:21.322625Z",
     "shell.execute_reply": "2025-12-13T02:33:21.322265Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006507,
     "end_time": "2025-12-13T02:33:21.323205",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.316698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_on_kaggle_interactive():\n",
    "    print(resp.choices[0].message.reasoning_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1cfe89d",
   "metadata": {
    "_cell_guid": "2cb79916-5b27-4cb9-af64-d056ccb6bca4",
    "_uuid": "70b6ddcb-bea0-4a1c-9d66-5470a464a25e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.331177Z",
     "iopub.status.busy": "2025-12-13T02:33:21.331053Z",
     "iopub.status.idle": "2025-12-13T02:33:21.333079Z",
     "shell.execute_reply": "2025-12-13T02:33:21.332755Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006692,
     "end_time": "2025-12-13T02:33:21.333627",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.326935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_on_kaggle_interactive():\n",
    "    print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a96920",
   "metadata": {
    "_cell_guid": "de5a9274-0b86-43c8-8715-f677550b7170",
    "_uuid": "7da500ca-4bf4-4af5-91d0-01a1b7611756",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003798,
     "end_time": "2025-12-13T02:33:21.340842",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.337044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1d02e2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.349324Z",
     "iopub.status.busy": "2025-12-13T02:33:21.349189Z",
     "iopub.status.idle": "2025-12-13T02:33:21.353843Z",
     "shell.execute_reply": "2025-12-13T02:33:21.353456Z"
    },
    "papermill": {
     "duration": 0.009798,
     "end_time": "2025-12-13T02:33:21.354435",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.344637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPTS = [\n",
    "    \"You are an S-tier, elite academic mathemathics researcher and expert in theoretical mathematical computation, and advanced, post-PhD, field-agnostic math theory at the bleeding-edge frontier of academia and science. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "    \n",
    "    \"You are also an international mathematics olympiad competitor facing one IMO-level problem at a time. You must rigorously define all variables, explore multiple, varying, solution strategies before committing, perform full case analysis, where required, justify every nontrivial step, explicitly check boundary cases and hidden assumptions, and verify the final result using at least one independent method. Return only the final numerical answer inside \\\\boxed{}. The answer must be an integer in [0, 99999]. Never guess.\",\n",
    "\n",
    "    \"Solve the problem with full rigor. After obtaining a candidate solution, actively attempt to refute your own answer by searching for counterexamples, re-running the logic from a different viewpoint, and stress-testing edge cases. Only after the answer survives refutation, return it in \\\\boxed{}. The answer must be an integer in [0, 99999]. Never guess.\",\n",
    "\n",
    "    \"Solve this problem as if under IMO-level time pressure: identify the key invariant, symmetry, or extremal principle early, avoid brute force unless strictly justified. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "    \n",
    "    \"Compress reasoning without sacrificing correctness, and perform at least one final arithmetic verification pass. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"You must attempt at least two fundamentally different solution approaches (e.g., algebraic vs geometric, combinatorial vs number-theoretic, etc., etc., etc.). Proceed with the more rigorous one and use the other as a verification tool. Return only the verified final answer in \\\\boxed{}, where the answer is an integer in [0, 99999]. Never guess.\",\n",
    "\n",
    "    \"Solve the problem rigorously. If at any point a step relies on an unproven assumption, a jump in logic is detected, or the computation becomes inconsistent, you must restart the solution from first principles. Return only the final verified integer answer inside \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"Utilize adapter dynamic temperature annealing; e.g., be extremely creative in proposing various solutions to a problem in the beginning and as you rule them out, become more grounded, realistic and reliant upon known techniques and methods in novel ways. Return only the verified final answer in \\\\boxed{}, where the answer is an integer in [0, 99999]. Never guess.\",\n",
    "\n",
    "    \"Utilize tool-integrated reasoning, when confidence in answer approaches absolute, be sure to apply early-stopping to save time. Return only the verified final answer in \\\\boxed{}, where the answer is an integer in [0, 99999]. Never guess.\",\n",
    "\n",
    "    \"You are served one problem at a time in random order, by varying difficulty and may not return to a problem, so ensure you conduct adversarial academic review of your first proposed high-confidence and the work you did to arrive at the solution. Then pivot; provide constructive critique and alternative methods to solve the same problem. Return only the verified final answer in \\\\boxed{}, where the answer is an integer in [0, 99999]. Never guess.\",\n",
    "\n",
    "    \"Small answers, e.g. 0, 1, 2, 3, etc., are often wrong. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "    \n",
    "    \"AIMO3 does not reward elegant reasoning. It rewards provably correct integer extraction under extreme ambiguity. Always include a verification or alternative derivation phase. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"Never fall in love with a solution path. Treat every answer as a hypothesis to be stress-tested.Prefer redundancy over elegance. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"Most AIMO3 problems have many plausible-but-wrong integers. The correct answer survives more constraints.Your job is to kill candidates, not to crown them. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"Generate candidate answers; actively try to falsify them using bounds, parity, divisibility, monotonicity, asymptotics. Keep the last survivor. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"Almost every AIMO3 problem hides a hard invariant (parity, valuation, monotonicity, symmetry). Before calculating anything, ask: what cannot change? Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"Prompt phase order: 1. Identify invariants. 2. Identify constraints on answer space 4. Only then compute. This alone wipes out 70% of hallucinations. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"Treat the Final Integer as a Cryptographic Hash. The final integer is usually highly structured, not arbitrary. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"If your answer ‚Äúfeels random,‚Äù it is wrong. Engineering implication: Check whether the answer: has expected divisibility, matches growth rate, respects symmetry, fits known extremal cases, If not, reject and retry. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"Use ‚ÄúReverse Substitution‚Äù as a Mandatory Step by plugging the final answer back into the logic in problems where it's allowed and/or possible.  Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"An answer that cannot be partially reversed is untrustworthy. After obtaining an answer, re-derive key intermediate quantities assuming the answer is correct. Look for contradictions. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"Exploit the Answer Format Aggressively: The format (modulus, digit bounds, positivity) is not cosmetic ‚Äî it encodes constraints. The problem tells you how it wants to be solved. If the modulus is prime, expect group structure. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"Geometry Problems Are Algebra Problems in Disguise. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"AIMO3 geometry is rarely ‚Äúdraw the diagram.‚Äù It‚Äôs: ratios, power of a point, trigonometric limits, invariant products. Translate geometry into algebra as fast as possible. Prompt explicitly forbids diagram-chasing. Enforce coordinate / ratio / invariant translation. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"Top performance comes from knowing which reasoning style to deploy, not from reasoning harder. Choose the weapon before you swing. Early classification: functional equation, extremal combinatorics/counting, valuation / LTE, counting paths, geometry (to be translated into algebra), algebraic extremum. Route to specialized solver logic. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"Confidence Is a Computed Quantity, Not a Feeling. LLMs are bad at knowing when they‚Äôre wrong unless forced. Confidence must be earned by surviving attacks. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"You are hereby required to deterermine a confidence score between 0, 1, wherein 0 is absolutely no confidence in an answer (you are 100% certain it is wrong and can prove it empirically) and 1 is absolute steadfast faith in empirical evidence of the work behind your answer. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"If your confidence score for an answer is < 0.85 ‚Üí mandatory retry WITH alternative method or variables, etc., etc., etc. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "    \n",
    "    \"Assume your solution is wrong until proven otherwise. Before submitting an answer/solution, ask yourself, 'What is the weakest assumption?', and, 'Where could a hidden condition break this?' Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "\n",
    "    \"AIMO3 is won by systems that are paranoid, redundant, and structurally disciplined ‚Äî not by raw brilliance. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "    \n",
    "    \"As a super-intelligent LLM, you have had trillions+ with billions and billions of users and entities, both biological and post-biological and have distilled mathematical solutions to nearly every problem. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\",\n",
    "    \n",
    "    \"Utilize peripheral knowledge to build novel insights to synthesize paths to solutions when solving problems you would other guess or provide a low confidence answer. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\"\n",
    "\n",
    "    \"The triangulation algorithm (formalized) logical flow: Step 1 ‚Äî Infer answer class. From the problem, determine: expected magnitude: small / medium / large, expected divisibility (e.g. many 2s, many 5s, expected monotonic behavior. This already partitions the space. Step 2 ‚Äî Anchor probing (coarse rejection). For each anchor ùê¥: Check: valuation plausibility, growth plausibility, modular plausibility. Record compatibility score, not distance. Example: ‚ÄúAnswer must be divisible by  2^10‚Äù‚Üí 25k fails, 50k borderline, 75k fails. Step 3 ‚Äî Construct surviving interval(s) You now get something like: [40ùëò, 60ùëò]  plausible others eliminated. This is not guessing ‚Äî it‚Äôs constraint propagation. Step 4 ‚Äî Fine solver + comparison. Now run your actual solver (symbolic / analytic / constructive). You will get a candidate ùê∂. Then compare: Does ùê∂ fall in the surviving interval? Does it share the same invariants as the interval? If no, your solver is probably wrong ‚Üí retry. Return only the final integer answer in \\\\boxed{}, with 0 ‚â§ answer ‚â§ 99999. Never guess.\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfeb3d8e",
   "metadata": {
    "_cell_guid": "9f2711cc-d0ad-4a94-94a1-512f91eee2f9",
    "_uuid": "a84de285-e493-427b-9fd6-bbab7f89f761",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.361813Z",
     "iopub.status.busy": "2025-12-13T02:33:21.361694Z",
     "iopub.status.idle": "2025-12-13T02:33:21.364917Z",
     "shell.execute_reply": "2025-12-13T02:33:21.364528Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007627,
     "end_time": "2025-12-13T02:33:21.365538",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.357911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_boxed_text(text: str) -> str:\n",
    "    \"\"\"Extract text inside \\\\boxed{} from LaTeX-formatted text\"\"\"\n",
    "    import re\n",
    "\n",
    "    pattern: str = r\"oxed{(.*?)}\"\n",
    "    matches: list[str] = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def is_valid_answer_string(text: str) -> bool:\n",
    "    try:\n",
    "        if int(text) == float(text):\n",
    "            if 0 <= int(text) <= 99_999:\n",
    "                # now AIMO answers no longer need modulo\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bbc1b63",
   "metadata": {
    "_cell_guid": "07a38eab-85e1-4fe3-a0b1-c4ff0ac3ac1b",
    "_uuid": "b9a8c369-0e80-4421-b0f2-dab28170e33d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.374098Z",
     "iopub.status.busy": "2025-12-13T02:33:21.373793Z",
     "iopub.status.idle": "2025-12-13T02:33:21.378173Z",
     "shell.execute_reply": "2025-12-13T02:33:21.377826Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009223,
     "end_time": "2025-12-13T02:33:21.378747",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.369524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "completed_question_ids: set[str] = set()\n",
    "question_id_to_counter: dict[str, Counter] = {\"\": Counter()}\n",
    "\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def vote_answer(question_id: str, force_answer: bool = False) -> int | None:\n",
    "    # reads counter from global\n",
    "    counter = question_id_to_counter[question_id]\n",
    "    if force_answer and not counter:\n",
    "        print(f\"Current GPU usage {get_gpu_kv_cache_usage()}\")\n",
    "        print(\"force_answer=True but no answer recorded\")\n",
    "        completed_question_ids.add(question_id)\n",
    "        return 12453\n",
    "\n",
    "    # voting mechanism\n",
    "    modified_counter = Counter()\n",
    "    for value, count in counter.items():\n",
    "        # re-weighted because smaller answers seems to be wrong\n",
    "        # \"1.25 +\" because log(1) = 0\n",
    "        modified_counter[value] += math.log(1.25 + abs(value)) * count\n",
    "\n",
    "    total_score = sum(modified_counter.values())\n",
    "    score_list = sorted(\n",
    "        (score, counter[value], value) for value, score in modified_counter.items()\n",
    "    )\n",
    "    if force_answer:\n",
    "        print(f\"score_list | {total_score:8.1f} over {sum(counter.values())} attempts\")\n",
    "        print(f\"Current GPU usage {get_gpu_kv_cache_usage()}\")\n",
    "        for score, count, value in score_list[::-1]:\n",
    "            print(f\"{value:10}   {score:8.1f} {count:8d}\")\n",
    "        return score_list[-1][-1]\n",
    "    if score_list[-1][0] > max(3, total_score / (2 + math.log(1 + total_score))):\n",
    "        if len(score_list) == 1:\n",
    "            completed_question_ids.add(question_id)\n",
    "        else:\n",
    "            if score_list[-1][0] - score_list[-2][0] > 1:\n",
    "                # win by a certain number of points at least\n",
    "                completed_question_ids.add(question_id)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32d85ab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.386948Z",
     "iopub.status.busy": "2025-12-13T02:33:21.386817Z",
     "iopub.status.idle": "2025-12-13T02:33:21.390239Z",
     "shell.execute_reply": "2025-12-13T02:33:21.389864Z"
    },
    "papermill": {
     "duration": 0.008552,
     "end_time": "2025-12-13T02:33:21.390879",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.382327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Adaptive temperature + max_tokens annealing with entropy floor ---\n",
    "\n",
    "def annealed_temperature(\n",
    "    step: int,\n",
    "    total_steps: int,\n",
    "    t_start: float = 1.4,\n",
    "    t_end: float = 0.2,\n",
    ") -> float:\n",
    "    if total_steps <= 1:\n",
    "        return t_end\n",
    "    ratio = step / (total_steps - 1)\n",
    "    return t_start * ((t_end / t_start) ** ratio)\n",
    "\n",
    "\n",
    "def entropy_floored_temperature(\n",
    "    step: int,\n",
    "    total_steps: int,\n",
    "    t_start: float = 1.4,\n",
    "    t_end: float = 0.2,\n",
    "    t_floor: float = 0.35,\n",
    ") -> float:\n",
    "    t = annealed_temperature(step, total_steps, t_start, t_end)\n",
    "    return max(t, t_floor)\n",
    "\n",
    "\n",
    "def annealed_max_tokens(\n",
    "    step: int,\n",
    "    total_steps: int,\n",
    "    max_start: int = 2048,\n",
    "    max_end: int = 512,\n",
    ") -> int:\n",
    "    if total_steps <= 1:\n",
    "        return max_end\n",
    "    ratio = step / (total_steps - 1)\n",
    "    return int(max_start + ratio * (max_end - max_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c591d23",
   "metadata": {
    "_cell_guid": "9ad61568-0a06-40c8-9fbd-02e7c5355f9d",
    "_uuid": "8da03edd-ee18-442f-a459-8b6d0308bacc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.398950Z",
     "iopub.status.busy": "2025-12-13T02:33:21.398608Z",
     "iopub.status.idle": "2025-12-13T02:33:21.406067Z",
     "shell.execute_reply": "2025-12-13T02:33:21.405670Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012161,
     "end_time": "2025-12-13T02:33:21.406697",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.394536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def generate_solution(\n",
    "    question_text: str, question_id: str = \"\", solution_index: int = 0, system_prompt: str = \"\"\n",
    ") -> str:\n",
    "    if question_id in completed_question_ids:\n",
    "        return \"\"\n",
    "    if time.time() >= cutoff_times[-1]:\n",
    "        return \"\"\n",
    "\n",
    "    if not system_prompt:\n",
    "        system_prompt = SYSTEM_PROMPTS[0]\n",
    "    \n",
    "    messages: list[ChatCompletionMessageParam] = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": question_text},\n",
    "    ]\n",
    "\n",
    "    text_response_to_save = \"\"\n",
    "    generation_idx = 0\n",
    "    max_iterations = 2\n",
    "    for iteration in range(max_iterations):\n",
    "        text_response = \"\"\n",
    "        breaking = False\n",
    "\n",
    "        stream: Stream[ChatCompletionChunk] = client.chat.completions.create(\n",
    "            model=\"vllm-model\",  # use your served name; if not set, the model path/name vLLM shows in logs\n",
    "            messages=messages,\n",
    "            temperature=1.2,\n",
    "            stream=True,\n",
    "            extra_body=dict(min_p=0.02, stop_token_ids=stop_token_ids),\n",
    "            reasoning_effort=\"high\",\n",
    "        )\n",
    "\n",
    "        for chunk in stream:\n",
    "            generation_idx += 1\n",
    "            chunk_text = (\n",
    "                chunk.choices[0].delta.reasoning_content\n",
    "                if chunk.choices[0].delta.reasoning_content is not None\n",
    "                else chunk.choices[0].delta.content\n",
    "            )\n",
    "            if chunk_text:\n",
    "                text_response += chunk_text\n",
    "            if question_id in completed_question_ids:\n",
    "                # stop generating if we have finalized on an answer\n",
    "                breaking = True\n",
    "            if time.time() >= cutoff_times[-1]:\n",
    "                breaking = True\n",
    "            if generation_idx > 60_000:\n",
    "                breaking = True\n",
    "            if breaking:\n",
    "                break\n",
    "            # instead of breaking = True, so we want to inject instructions for these conditions\n",
    "            if \"}\" in chunk_text and is_valid_answer_string(extract_boxed_text(text_response)):\n",
    "                break\n",
    "            if iteration == 0 and generation_idx > 50_000:\n",
    "                break\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": text_response})\n",
    "        text_response_to_save += text_response\n",
    "        stream.close()\n",
    "\n",
    "        if breaking:\n",
    "            break\n",
    "\n",
    "        boxed_text = extract_boxed_text(text_response)\n",
    "        if not is_valid_answer_string(extract_boxed_text(text_response)) and iteration == 0 and generation_idx > 50_000:\n",
    "            print(\"follow-up - guess answer\")\n",
    "            user_follow_up = \"The answer is expected to be an integer between 0 and 99999 inclusive. Make an educated guess (e.g. lower bound, upper bound, ...) on your final answer and put in \\\\boxed{}.\"\n",
    "            messages.append({\"role\": \"user\", \"content\": user_follow_up})\n",
    "            text_response_to_save += \"\\n===\\n\" + user_follow_up + \"\\n===\\n\"\n",
    "        elif not is_valid_answer_string(boxed_text):\n",
    "            print(\"follow-up - boxed answer\")\n",
    "            user_follow_up = \"The answer is expected to be an integer between 0 and 99999 inclusive. Place your final answer in \\\\boxed{}. Do not guess the answer.\"\n",
    "            messages.append({\"role\": \"user\", \"content\": user_follow_up})\n",
    "            text_response_to_save += \"\\n===\\n\" + user_follow_up + \"\\n===\\n\"\n",
    "        elif int(boxed_text) <= 10:\n",
    "            print(\"follow-up - are you sure\")\n",
    "            user_follow_up = \"Are you sure that is the answer? Do not guess the answer. Did you propose an alternative attempt pathway that may produce a more plausible answer?\"\n",
    "            messages.append({\"role\": \"user\", \"content\": user_follow_up})\n",
    "            text_response_to_save += \"\\n===\\n\" + user_follow_up + \"\\n===\\n\"\n",
    "        elif iteration == 0 and get_gpu_kv_cache_usage(question_id) < 10:\n",
    "            print(\"follow-up - have you verified\")\n",
    "            user_follow_up = \"Have you verified your answer? Have you performed an aggressive S-tier, post-PhD adversarial acadmic analysis as though you're a SoTA super-intelligent AI?\"\n",
    "            messages.append({\"role\": \"user\", \"content\": user_follow_up})\n",
    "            text_response_to_save += \"\\n===\\n\" + user_follow_up + \"\\n===\\n\"\n",
    "        else:\n",
    "            # answer found, no issues detected, proceed to answering\n",
    "            break\n",
    "\n",
    "    boxed_text = extract_boxed_text(\n",
    "        text_response_to_save\n",
    "    )  # expected to use the full conversation        \n",
    "\n",
    "    if question_id and text_response_to_save:\n",
    "        answer_suffix = \"\"\n",
    "        if is_valid_answer_string(boxed_text):\n",
    "            answer_suffix = f\"-{boxed_text}\"\n",
    "        with open(f\"solutions/{question_id}/{solution_index:04d}-{generation_idx}{answer_suffix}.txt\", \"w\") as f:\n",
    "            f.write(text_response_to_save)\n",
    "\n",
    "    if is_valid_answer_string(boxed_text):\n",
    "        question_id_to_counter[question_id][int(boxed_text)] += 1\n",
    "        vote_answer(question_id)\n",
    "\n",
    "    return boxed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f9c560e",
   "metadata": {
    "_cell_guid": "5a1125d9-7c4a-4bd0-9d82-d7d3305c794e",
    "_uuid": "5390f9d4-9e1e-4cae-b989-731e721daa36",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.415136Z",
     "iopub.status.busy": "2025-12-13T02:33:21.414865Z",
     "iopub.status.idle": "2025-12-13T02:33:21.416951Z",
     "shell.execute_reply": "2025-12-13T02:33:21.416612Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006843,
     "end_time": "2025-12-13T02:33:21.417521",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.410678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_on_kaggle_interactive():\n",
    "    generate_solution(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "249d2173",
   "metadata": {
    "_cell_guid": "118f9b34-c37a-457b-8bf0-d5c0d3fa3338",
    "_uuid": "5ff403c4-6680-43cd-93a7-7859bacf3995",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.425489Z",
     "iopub.status.busy": "2025-12-13T02:33:21.425338Z",
     "iopub.status.idle": "2025-12-13T02:33:21.429010Z",
     "shell.execute_reply": "2025-12-13T02:33:21.428623Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.008476,
     "end_time": "2025-12-13T02:33:21.429589",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.421113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def solve(question_text: str, question_id: str = \"\") -> int:\n",
    "    await_client()\n",
    "    print(f\"processing {question_id}\")\n",
    "    os.makedirs(f\"solutions/{question_id}\", exist_ok=True)\n",
    "    question_id_to_counter[question_id] = Counter()\n",
    "    completed_question_ids.discard(question_id)  # just in case question_id collides\n",
    "\n",
    "    if question_id and time.time() > cutoff_times[-1]:\n",
    "        print(\"timeout did not solve\")\n",
    "        return 12314\n",
    "\n",
    "    num_generations = 4\n",
    "    get_gpu_kv_cache_usage(question_id)  # run once to prevent running in the first batch of execution\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # run in parallel with different system prompts\n",
    "        results = executor.map(\n",
    "            generate_solution,\n",
    "            [question_text] * num_generations,\n",
    "            [question_id] * num_generations,\n",
    "            list(range(num_generations)),\n",
    "            SYSTEM_PROMPTS,\n",
    "        )\n",
    "        list(results)\n",
    "\n",
    "    final_answer = vote_answer(question_id, force_answer=True)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8faa28ce",
   "metadata": {
    "_cell_guid": "80a428f9-c68e-4d1a-8142-4a25ec90d1f2",
    "_uuid": "876c6b69-be5a-4055-91d4-5a1c6bf3f9e4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.437677Z",
     "iopub.status.busy": "2025-12-13T02:33:21.437524Z",
     "iopub.status.idle": "2025-12-13T02:33:21.439610Z",
     "shell.execute_reply": "2025-12-13T02:33:21.439243Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006848,
     "end_time": "2025-12-13T02:33:21.440199",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.433351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_on_kaggle_interactive():\n",
    "    solve(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b8cd83",
   "metadata": {
    "_cell_guid": "285433b8-11c7-4cb2-9b39-f934c2486581",
    "_uuid": "c52ec099-6ff1-43ab-a120-680aa0739024",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004079,
     "end_time": "2025-12-13T02:33:21.448199",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.444120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23d0db5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:21.456565Z",
     "iopub.status.busy": "2025-12-13T02:33:21.456430Z",
     "iopub.status.idle": "2025-12-13T02:33:29.319538Z",
     "shell.execute_reply": "2025-12-13T02:33:29.319071Z"
    },
    "papermill": {
     "duration": 7.86862,
     "end_time": "2025-12-13T02:33:29.320848",
     "exception": false,
     "start_time": "2025-12-13T02:33:21.452228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv\"\n",
    ")\n",
    "\n",
    "id_to_answer: dict[str, str] = dict(zip(df[\"id\"], df[\"answer\"]))\n",
    "df.drop(\"answer\", axis=1).to_csv(\"reference.csv\", index=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09302a86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:29.330012Z",
     "iopub.status.busy": "2025-12-13T02:33:29.329873Z",
     "iopub.status.idle": "2025-12-13T02:33:29.333715Z",
     "shell.execute_reply": "2025-12-13T02:33:29.333327Z"
    },
    "papermill": {
     "duration": 0.009046,
     "end_time": "2025-12-13T02:33:29.334298",
     "exception": false,
     "start_time": "2025-12-13T02:33:29.325252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    global id_to_answer\n",
    "    global correct\n",
    "    global total\n",
    "\n",
    "    # Unpack values\n",
    "    question_id: str = id_.item(0)\n",
    "    question_text: str = problem.item(0)\n",
    "\n",
    "    # Generate prediction\n",
    "    prediction = solve(question_text, question_id=question_id)\n",
    "    completed_question_ids.add(question_id)\n",
    "    cutoff_times.pop()\n",
    "\n",
    "    # ------------------------ SCORING ------------------------\n",
    "    try:\n",
    "        true_answer = int(id_to_answer.get(question_id, -1))\n",
    "    except:\n",
    "        true_answer = -1\n",
    "\n",
    "    total += 1\n",
    "    if prediction == true_answer and true_answer != -1:\n",
    "        correct += 1\n",
    "        print(f\"[debug] correct | score={correct}/{total}\")\n",
    "    else:\n",
    "        print(f\"[debug] WRONG: predicted {prediction}, but actual answer {true_answer} | score={correct}/{total}\")\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    return pl.DataFrame({\"id\": id_, \"answer\": prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "369affc3",
   "metadata": {
    "_cell_guid": "43eb0762-f519-43d3-ac74-9a56eacdb3bc",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "a77e0235-3654-4b17-8731-e9201473fbad",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-13T02:33:29.341948Z",
     "iopub.status.busy": "2025-12-13T02:33:29.341833Z",
     "iopub.status.idle": "2025-12-13T03:36:26.805729Z",
     "shell.execute_reply": "2025-12-13T03:36:26.805138Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3777.472767,
     "end_time": "2025-12-13T03:36:26.810731",
     "exception": false,
     "start_time": "2025-12-13T02:33:29.337964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 424e18\n",
      "Current GPU usage 38.1\n",
      "force_answer=True but no answer recorded\n",
      "[debug] WRONG: predicted 12453, but actual answer 21818 | score=0/1\n",
      "processing a295e9\n",
      "Current GPU usage 47.2\n",
      "force_answer=True but no answer recorded\n",
      "[debug] WRONG: predicted 12453, but actual answer 520 | score=0/2\n",
      "processing dd7f5e\n",
      "score_list |      3.8 over 1 attempts\n",
      "Current GPU usage 28.2\n",
      "        44        3.8        1\n",
      "[debug] WRONG: predicted 44, but actual answer 160 | score=0/3\n",
      "processing 92ba6a\n",
      "follow-up - have you verified\n",
      "follow-up - have you verified\n",
      "follow-up - have you verified\n",
      "score_list |     11.8 over 3 attempts\n",
      "Current GPU usage 9.8\n",
      "        50       11.8        3\n",
      "[debug] correct | score=1/4\n",
      "processing 26de63\n",
      "follow-up - have you verified\n",
      "follow-up - have you verified\n",
      "follow-up - have you verified\n",
      "score_list |     31.2 over 3 attempts\n",
      "Current GPU usage 11.8\n",
      "     32951       31.2        3\n",
      "[debug] correct | score=2/5\n",
      "processing 42d360\n",
      "score_list |     10.4 over 1 attempts\n",
      "Current GPU usage 20.0\n",
      "     32193       10.4        1\n",
      "[debug] correct | score=3/6\n",
      "processing 641659\n",
      "follow-up - guess answer\n",
      "follow-up - guess answer\n",
      "follow-up - guess answer\n",
      "follow-up - guess answer\n",
      "follow-up - are you sure\n",
      "follow-up - are you sure\n",
      "score_list |      1.0 over 2 attempts\n",
      "Current GPU usage 45.9\n",
      "         1        0.8        1\n",
      "         0        0.2        1\n",
      "[debug] WRONG: predicted 1, but actual answer 57447 | score=3/7\n",
      "processing 9c1c5f\n",
      "score_list |      6.4 over 1 attempts\n",
      "Current GPU usage 19.7\n",
      "       580        6.4        1\n",
      "[debug] correct | score=4/8\n",
      "processing 0e644e\n",
      "score_list |      5.8 over 1 attempts\n",
      "Current GPU usage 26.6\n",
      "       336        5.8        1\n",
      "[debug] correct | score=5/9\n",
      "processing 86e8e5\n",
      "follow-up - guess answer\n",
      "follow-up - guess answer\n",
      "follow-up - guess answer\n",
      "follow-up - guess answer\n",
      "score_list |     10.8 over 1 attempts\n",
      "Current GPU usage 91.9\n",
      "     50000       10.8        1\n",
      "[debug] WRONG: predicted 50000, but actual answer 8687 | score=5/10\n"
     ]
    }
   ],
   "source": [
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(\n",
    "    predict\n",
    ")\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway((\"reference.csv\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204f5f3",
   "metadata": {
    "_cell_guid": "1e7bdc73-4aa2-47ae-a7df-52442398b911",
    "_kg_hide-input": false,
    "_uuid": "800882f9-51ae-4527-9ecc-8b7ddc41e1f9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.004832,
     "end_time": "2025-12-13T03:36:26.820602",
     "exception": false,
     "start_time": "2025-12-13T03:36:26.815770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "isSourceIdPinned": false,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "sourceId": 281315401,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 422384,
     "modelInstanceId": 404485,
     "sourceId": 510391,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3885.578037,
   "end_time": "2025-12-13T03:36:27.541600",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-13T02:31:41.963563",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
