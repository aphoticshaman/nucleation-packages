{
  "nbformat": 4,
  "nbformat_minor": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PROMETHEUS v4.2 - NF4 Quantized Edition\n",
        "\n",
        "**Model:** Qwen2.5-Math-72B-Instruct (NF4 4-bit)  \n",
        "**Framework:** transformers + bitsandbytes  \n",
        "**Budget:** 280 minutes  \n",
        "\n",
        "---\n",
        "\n",
        "**Required:** Upload your NF4 model to Kaggle as a dataset first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# CELL 1: Install Dependencies + GPU Check\nimport sys\nimport subprocess\nimport os\n\nprint(\"=== CELL 1: Setup ===\")\n\n# Purge any utility script paths\nbad_paths = [p for p in sys.path if 'utility_script' in p.lower()]\nfor p in bad_paths:\n    sys.path.remove(p)\n    print(f\"Removed from sys.path: {p}\")\n\n# Check triad-dev wheels\nWHEEL_PATH = \"/kaggle/input/triad-dev/utility_wheels\"\nprint(f\"Wheel path exists: {os.path.exists(WHEEL_PATH)}\")\nif os.path.exists(WHEEL_PATH):\n    print(f\"Wheels available: {os.listdir(WHEEL_PATH)}\")\n\n# Install bitsandbytes + accelerate from wheels\ndef install_wheel(name_prefix):\n    if not os.path.exists(WHEEL_PATH):\n        print(f\"ERROR: {WHEEL_PATH} not found!\")\n        return False\n    \n    for f in os.listdir(WHEEL_PATH):\n        if f.startswith(name_prefix):\n            wheel = f\"{WHEEL_PATH}/{f}\"\n            print(f\"Installing {f}...\")\n            result = subprocess.run(\n                [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-index\", \"--no-deps\", wheel],\n                capture_output=True, text=True\n            )\n            if result.returncode != 0:\n                print(f\"  Error: {result.stderr[:200]}\")\n                return False\n            print(f\"  \u2713 Installed\")\n            return True\n    print(f\"  Wheel not found for {name_prefix}\")\n    return False\n\n# Install both\ninstall_wheel(\"bitsandbytes\")\ninstall_wheel(\"accelerate\")\n\n# Verify\ntry:\n    import bitsandbytes as bnb\n    print(f\"\u2713 bitsandbytes {bnb.__version__}\")\nexcept ImportError as e:\n    print(f\"\u2717 bitsandbytes import failed: {e}\")\n\ntry:\n    import accelerate\n    print(f\"\u2713 accelerate {accelerate.__version__}\")\nexcept ImportError as e:\n    print(f\"\u2717 accelerate import failed: {e}\")\n\n# GPU check\nimport torch\nassert torch.cuda.is_available(), \"GPU NOT ENABLED\"\nprint(f\"\u2713 GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"\u2713 VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# CELL 2: Imports & Configuration\nimport os, sys, gc, time, random, warnings, re, traceback\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Tuple, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom collections import Counter\n\nwarnings.filterwarnings('ignore')\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nimport polars as pl\n\n# ============== YOUR PATHS ==============\nMODEL_BASE = \"/kaggle/input/m/ryancardwell/qwen-72b-math-nf4/transformers/v1/1\"\n\n# Find config.json - could be in base or a subfolder\ndef find_model_path(base):\n    # Check base directly\n    if os.path.exists(f\"{base}/config.json\"):\n        return base\n    \n    # Check immediate subfolders\n    if os.path.isdir(base):\n        for item in os.listdir(base):\n            subpath = f\"{base}/{item}\"\n            if os.path.isdir(subpath) and os.path.exists(f\"{subpath}/config.json\"):\n                print(f\"Found config.json in subfolder: {item}\")\n                return subpath\n    \n    # Recursive search (one more level)\n    if os.path.isdir(base):\n        for item in os.listdir(base):\n            subpath = f\"{base}/{item}\"\n            if os.path.isdir(subpath):\n                for subitem in os.listdir(subpath):\n                    subsubpath = f\"{subpath}/{subitem}\"\n                    if os.path.isdir(subsubpath) and os.path.exists(f\"{subsubpath}/config.json\"):\n                        print(f\"Found config.json in: {item}/{subitem}\")\n                        return subsubpath\n    \n    print(f\"WARNING: No config.json found! Using base: {base}\")\n    return base\n\nMODEL_PATH = find_model_path(MODEL_BASE)\nprint(f\"Model path: {MODEL_PATH}\")\n\n# Verify config.json exists\nif os.path.exists(f\"{MODEL_PATH}/config.json\"):\n    print(f\"\u2713 config.json found\")\n    # Show model_type\n    import json as _json\n    with open(f\"{MODEL_PATH}/config.json\") as f:\n        cfg = _json.load(f)\n        print(f\"\u2713 model_type: {cfg.get('model_type', 'NOT FOUND')}\")\nelse:\n    print(f\"\u2717 config.json NOT FOUND at {MODEL_PATH}\")\n    print(f\"  Contents: {os.listdir(MODEL_PATH) if os.path.isdir(MODEL_PATH) else 'NOT A DIR'}\")\n\nCOMPETITION_DIR = \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3\"\nTRIAD_DEV = \"/kaggle/input/triad-dev\"\nTEST_CSV = f\"{COMPETITION_DIR}/test.csv\"\nLOCAL_TEST_JSONL = f\"{TRIAD_DEV}/test_dataset/test.jsonl\"\nLOCAL_ANSWERS_JSON = f\"{TRIAD_DEV}/test_dataset/answers.json\"\n\n# Budget\nBUDGET_SECONDS = 280 * 60\nSTART_TIME = time.time()\n\ndef time_remaining() -> float:\n    return BUDGET_SECONDS - (time.time() - START_TIME)\n\ndef time_str() -> str:\n    r = time_remaining()\n    return f\"{int(r//60)}m{int(r%60)}s\"\n\nprint(f\"\u2713 Budget: {BUDGET_SECONDS//60} minutes\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3: Load NF4 Model with bitsandbytes\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "print(f\"Loading NF4 model from {MODEL_PATH}...\")\n",
        "print(f\"Time: {time_str()}\")\n",
        "\n",
        "# BitsAndBytes 4-bit config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(f\"\u2713 Model loaded in {time.time() - START_TIME:.1f}s\")\n",
        "print(f\"\u2713 Memory: {torch.cuda.memory_allocated()/1e9:.1f}GB allocated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4: Problem Classification\n",
        "class ProblemType(Enum):\n",
        "    NUMBER_THEORY = \"number_theory\"\n",
        "    COMBINATORICS = \"combinatorics\"\n",
        "    ALGEBRA = \"algebra\"\n",
        "    GEOMETRY = \"geometry\"\n",
        "    MIXED = \"mixed\"\n",
        "\n",
        "@dataclass\n",
        "class ProblemProfile:\n",
        "    ptype: ProblemType\n",
        "    has_modulo: bool = False\n",
        "    modulo_target: Optional[int] = None\n",
        "    is_counting: bool = False\n",
        "\n",
        "def classify_problem(text: str) -> ProblemProfile:\n",
        "    t = text.lower()\n",
        "    \n",
        "    # Modulo detection\n",
        "    mod_match = re.search(r'(?:mod|modulo)\\s*(\\d+)', t)\n",
        "    remainder_match = re.search(r'remainder.*?(?:divided by|when.*?by)\\s*(\\d+)', t)\n",
        "    has_mod = bool(mod_match or remainder_match or 'modulo' in t)\n",
        "    mod_target = int(mod_match.group(1)) if mod_match else (int(remainder_match.group(1)) if remainder_match else None)\n",
        "    \n",
        "    # Type keywords\n",
        "    scores = {\n",
        "        ProblemType.NUMBER_THEORY: sum(1 for k in ['prime', 'divisor', 'gcd', 'lcm', 'modulo', 'factorial'] if k in t),\n",
        "        ProblemType.COMBINATORICS: sum(1 for k in ['how many', 'count', 'ways', 'permutation', 'combination'] if k in t),\n",
        "        ProblemType.GEOMETRY: sum(1 for k in ['triangle', 'circle', 'angle', 'area', 'perimeter'] if k in t),\n",
        "        ProblemType.ALGEBRA: sum(1 for k in ['polynomial', 'roots', 'equation', 'coefficient'] if k in t)\n",
        "    }\n",
        "    \n",
        "    best = max(scores.items(), key=lambda x: x[1])\n",
        "    ptype = best[0] if best[1] > 0 else ProblemType.MIXED\n",
        "    \n",
        "    return ProblemProfile(ptype=ptype, has_modulo=has_mod, modulo_target=mod_target,\n",
        "                         is_counting=any(k in t for k in ['how many', 'count']))\n",
        "\n",
        "print(\"\u2713 Classifier ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5: Code Execution\n",
        "STDLIB = '''\n",
        "import math\n",
        "from math import gcd, factorial, comb, isqrt, sqrt, ceil, floor\n",
        "from itertools import permutations, combinations, product\n",
        "from functools import reduce, lru_cache\n",
        "from collections import Counter, defaultdict\n",
        "from fractions import Fraction\n",
        "try:\n",
        "    from sympy import *\n",
        "    from sympy.ntheory import factorint, divisors, totient, isprime\n",
        "except: pass\n",
        "\n",
        "def lcm(a, b): return abs(a * b) // gcd(a, b)\n",
        "def is_prime(n):\n",
        "    if n < 2: return False\n",
        "    if n < 4: return True\n",
        "    if n % 2 == 0: return False\n",
        "    for i in range(3, isqrt(n) + 1, 2):\n",
        "        if n % i == 0: return False\n",
        "    return True\n",
        "def C(n, k): return comb(n, k) if 0 <= k <= n else 0\n",
        "'''\n",
        "\n",
        "SNOOP = '''\n",
        "for _v in [\"answer\", \"ans\", \"result\", \"res\", \"total\", \"count\"]:\n",
        "    if _v in dir() and isinstance(eval(_v), (int, float)):\n",
        "        print(f\"EXTRACTED:{int(eval(_v))}\")\n",
        "        break\n",
        "'''\n",
        "\n",
        "def execute_code(code: str, timeout: int = 30) -> Tuple[Optional[int], str]:\n",
        "    import signal\n",
        "    from io import StringIO\n",
        "    import contextlib\n",
        "    \n",
        "    full_code = STDLIB + '\\n' + code + '\\n' + SNOOP\n",
        "    stdout = StringIO()\n",
        "    \n",
        "    def handler(signum, frame): raise TimeoutError()\n",
        "    \n",
        "    try:\n",
        "        signal.signal(signal.SIGALRM, handler)\n",
        "        signal.alarm(timeout)\n",
        "        with contextlib.redirect_stdout(stdout):\n",
        "            exec(full_code, {'__builtins__': __builtins__})\n",
        "        signal.alarm(0)\n",
        "        \n",
        "        output = stdout.getvalue()\n",
        "        match = re.search(r'EXTRACTED:(\\d+)', output)\n",
        "        if match:\n",
        "            return int(match.group(1)), \"\"\n",
        "        # Try last number\n",
        "        nums = re.findall(r'\\b(\\d+)\\b', output)\n",
        "        if nums:\n",
        "            return int(nums[-1]), \"\"\n",
        "        return None, \"No answer found\"\n",
        "    except TimeoutError:\n",
        "        return None, \"Timeout\"\n",
        "    except Exception as e:\n",
        "        return None, f\"{type(e).__name__}: {str(e)[:50]}\"\n",
        "    finally:\n",
        "        signal.alarm(0)\n",
        "\n",
        "def extract_code(text: str) -> Optional[str]:\n",
        "    for pat in [r'```python\\n(.*?)```', r'```\\n(.*?)```']:\n",
        "        m = re.findall(pat, text, re.DOTALL)\n",
        "        if m: return m[0].strip()\n",
        "    return None\n",
        "\n",
        "print(\"\u2713 Executor ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6: Generation\n",
        "SYSTEM = \"\"\"You are a math solver. Write Python code to solve the problem.\n",
        "Store the final answer in a variable called 'answer'. Answer must be 0-99999.\"\"\"\n",
        "\n",
        "def generate(question: str, temperature: float = 0.7) -> str:\n",
        "    \"\"\"Generate single response\"\"\"\n",
        "    prompt = f\"<|im_start|>system\\n{SYSTEM}<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=2048,\n",
        "            temperature=temperature,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "def generate_batch(questions: List[str], temperature: float = 0.7) -> List[str]:\n",
        "    \"\"\"Generate for multiple prompts (sequential - bitsandbytes doesn't batch well)\"\"\"\n",
        "    return [generate(q, temperature) for q in questions]\n",
        "\n",
        "print(\"\u2713 Generator ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7: Main Solver\n",
        "@dataclass\n",
        "class Candidate:\n",
        "    answer: int\n",
        "    confidence: float = 0.5\n",
        "\n",
        "def normalize_answer(x: Any) -> Optional[int]:\n",
        "    try:\n",
        "        val = int(float(x))\n",
        "        return max(0, min(99999, val))\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def extract_text_answer(text: str) -> Optional[int]:\n",
        "    for pat in [r'\\\\boxed\\{(\\d+)\\}', r'[Aa]nswer[:\\s]+(\\d+)', r'= (\\d+)$']:\n",
        "        m = re.search(pat, text)\n",
        "        if m: return normalize_answer(m.group(1))\n",
        "    nums = re.findall(r'\\b(\\d+)\\b', text)\n",
        "    return normalize_answer(nums[-1]) if nums else None\n",
        "\n",
        "def predict_for_question(question: str) -> int:\n",
        "    profile = classify_problem(question)\n",
        "    print(f\"  Type: {profile.ptype.value}\")\n",
        "    \n",
        "    candidates = []\n",
        "    \n",
        "    # Generate 3 solutions (fewer than vLLM version - slower inference)\n",
        "    for i, temp in enumerate([0.7, 0.5, 0.3]):\n",
        "        if time_remaining() < 60:\n",
        "            print(f\"  Low time, stopping early\")\n",
        "            break\n",
        "            \n",
        "        print(f\"  Gen {i+1} @ temp={temp}...\")\n",
        "        resp = generate(question, temp)\n",
        "        \n",
        "        code = extract_code(resp)\n",
        "        if code:\n",
        "            result, err = execute_code(code)\n",
        "            if result is not None:\n",
        "                candidates.append(Candidate(result, 0.9 - i*0.1))\n",
        "                print(f\"    \u2192 {result}\")\n",
        "            else:\n",
        "                print(f\"    \u2717 {err[:30]}\")\n",
        "        else:\n",
        "            ans = extract_text_answer(resp)\n",
        "            if ans:\n",
        "                candidates.append(Candidate(ans, 0.4))\n",
        "                print(f\"    \u2192 {ans} (text)\")\n",
        "    \n",
        "    if not candidates:\n",
        "        return 0\n",
        "    \n",
        "    # Weighted vote\n",
        "    votes = {}\n",
        "    for c in candidates:\n",
        "        votes[c.answer] = votes.get(c.answer, 0) + c.confidence\n",
        "    \n",
        "    best = max(votes.items(), key=lambda x: x[1])[0]\n",
        "    print(f\"  Selected: {best}\")\n",
        "    return best\n",
        "\n",
        "print(\"\u2713 Solver ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8: API Interface\n",
        "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame:\n",
        "    id_val = id_.item()\n",
        "    q_text = question.item()\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Problem: {id_val} | Time: {time_str()}\")\n",
        "    print(f\"Q: {q_text[:150]}...\" if len(q_text) > 150 else f\"Q: {q_text}\")\n",
        "    \n",
        "    try:\n",
        "        answer = predict_for_question(q_text)\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR: {e}\")\n",
        "        answer = 0\n",
        "    \n",
        "    answer = max(0, min(99999, int(answer)))\n",
        "    print(f\"  FINAL: {answer}\")\n",
        "    \n",
        "    return pl.DataFrame({'id': id_val, 'answer': answer})\n",
        "\n",
        "print(\"\u2713 API ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 9: Local Validation (optional)\n",
        "def validate_local(n: int = 5):\n",
        "    import json as _json\n",
        "    with open(LOCAL_ANSWERS_JSON) as f:\n",
        "        answers = _json.load(f)\n",
        "    problems = [_json.loads(l) for l in open(LOCAL_TEST_JSONL)]\n",
        "    \n",
        "    correct = 0\n",
        "    for prob in problems[:n]:\n",
        "        pred = predict_for_question(prob['problem'])\n",
        "        exp = answers[prob['id']]\n",
        "        ok = pred == exp\n",
        "        correct += ok\n",
        "        print(f\"{'\u2713' if ok else '\u2717'} {prob['id']}: {pred} vs {exp}\")\n",
        "    \n",
        "    print(f\"\\nScore: {correct}/{n} ({100*correct/n:.0f}%)\")\n",
        "\n",
        "# Uncomment to test:\n",
        "# validate_local(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 10: Start Server\n",
        "import kaggle_evaluation.aimo_3_inference_server\n",
        "\n",
        "print(f\"\\nPROMETHEUS v4.2 NF4 | Budget: {BUDGET_SECONDS//60}m | Time: {time_str()}\")\n",
        "\n",
        "server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
        "\n",
        "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
        "    print(\"MODE: Competition\")\n",
        "    server.serve()\n",
        "else:\n",
        "    print(\"MODE: Local\")\n",
        "    server.run_local_gateway((TEST_CSV,))"
      ]
    }
  ]
}