{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb3d374",
   "metadata": {
    "papermill": {
     "duration": 0.00317,
     "end_time": "2025-12-06T04:28:16.268777",
     "exception": false,
     "start_time": "2025-12-06T04:28:16.265607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AIMO3 Turbo-Strap V4 (PROMETHEUS)\n",
    "## Unknown Knowns Extracted & Operationalized\n",
    "\n",
    "**V4 Novel Features:**\n",
    "1. **Problem Classifier** - Route to optimal strategy ensemble BEFORE inference\n",
    "2. **Kolmogorov Weighting** - Shorter code = higher confidence (Occam's Razor)\n",
    "3. **Oracle Enumeration** - If space <10^6, just brute force\n",
    "4. **Causal Grammar Extraction** - Parse constraints from problem text\n",
    "5. **Adaptive Temperature** - Start high, anneal on consensus detection\n",
    "6. **Time Budget Decay** - Exponential allocation (hard problems get more)\n",
    "7. **Entropy Sentinel** - Early stop on confidence spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f23729",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:28:16.275409Z",
     "iopub.status.busy": "2025-12-06T04:28:16.274833Z",
     "iopub.status.idle": "2025-12-06T04:28:39.367870Z",
     "shell.execute_reply": "2025-12-06T04:28:39.367232Z"
    },
    "papermill": {
     "duration": 23.097208,
     "end_time": "2025-12-06T04:28:39.368692",
     "exception": false,
     "start_time": "2025-12-06T04:28:16.271484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.18.0\r\n",
      "Uninstalling tensorflow-2.18.0:\r\n",
      "  Successfully uninstalled tensorflow-2.18.0\r\n",
      "Found existing installation: matplotlib 3.7.2\r\n",
      "Uninstalling matplotlib-3.7.2:\r\n",
      "  Successfully uninstalled matplotlib-3.7.2\r\n",
      "Found existing installation: keras 3.8.0\r\n",
      "Uninstalling keras-3.8.0:\r\n",
      "  Successfully uninstalled keras-3.8.0\r\n",
      "Found existing installation: scikit-learn 1.2.2\r\n",
      "Uninstalling scikit-learn-1.2.2:\r\n",
      "  Successfully uninstalled scikit-learn-1.2.2\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall --yes \"tensorflow\" \"matplotlib\" \"keras\" \"scikit-learn\" 2>/dev/null || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f61d67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:28:39.376169Z",
     "iopub.status.busy": "2025-12-06T04:28:39.376005Z",
     "iopub.status.idle": "2025-12-06T04:28:54.480949Z",
     "shell.execute_reply": "2025-12-06T04:28:54.480330Z"
    },
    "papermill": {
     "duration": 15.109339,
     "end_time": "2025-12-06T04:28:54.481646",
     "exception": false,
     "start_time": "2025-12-06T04:28:39.372307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU: NVIDIA H100 80GB HBM3 | VRAM: 85.3GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "assert torch.cuda.is_available(), \"GPU NOT ENABLED\"\n",
    "print(f\"✓ GPU: {torch.cuda.get_device_name(0)} | VRAM: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d57d8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:28:54.488370Z",
     "iopub.status.busy": "2025-12-06T04:28:54.488158Z",
     "iopub.status.idle": "2025-12-06T04:29:40.087138Z",
     "shell.execute_reply": "2025-12-06T04:29:40.086577Z"
    },
    "papermill": {
     "duration": 45.603093,
     "end_time": "2025-12-06T04:29:40.087960",
     "exception": false,
     "start_time": "2025-12-06T04:28:54.484867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 04:29:30 [__init__.py:216] Automatically detected platform cuda.\n",
      "✓ Environment ready. Budget: 280min\n"
     ]
    }
   ],
   "source": [
    "import os, sys, gc, time, random, warnings, re, zlib, statistics, tempfile, subprocess, math\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Optional, List, Dict, Any, Tuple, Set\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "sys.setrecursionlimit(20000)\n",
    "warnings.simplefilter('ignore')\n",
    "os.environ.update({\"TRANSFORMERS_NO_TF\":\"1\",\"TRANSFORMERS_NO_FLAX\":\"1\",\"TRITON_PTXAS_PATH\":\"/usr/local/cuda/bin/ptxas\",\"CUDA_VISIBLE_DEVICES\":\"0\",\"TOKENIZERS_PARALLELISM\":\"false\"})\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "from transformers import set_seed\n",
    "from vllm import LLM, SamplingParams\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "set_seed(SEED)\n",
    "\n",
    "START_TIME = time.time()\n",
    "TOTAL_BUDGET = (4*60+40)*60\n",
    "cutoff_time = START_TIME + TOTAL_BUDGET\n",
    "AAR_LOG = defaultdict(int)\n",
    "PROBLEM_COUNT = 0\n",
    "SOLVED_COUNT = 0\n",
    "\n",
    "print(f\"✓ Environment ready. Budget: {TOTAL_BUDGET//60}min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc1ac38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:29:40.096165Z",
     "iopub.status.busy": "2025-12-06T04:29:40.095779Z",
     "iopub.status.idle": "2025-12-06T04:29:40.105945Z",
     "shell.execute_reply": "2025-12-06T04:29:40.105512Z"
    },
    "papermill": {
     "duration": 0.015115,
     "end_time": "2025-12-06T04:29:40.106536",
     "exception": false,
     "start_time": "2025-12-06T04:29:40.091421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Problem Classifier loaded\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# PROBLEM CLASSIFIER (Unknown Known #1)\n",
    "# Route to optimal strategy ensemble BEFORE inference\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class ProblemType(Enum):\n",
    "    NUMBER_THEORY = \"nt\"      # Modular arithmetic, divisibility, primes\n",
    "    COMBINATORICS = \"comb\"    # Counting, arrangements, probability\n",
    "    GEOMETRY = \"geom\"         # Shapes, coordinates, angles\n",
    "    ALGEBRA = \"alg\"           # Equations, polynomials, sequences\n",
    "    MIXED = \"mixed\"           # Multi-domain or unclear\n",
    "\n",
    "@dataclass\n",
    "class ProblemProfile:\n",
    "    primary_type: ProblemType\n",
    "    secondary_type: Optional[ProblemType]\n",
    "    constraints: List[str]       # Extracted constraints\n",
    "    objective: str               # What to find\n",
    "    enumeration_hint: Optional[int]  # If small space, estimate size\n",
    "    modulo_target: Optional[int] # If answer needs mod N\n",
    "    confidence: float            # Classification confidence\n",
    "\n",
    "def classify_problem(question: str) -> ProblemProfile:\n",
    "    \"\"\"Extract causal grammar and classify problem type.\"\"\"\n",
    "    q = question.lower()\n",
    "    \n",
    "    # Scoring for each type\n",
    "    scores = {t: 0.0 for t in ProblemType}\n",
    "    \n",
    "    # NUMBER THEORY signals\n",
    "    nt_keywords = ['remainder', 'modulo', 'mod ', 'divisible', 'prime', 'gcd', 'lcm', \n",
    "                   'factor', 'digit', 'divides', 'coprime', 'congruent']\n",
    "    for kw in nt_keywords:\n",
    "        if kw in q: scores[ProblemType.NUMBER_THEORY] += 2\n",
    "    \n",
    "    # COMBINATORICS signals\n",
    "    comb_keywords = ['how many', 'count', 'number of ways', 'arrangements', 'permutation',\n",
    "                     'combination', 'choose', 'select', 'probability', 'subset', 'partition']\n",
    "    for kw in comb_keywords:\n",
    "        if kw in q: scores[ProblemType.COMBINATORICS] += 2\n",
    "    \n",
    "    # GEOMETRY signals\n",
    "    geom_keywords = ['triangle', 'circle', 'square', 'rectangle', 'polygon', 'angle',\n",
    "                     'area', 'perimeter', 'diameter', 'radius', 'point', 'line', 'plane',\n",
    "                     'perpendicular', 'parallel', 'tangent', 'inscribed', 'circumscribed']\n",
    "    for kw in geom_keywords:\n",
    "        if kw in q: scores[ProblemType.GEOMETRY] += 2\n",
    "    \n",
    "    # ALGEBRA signals\n",
    "    alg_keywords = ['equation', 'polynomial', 'root', 'solve', 'sequence', 'series',\n",
    "                    'function', 'variable', 'expression', 'inequality', 'sum of', 'product of']\n",
    "    for kw in alg_keywords:\n",
    "        if kw in q: scores[ProblemType.ALGEBRA] += 2\n",
    "    \n",
    "    # Sort by score\n",
    "    sorted_types = sorted(scores.items(), key=lambda x: -x[1])\n",
    "    primary = sorted_types[0][0]\n",
    "    secondary = sorted_types[1][0] if sorted_types[1][1] > 2 else None\n",
    "    \n",
    "    # If no clear winner, mark as MIXED\n",
    "    if sorted_types[0][1] < 2:\n",
    "        primary = ProblemType.MIXED\n",
    "    \n",
    "    # Extract constraints (\"such that\", \"where\", \"given that\")\n",
    "    constraints = []\n",
    "    constraint_patterns = [\n",
    "        r'such that ([^.]+)',\n",
    "        r'where ([^.]+)',\n",
    "        r'given that ([^.]+)',\n",
    "        r'with ([^.]+)',\n",
    "        r'satisf(?:y|ies) ([^.]+)',\n",
    "    ]\n",
    "    for pattern in constraint_patterns:\n",
    "        matches = re.findall(pattern, q)\n",
    "        constraints.extend(matches)\n",
    "    \n",
    "    # Extract objective (\"find\", \"compute\", \"determine\", \"what is\")\n",
    "    objective = \"unknown\"\n",
    "    obj_patterns = [\n",
    "        r'find (?:the )?(.+?)(?:\\.|$)',\n",
    "        r'compute (?:the )?(.+?)(?:\\.|$)',\n",
    "        r'determine (?:the )?(.+?)(?:\\.|$)',\n",
    "        r'what is (?:the )?(.+?)(?:\\?|$)',\n",
    "        r'calculate (?:the )?(.+?)(?:\\.|$)',\n",
    "    ]\n",
    "    for pattern in obj_patterns:\n",
    "        match = re.search(pattern, q)\n",
    "        if match:\n",
    "            objective = match.group(1)[:100]\n",
    "            break\n",
    "    \n",
    "    # Detect enumeration hint (small bounded space)\n",
    "    enum_hint = None\n",
    "    numbers = [int(x) for x in re.findall(r'\\b(\\d+)\\b', question) if 1 < int(x) < 10000]\n",
    "    if numbers:\n",
    "        # Heuristic: if largest number < 1000 and we're counting, space is likely enumerable\n",
    "        if max(numbers) < 1000 and primary == ProblemType.COMBINATORICS:\n",
    "            enum_hint = max(numbers) ** 2  # Rough upper bound\n",
    "    \n",
    "    # Detect modulo target\n",
    "    mod_target = None\n",
    "    mod_match = re.search(r'(?:mod|modulo|remainder when divided by)\\s*(\\d+)', q)\n",
    "    if mod_match:\n",
    "        mod_target = int(mod_match.group(1))\n",
    "    elif 'mod 1000' in q or 'modulo 1000' in q:\n",
    "        mod_target = 1000\n",
    "    \n",
    "    confidence = min(1.0, sorted_types[0][1] / 10.0)\n",
    "    \n",
    "    return ProblemProfile(\n",
    "        primary_type=primary,\n",
    "        secondary_type=secondary,\n",
    "        constraints=constraints,\n",
    "        objective=objective,\n",
    "        enumeration_hint=enum_hint,\n",
    "        modulo_target=mod_target,\n",
    "        confidence=confidence\n",
    "    )\n",
    "\n",
    "print(\"✓ Problem Classifier loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c9c12b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:29:40.113364Z",
     "iopub.status.busy": "2025-12-06T04:29:40.113210Z",
     "iopub.status.idle": "2025-12-06T04:29:40.125242Z",
     "shell.execute_reply": "2025-12-06T04:29:40.124815Z"
    },
    "papermill": {
     "duration": 0.016346,
     "end_time": "2025-12-06T04:29:40.125894",
     "exception": false,
     "start_time": "2025-12-06T04:29:40.109548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config: Sample=True\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "IS_SAMPLE = pd.read_csv(\"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\").shape[0] == 3\n",
    "LLM_MODEL_PATH = '/kaggle/input/gpt-oss-20b/transformers/default/1'\n",
    "\n",
    "# Strategy weights by problem type (tuned ensemble)\n",
    "STRATEGY_WEIGHTS = {\n",
    "    ProblemType.NUMBER_THEORY: {'pot_sympy': 3, 'pot_algorithmic': 2, 'pot_bruteforce': 1},\n",
    "    ProblemType.COMBINATORICS: {'pot_bruteforce': 3, 'pot_algorithmic': 2, 'pot_constraint': 1},\n",
    "    ProblemType.GEOMETRY: {'pot_sympy': 2, 'pot_algorithmic': 2, 'pot_constraint': 2},\n",
    "    ProblemType.ALGEBRA: {'pot_sympy': 3, 'pot_algorithmic': 2, 'pot_verification': 1},\n",
    "    ProblemType.MIXED: {'pot_algorithmic': 2, 'pot_sympy': 2, 'pot_bruteforce': 1, 'pot_constraint': 1},\n",
    "}\n",
    "\n",
    "# Adaptive parameters\n",
    "BASE_SAMPLES = 10\n",
    "CONSENSUS_THRESHOLD = 0.5\n",
    "BASIN_THRESHOLD = 0.02\n",
    "NCD_GROK_THRESHOLD = 0.35\n",
    "PANIC_TIME = 300\n",
    "ORACLE_THRESHOLD = 1_000_000  # If enum space < this, brute force\n",
    "\n",
    "# Temperature annealing\n",
    "TEMP_START = 0.7\n",
    "TEMP_MIN = 0.3\n",
    "TEMP_DECAY = 0.85\n",
    "\n",
    "# Kolmogorov constants\n",
    "KOLMOGOROV_WEIGHT = 0.15  # How much to weight by inverse code length\n",
    "\n",
    "PHI = (1 + 5**0.5) / 2\n",
    "FIBONACCI = [1,1,2,3,5,8,13,21,34,55,89,144,233,377,610,987,1597,2584,4181,6765]\n",
    "\n",
    "print(f\"✓ Config: Sample={IS_SAMPLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b521fcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:29:40.132938Z",
     "iopub.status.busy": "2025-12-06T04:29:40.132796Z",
     "iopub.status.idle": "2025-12-06T04:33:22.996457Z",
     "shell.execute_reply": "2025-12-06T04:33:22.995972Z"
    },
    "papermill": {
     "duration": 222.868106,
     "end_time": "2025-12-06T04:33:22.997200",
     "exception": false,
     "start_time": "2025-12-06T04:29:40.129094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 04:29:40 [utils.py:328] non-default args: {'trust_remote_code': True, 'kv_cache_dtype': 'fp8', 'seed': 42, 'max_model_len': 16384, 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.95, 'max_num_seqs': 32, 'disable_log_stats': True, 'model': '/kaggle/input/gpt-oss-20b/transformers/default/1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 04:30:15 [__init__.py:742] Resolved architecture: GptOssForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 12-06 04:30:15 [config.py:278] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/gpt-oss-20b/transformers/default/1'. Use `repo_type` argument if needed., retrying 1 of 2\n",
      "ERROR 12-06 04:30:17 [config.py:276] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/gpt-oss-20b/transformers/default/1'. Use `repo_type` argument if needed.\n",
      "INFO 12-06 04:30:17 [__init__.py:2764] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 12-06 04:30:17 [__init__.py:1815] Using max model len 16384\n",
      "WARNING 12-06 04:30:21 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'\n",
      "WARNING 12-06 04:30:21 [__init__.py:1217] mxfp4 quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 12-06 04:30:21 [cache.py:174] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.\n",
      "INFO 12-06 04:30:24 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 12-06 04:30:24 [config.py:284] Overriding max cuda graph capture size to 1024 for performance.\n",
      "WARNING 12-06 04:30:26 [__init__.py:2974] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "INFO 12-06 04:30:45 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:30:52 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:30:52 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/kaggle/input/gpt-oss-20b/transformers/default/1', speculative_config=None, tokenizer='/kaggle/input/gpt-oss-20b/transformers/default/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend='openai_gptoss'), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=/kaggle/input/gpt-oss-20b/transformers/default/1, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[1024,1008,992,976,960,944,928,912,896,880,864,848,832,816,800,784,768,752,736,720,704,688,672,656,640,624,608,592,576,560,544,528,512,496,480,464,448,432,416,400,384,368,352,336,320,304,288,272,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":1024,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1206 04:30:57.610701829 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1206 04:30:57.611240795 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W1206 04:30:57.615301118 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:30:57 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m WARNING 12-06 04:30:57 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:30:57 [gpu_model_runner.py:2338] Starting to load model /kaggle/input/gpt-oss-20b/transformers/default/1...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:30:58 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:30:58 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m WARNING 12-06 04:30:58 [mxfp4.py:84] MXFP4 MoE is enabled on Hopper/Blackwell but FlashInfer is not available. This may result in degraded performance. Please `pip install vllm[flashinfer]` for best results.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:30:58 [mxfp4.py:93] Using Marlin backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:33<01:07, 33.58s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [01:04<00:32, 32.28s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [01:32<00:00, 30.08s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [01:32<00:00, 30.81s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:32:31 [default_loader.py:268] Loading weights took 92.48 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m WARNING 12-06 04:32:31 [marlin_utils_fp4.py:196] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:32:32 [gpu_model_runner.py:2392] Model loading took 13.7164 GiB and 93.615974 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:32:44 [backends.py:539] Using cache directory: /root/.cache/vllm/torch_compile_cache/7161b16b9d/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:32:44 [backends.py:550] Dynamo bytecode transform time: 12.19 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:32:48 [backends.py:194] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:33:12 [backends.py:215] Compiling a graph for dynamic shape takes 27.92 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:33:13 [monitor.py:34] torch.compile takes 40.11 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:33:15 [gpu_worker.py:298] Available KV cache memory: 59.99 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:33:15 [kv_cache_utils.py:1028] GPU KV cache size: 2,621,136 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:33:15 [kv_cache_utils.py:1032] Maximum concurrency for 16,384 tokens per request: 159.90x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 83/83 [00:04<00:00, 18.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:33:21 [gpu_model_runner.py:3118] Graph capturing finished in 5 secs, took 0.94 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:33:21 [gpu_worker.py:391] Free memory on device (78.84/79.44 GiB) on startup. Desired GPU memory utilization is (0.95, 75.47 GiB). Actual usage is 13.72 GiB for weight, 1.69 GiB for peak activation, 0.07 GiB for non-torch memory, and 0.94 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=63251194777` to fit into requested memory, or `--kv-cache-memory=66871069184` to fully utilize gpu memory. Current kv cache memory in use is 64417211289 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=188)\u001b[0;0m INFO 12-06 04:33:21 [core.py:218] init engine (profile, create kv cache, warmup model) took 49.09 seconds\n",
      "INFO 12-06 04:33:22 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 12-06 04:33:22 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "✓ Model (FP8 KV)\n"
     ]
    }
   ],
   "source": [
    "# Model Load\n",
    "MAX_NUM_SEQS = 32\n",
    "MAX_MODEL_LEN = 16384\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_num_seqs\": MAX_NUM_SEQS, \"max_model_len\": MAX_MODEL_LEN,\n",
    "    \"trust_remote_code\": True, \"tensor_parallel_size\": 1,\n",
    "    \"gpu_memory_utilization\": 0.95, \"seed\": SEED, \"enable_prefix_caching\": True,\n",
    "}\n",
    "\n",
    "try:\n",
    "    llm = LLM(LLM_MODEL_PATH, kv_cache_dtype=\"fp8\", enforce_eager=False, **model_kwargs)\n",
    "    print(\"✓ Model (FP8 KV)\")\n",
    "except:\n",
    "    llm = LLM(LLM_MODEL_PATH, **model_kwargs)\n",
    "    print(\"✓ Model (standard)\")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4e0d4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:33:23.008515Z",
     "iopub.status.busy": "2025-12-06T04:33:23.008337Z",
     "iopub.status.idle": "2025-12-06T04:33:23.012169Z",
     "shell.execute_reply": "2025-12-06T04:33:23.011745Z"
    },
    "papermill": {
     "duration": 0.010204,
     "end_time": "2025-12-06T04:33:23.012796",
     "exception": false,
     "start_time": "2025-12-06T04:33:23.002592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Stdlib (2351 chars)\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Stdlib\n",
    "STDLIB = '''\n",
    "import sys; sys.setrecursionlimit(20000)\n",
    "import math, numpy as np\n",
    "from itertools import *\n",
    "from collections import *\n",
    "from functools import lru_cache, reduce\n",
    "from fractions import Fraction\n",
    "try:\n",
    "    import sympy\n",
    "    from sympy import *\n",
    "    from sympy import Rational, Integer, symbols, solve, simplify, sqrt, Abs, binomial\n",
    "    from sympy.ntheory import factorint, divisors, totient, mobius, primefactors, isprime\n",
    "    from sympy.ntheory.modular import crt as sympy_crt\n",
    "    from sympy.solvers.diophantine import diophantine\n",
    "    from sympy.geometry import Point, Line, Circle, Triangle, Polygon\n",
    "except: pass\n",
    "\n",
    "def is_prime(n):\n",
    "    if n < 2: return False\n",
    "    if n < 4: return True\n",
    "    if n % 2 == 0: return False\n",
    "    for i in range(3, int(n**0.5) + 1, 2):\n",
    "        if n % i == 0: return False\n",
    "    return True\n",
    "\n",
    "def prime_sieve(n):\n",
    "    if n < 2: return []\n",
    "    sieve = [True] * (n + 1)\n",
    "    sieve[0] = sieve[1] = False\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if sieve[i]:\n",
    "            for j in range(i*i, n + 1, i):\n",
    "                sieve[j] = False\n",
    "    return [i for i, v in enumerate(sieve) if v]\n",
    "\n",
    "def prime_factors(n):\n",
    "    factors = []\n",
    "    d = 2\n",
    "    while d * d <= n:\n",
    "        while n % d == 0: factors.append(d); n //= d\n",
    "        d += 1\n",
    "    if n > 1: factors.append(n)\n",
    "    return factors\n",
    "\n",
    "def gcd(a, b):\n",
    "    while b: a, b = b, a % b\n",
    "    return a\n",
    "\n",
    "def lcm(a, b): return a * b // gcd(a, b)\n",
    "\n",
    "def extended_gcd(a, b):\n",
    "    if a == 0: return b, 0, 1\n",
    "    g, x, y = extended_gcd(b % a, a)\n",
    "    return g, y - (b // a) * x, x\n",
    "\n",
    "def mod_inverse(a, m):\n",
    "    g, x, _ = extended_gcd(a % m, m)\n",
    "    return x % m if g == 1 else None\n",
    "\n",
    "def crt(remainders, moduli):\n",
    "    M = reduce(lambda x, y: x * y, moduli)\n",
    "    x = 0\n",
    "    for r, m in zip(remainders, moduli):\n",
    "        Mi = M // m\n",
    "        inv = mod_inverse(Mi, m)\n",
    "        if inv is None: return None\n",
    "        x += r * Mi * inv\n",
    "    return x % M\n",
    "\n",
    "def shoelace_area(coords):\n",
    "    n = len(coords)\n",
    "    area = sum(coords[i][0] * coords[(i+1)%n][1] - coords[(i+1)%n][0] * coords[i][1] for i in range(n))\n",
    "    return abs(area) / 2\n",
    "\n",
    "def dist(p1, p2): return math.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
    "\n",
    "def C(n, k):\n",
    "    if k < 0 or k > n: return 0\n",
    "    if k == 0 or k == n: return 1\n",
    "    return math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n",
    "\n",
    "def P(n, k): return math.factorial(n) // math.factorial(n - k) if k <= n else 0\n",
    "'''\n",
    "\n",
    "SNOOP = '''\n",
    "_vars = dict(globals())\n",
    "for _v in ['answer', 'result', 'ans', 'res', 'final', 'output', 'solution', 'total', 'count']:\n",
    "    if _v in _vars and _vars[_v] is not None:\n",
    "        try: print(int(_vars[_v])); break\n",
    "        except: pass\n",
    "'''\n",
    "\n",
    "print(f\"✓ Stdlib ({len(STDLIB)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b50f7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:33:23.023773Z",
     "iopub.status.busy": "2025-12-06T04:33:23.023447Z",
     "iopub.status.idle": "2025-12-06T04:33:23.034371Z",
     "shell.execute_reply": "2025-12-06T04:33:23.033952Z"
    },
    "papermill": {
     "duration": 0.017124,
     "end_time": "2025-12-06T04:33:23.035003",
     "exception": false,
     "start_time": "2025-12-06T04:33:23.017879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Kolmogorov selection loaded\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# KOLMOGOROV WEIGHTING (Unknown Known #2)\n",
    "# Shorter code = higher confidence (Occam's Razor operationalized)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "@dataclass\n",
    "class AnswerCandidate:\n",
    "    value: int\n",
    "    code: str\n",
    "    strategy: str\n",
    "    code_length: int\n",
    "    kolmogorov_weight: float\n",
    "\n",
    "def compute_kolmogorov_weight(code: str) -> float:\n",
    "    \"\"\"Inverse complexity weight. Shorter = better.\"\"\"\n",
    "    if not code: return 0.5\n",
    "    # Normalize by typical code length (500 chars)\n",
    "    normalized_len = len(code) / 500.0\n",
    "    # Sigmoid-ish: short code gets weight ~1.5, long code ~0.5\n",
    "    return 1.0 / (0.5 + normalized_len * KOLMOGOROV_WEIGHT)\n",
    "\n",
    "def relative_distance(a: float, b: float) -> float:\n",
    "    if a == b: return 0.0\n",
    "    if a == 0 or b == 0: return 1.0\n",
    "    return abs(a - b) / max(abs(a), abs(b))\n",
    "\n",
    "def normalize_answer(raw: Any) -> Optional[int]:\n",
    "    if raw is None: return None\n",
    "    try:\n",
    "        s = str(raw).replace(',', '').replace(' ', '').strip()\n",
    "        s = re.sub(r'[^0-9.eE+-]', '', s)\n",
    "        if not s: return None\n",
    "        val = float(s)\n",
    "        if val != val or abs(val) == float('inf'): return None\n",
    "        result = int(round(val))\n",
    "        if result < 0: result = abs(result)\n",
    "        return result % 100000\n",
    "    except: return None\n",
    "\n",
    "def benford_proximity(n: int) -> float:\n",
    "    if n <= 0: return 1000\n",
    "    fib_dist = min(abs(n - f) for f in FIBONACCI if f > 0)\n",
    "    phi_dist = min(abs(n - int(PHI ** k)) for k in range(1, 20) if PHI ** k < 100000)\n",
    "    round_score = -50 if n % 100 == 0 else (-20 if n % 10 == 0 else 0)\n",
    "    return min(fib_dist, phi_dist) + round_score\n",
    "\n",
    "def select_answer_with_kolmogorov(candidates: List[AnswerCandidate], \n",
    "                                   mod_target: Optional[int] = None) -> int:\n",
    "    \"\"\"CIC Basin Clustering + Kolmogorov Weighting + Integration Force.\"\"\"\n",
    "    if not candidates: return 0\n",
    "    \n",
    "    # Apply modulo if specified\n",
    "    if mod_target:\n",
    "        for c in candidates:\n",
    "            c.value = c.value % mod_target\n",
    "    \n",
    "    # Group into basins\n",
    "    values = sorted(set(c.value for c in candidates))\n",
    "    basins = defaultdict(list)\n",
    "    \n",
    "    for c in candidates:\n",
    "        # Find basin by proximity\n",
    "        assigned = False\n",
    "        for bval in basins.keys():\n",
    "            if relative_distance(c.value, bval) < BASIN_THRESHOLD:\n",
    "                basins[bval].append(c)\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            basins[c.value].append(c)\n",
    "    \n",
    "    # Score basins\n",
    "    basin_scores = []\n",
    "    for bval, members in basins.items():\n",
    "        # Mass (count)\n",
    "        mass = len(members)\n",
    "        \n",
    "        # Kolmogorov weight (sum of inverse complexities)\n",
    "        k_weight = sum(c.kolmogorov_weight for c in members)\n",
    "        \n",
    "        # Integration force (strategy diversity)\n",
    "        strategies = set(c.strategy for c in members)\n",
    "        integration = 1.5 if len(strategies) > 1 else 1.0\n",
    "        \n",
    "        # Combined score\n",
    "        score = mass * k_weight * integration\n",
    "        \n",
    "        # Median value of basin\n",
    "        median_val = int(statistics.median([c.value for c in members]))\n",
    "        \n",
    "        basin_scores.append((score, median_val, members))\n",
    "    \n",
    "    basin_scores.sort(key=lambda x: -x[0])\n",
    "    \n",
    "    # Tiebreaker: Benford proximity\n",
    "    if len(basin_scores) >= 2 and basin_scores[0][0] < basin_scores[1][0] * 1.1:\n",
    "        top2 = basin_scores[:2]\n",
    "        top2.sort(key=lambda x: benford_proximity(x[1]))\n",
    "        AAR_LOG['benford_tiebreak'] += 1\n",
    "        return max(0, min(top2[0][1], 99999))\n",
    "    \n",
    "    final = basin_scores[0][1]\n",
    "    \n",
    "    # 0/1 sanity check\n",
    "    if final in (0, 1) and len(candidates) > 3:\n",
    "        non_trivial = [c for c in candidates if c.value not in (0, 1)]\n",
    "        if len(non_trivial) >= 2:\n",
    "            return select_answer_with_kolmogorov(non_trivial, mod_target)\n",
    "    \n",
    "    return max(0, min(final, 99999))\n",
    "\n",
    "print(\"✓ Kolmogorov selection loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d879628c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:33:23.045814Z",
     "iopub.status.busy": "2025-12-06T04:33:23.045623Z",
     "iopub.status.idle": "2025-12-06T04:33:23.054845Z",
     "shell.execute_reply": "2025-12-06T04:33:23.054411Z"
    },
    "papermill": {
     "duration": 0.015542,
     "end_time": "2025-12-06T04:33:23.055467",
     "exception": false,
     "start_time": "2025-12-06T04:33:23.039925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Execution engine loaded\n"
     ]
    }
   ],
   "source": [
    "# Code Execution\n",
    "\n",
    "def extract_python_code(text: str) -> Optional[str]:\n",
    "    for pattern in [r'```python\\s*\\n(.*?)```', r'```py\\s*\\n(.*?)```', r'```\\s*\\n(.*?)```']:\n",
    "        matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if matches: return matches[-1].strip()\n",
    "    return None\n",
    "\n",
    "def calc_timeout(code: str) -> int:\n",
    "    base = 10\n",
    "    loops = len(re.findall(r'\\bfor\\b|\\bwhile\\b', code))\n",
    "    if loops >= 2: base += 10\n",
    "    if any(kw in code for kw in ['itertools', 'permutations', 'combinations', 'product']): base += 10\n",
    "    if '@lru_cache' in code: base += 5\n",
    "    return min(base, 30)\n",
    "\n",
    "def self_heal(code: str, stderr: str) -> Optional[str]:\n",
    "    healed = code\n",
    "    if 'NameError' in stderr:\n",
    "        missing = re.findall(r\"name '(\\w+)' is not defined\", stderr)\n",
    "        for name in missing:\n",
    "            if name in ['np', 'numpy']: healed = 'import numpy as np\\n' + healed\n",
    "            elif name in ['math', 'sqrt', 'gcd']: healed = 'import math\\nfrom math import *\\n' + healed\n",
    "            elif name in ['sympy', 'Symbol', 'solve']: healed = 'from sympy import *\\n' + healed\n",
    "            elif name in ['Counter', 'defaultdict']: healed = 'from collections import *\\n' + healed\n",
    "            elif name in ['combinations', 'permutations']: healed = 'from itertools import *\\n' + healed\n",
    "    if 'RecursionError' in stderr: healed = 'import sys; sys.setrecursionlimit(50000)\\n' + healed\n",
    "    return healed if healed != code else None\n",
    "\n",
    "def execute_code(code: str, retries: int = 2) -> Tuple[Optional[int], str]:\n",
    "    if not code: return None, \"\"\n",
    "    has_print = 'print(' in code\n",
    "    full = STDLIB + \"\\n\" + code + (\"\" if has_print else \"\\n\" + SNOOP)\n",
    "    timeout = calc_timeout(code)\n",
    "    \n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "                f.write(full); f.flush()\n",
    "                result = subprocess.run(['python', f.name], capture_output=True, text=True, timeout=timeout)\n",
    "                os.unlink(f.name)\n",
    "                \n",
    "                if result.returncode == 0 and result.stdout.strip():\n",
    "                    numbers = re.findall(r'-?\\d+\\.?\\d*', result.stdout)\n",
    "                    if numbers: return normalize_answer(numbers[-1]), code\n",
    "                \n",
    "                if result.stderr and attempt < retries:\n",
    "                    healed = self_heal(code, result.stderr)\n",
    "                    if healed:\n",
    "                        AAR_LOG['self_heal'] += 1\n",
    "                        full = STDLIB + \"\\n\" + healed + (\"\" if has_print else \"\\n\" + SNOOP)\n",
    "                        code = healed\n",
    "                        continue\n",
    "                if 'Error' in str(result.stderr): AAR_LOG['runtime_error'] += 1\n",
    "        except subprocess.TimeoutExpired: AAR_LOG['timeout'] += 1\n",
    "        except: AAR_LOG['exec_exception'] += 1\n",
    "        break\n",
    "    return None, code\n",
    "\n",
    "def extract_boxed(text: str) -> Optional[int]:\n",
    "    for pattern in [r'\\\\boxed\\{([^}]+)\\}', r'boxed\\{([^}]+)\\}', r'[Aa]nswer\\s*(?:is|=|:)\\s*([0-9]+)']:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        if matches: return normalize_answer(matches[-1])\n",
    "    return None\n",
    "\n",
    "print(\"✓ Execution engine loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b733ebc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:33:23.066554Z",
     "iopub.status.busy": "2025-12-06T04:33:23.066396Z",
     "iopub.status.idle": "2025-12-06T04:33:23.071853Z",
     "shell.execute_reply": "2025-12-06T04:33:23.071425Z"
    },
    "papermill": {
     "duration": 0.011742,
     "end_time": "2025-12-06T04:33:23.072445",
     "exception": false,
     "start_time": "2025-12-06T04:33:23.060703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prompts loaded\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# TYPE-SPECIFIC PROMPTS (Causal Grammar Injection)\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "STRATEGY_PROMPTS = {\n",
    "    'pot_algorithmic': (\n",
    "        \"You are an algorithmic mathematician. Write self-contained Python. \"\n",
    "        \"Start with imports. Use assert to validate. Handle edge cases. \"\n",
    "        \"Use int() not float. Use pow(a,b,m) for modular exp. Print ONE integer.\"\n",
    "    ),\n",
    "    'pot_sympy': (\n",
    "        \"You are a symbolic computation expert. Use sympy for EXACT arithmetic. \"\n",
    "        \"Use Rational(a,b) NOT a/b. Never use floats. Verify with simplify(). \"\n",
    "        \"Print ONE integer.\"\n",
    "    ),\n",
    "    'pot_bruteforce': (\n",
    "        \"You are a computational enumerator. Systematically enumerate. \"\n",
    "        \"Budget: 10^7 iterations max. Use itertools, memoization, early termination. \"\n",
    "        \"Assert bounds before loops. Print ONE integer.\"\n",
    "    ),\n",
    "    'pot_constraint': (\n",
    "        \"You are a constraint satisfaction expert. Model as CSP. \"\n",
    "        \"List ALL constraints first. Propagate. Backtrack on violation. Print ONE integer.\"\n",
    "    ),\n",
    "    'pot_verification': (\n",
    "        \"You are a verification mathematician. Solve TWO different ways. \"\n",
    "        \"Compare. Only print when BOTH agree. Print ONE integer.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "TYPE_HINTS = {\n",
    "    ProblemType.NUMBER_THEORY: \"\\n\\nHINT: This is a NUMBER THEORY problem. Consider: modular arithmetic, divisibility, prime factorization, Chinese Remainder Theorem, Fermat's Little Theorem.\",\n",
    "    ProblemType.COMBINATORICS: \"\\n\\nHINT: This is a COMBINATORICS problem. Consider: counting principles, binomial coefficients, inclusion-exclusion, generating functions, recursion with memoization.\",\n",
    "    ProblemType.GEOMETRY: \"\\n\\nHINT: This is a GEOMETRY problem. Consider: coordinate geometry, distance formula, shoelace formula for area, similar triangles, trigonometry.\",\n",
    "    ProblemType.ALGEBRA: \"\\n\\nHINT: This is an ALGEBRA problem. Consider: polynomial manipulation, Vieta's formulas, sequences and series, solving systems of equations.\",\n",
    "    ProblemType.MIXED: \"\",\n",
    "}\n",
    "\n",
    "def create_prompt(question: str, strategy: str, profile: ProblemProfile, \n",
    "                  prev_wrong: Optional[int] = None, temp_idx: int = 0) -> Tuple[List[Dict], str]:\n",
    "    \"\"\"Create prompt with causal grammar injection.\"\"\"\n",
    "    \n",
    "    system = STRATEGY_PROMPTS.get(strategy, STRATEGY_PROMPTS['pot_algorithmic'])\n",
    "    system += \" Enclose code in ```python ... ```.\"\n",
    "    \n",
    "    # Inject type hint\n",
    "    type_hint = TYPE_HINTS.get(profile.primary_type, \"\")\n",
    "    \n",
    "    # Inject extracted constraints\n",
    "    constraint_injection = \"\"\n",
    "    if profile.constraints:\n",
    "        constraint_injection = f\"\\n\\nEXTRACTED CONSTRAINTS: {'; '.join(profile.constraints[:3])}\"\n",
    "    \n",
    "    # Inject modulo target\n",
    "    mod_injection = \"\"\n",
    "    if profile.modulo_target:\n",
    "        mod_injection = f\"\\n\\nIMPORTANT: Final answer must be mod {profile.modulo_target}.\"\n",
    "    \n",
    "    # Ratcheting injection\n",
    "    ratchet = \"\"\n",
    "    if prev_wrong is not None:\n",
    "        ratchet = f\"\\n\\nWARNING: A previous attempt got {prev_wrong}, which was WRONG. Use a different approach.\"\n",
    "    \n",
    "    user = f\"Problem: {question}{type_hint}{constraint_injection}{mod_injection}{ratchet}\\n\\nWrite Python. Print ONLY the integer answer.\"\n",
    "    \n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}], strategy\n",
    "\n",
    "def create_cot_prompt(question: str, profile: ProblemProfile) -> Tuple[List[Dict], str]:\n",
    "    system = \"You are a mathematical oracle. Think step-by-step. ATTACK your solution before finalizing. Only output \\\\boxed{answer} at 95%+ confidence.\"\n",
    "    type_hint = TYPE_HINTS.get(profile.primary_type, \"\")\n",
    "    user = f\"{question}{type_hint}\\n\\nSolve completely. Final integer in \\\\boxed{{}}.\"\n",
    "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}], \"cot\"\n",
    "\n",
    "print(\"✓ Prompts loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caa842f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:33:23.083479Z",
     "iopub.status.busy": "2025-12-06T04:33:23.083325Z",
     "iopub.status.idle": "2025-12-06T04:33:23.089404Z",
     "shell.execute_reply": "2025-12-06T04:33:23.088992Z"
    },
    "papermill": {
     "duration": 0.012391,
     "end_time": "2025-12-06T04:33:23.090028",
     "exception": false,
     "start_time": "2025-12-06T04:33:23.077637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Batch processing loaded\n"
     ]
    }
   ],
   "source": [
    "# Batch Generation with Adaptive Temperature\n",
    "\n",
    "POT_STOP = [\"```output\", \"```\\nOutput\", \"# Output\", \"\\n\\n\\n\"]\n",
    "COT_STOP = [\"</think>\", \"\\n\\nQuestion:\"]\n",
    "\n",
    "def batch_generate(messages: List[List[Dict]], mode: str = 'pot', temp: float = 0.6) -> List[str]:\n",
    "    prompts = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in messages]\n",
    "    params = SamplingParams(\n",
    "        temperature=temp, top_p=0.9, min_p=0.05, max_tokens=8192,\n",
    "        skip_special_tokens=True, stop=POT_STOP if mode == 'pot' else COT_STOP\n",
    "    )\n",
    "    outputs = llm.generate(prompts, sampling_params=params)\n",
    "    return [o.outputs[0].text for o in outputs]\n",
    "\n",
    "def process_pot(outputs: List[str], strategies: List[str]) -> List[AnswerCandidate]:\n",
    "    candidates = []\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = {}\n",
    "        for i, out in enumerate(outputs):\n",
    "            code = extract_python_code(out)\n",
    "            if code:\n",
    "                futures[executor.submit(execute_code, code)] = (i, code, strategies[i] if i < len(strategies) else 'pot')\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            i, code, strat = futures[future]\n",
    "            try:\n",
    "                ans, final_code = future.result()\n",
    "                if ans is not None:\n",
    "                    candidates.append(AnswerCandidate(\n",
    "                        value=ans,\n",
    "                        code=final_code,\n",
    "                        strategy=strat,\n",
    "                        code_length=len(final_code),\n",
    "                        kolmogorov_weight=compute_kolmogorov_weight(final_code)\n",
    "                    ))\n",
    "            except: pass\n",
    "    return candidates\n",
    "\n",
    "def process_cot(outputs: List[str]) -> List[AnswerCandidate]:\n",
    "    candidates = []\n",
    "    for out in outputs:\n",
    "        ans = extract_boxed(out)\n",
    "        if ans is not None:\n",
    "            candidates.append(AnswerCandidate(\n",
    "                value=ans, code=\"\", strategy=\"cot\",\n",
    "                code_length=0, kolmogorov_weight=1.0  # No code = neutral weight\n",
    "            ))\n",
    "    return candidates\n",
    "\n",
    "print(\"✓ Batch processing loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "820b476a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:33:23.101048Z",
     "iopub.status.busy": "2025-12-06T04:33:23.100897Z",
     "iopub.status.idle": "2025-12-06T04:33:23.105054Z",
     "shell.execute_reply": "2025-12-06T04:33:23.104618Z"
    },
    "papermill": {
     "duration": 0.010368,
     "end_time": "2025-12-06T04:33:23.105694",
     "exception": false,
     "start_time": "2025-12-06T04:33:23.095326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Oracle enumeration loaded\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# ORACLE ENUMERATION (Unknown Known #3)\n",
    "# If space is small enough, just brute force in code\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def try_oracle_enumeration(question: str, profile: ProblemProfile) -> Optional[int]:\n",
    "    \"\"\"For small enumeration spaces, just brute force.\"\"\"\n",
    "    if profile.enumeration_hint is None or profile.enumeration_hint > ORACLE_THRESHOLD:\n",
    "        return None\n",
    "    \n",
    "    AAR_LOG['oracle_attempt'] += 1\n",
    "    \n",
    "    # Generate a brute-force enumeration prompt\n",
    "    oracle_prompt = f\"\"\"Problem: {question}\n",
    "\n",
    "The solution space appears small (estimated {profile.enumeration_hint} possibilities).\n",
    "Write Python that EXHAUSTIVELY ENUMERATES all possibilities and checks each one.\n",
    "Do NOT try to be clever. Just iterate through ALL cases.\n",
    "Print the answer.\"\"\"\n",
    "    \n",
    "    system = \"You are a brute-force enumerator. Write code that checks EVERY possibility. No optimization needed - just exhaustive search. Enclose in ```python ... ```.\"\n",
    "    msgs = [[{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": oracle_prompt}]]\n",
    "    \n",
    "    outputs = batch_generate(msgs, 'pot', temp=0.3)  # Low temp for deterministic\n",
    "    candidates = process_pot(outputs, ['oracle'])\n",
    "    \n",
    "    if candidates:\n",
    "        AAR_LOG['oracle_success'] += 1\n",
    "        return candidates[0].value\n",
    "    return None\n",
    "\n",
    "print(\"✓ Oracle enumeration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf7d4d5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:33:23.116894Z",
     "iopub.status.busy": "2025-12-06T04:33:23.116747Z",
     "iopub.status.idle": "2025-12-06T04:33:23.120569Z",
     "shell.execute_reply": "2025-12-06T04:33:23.120157Z"
    },
    "papermill": {
     "duration": 0.01015,
     "end_time": "2025-12-06T04:33:23.121190",
     "exception": false,
     "start_time": "2025-12-06T04:33:23.111040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Panic mode loaded\n"
     ]
    }
   ],
   "source": [
    "# Panic Mode\n",
    "\n",
    "def panic_guess(question: str, profile: ProblemProfile) -> int:\n",
    "    AAR_LOG['panic'] += 1\n",
    "    q = question.lower()\n",
    "    numbers = [int(x) for x in re.findall(r'\\b\\d+\\b', question) if 0 < int(x) < 100000]\n",
    "    \n",
    "    if profile.modulo_target:\n",
    "        if numbers: return numbers[0] % profile.modulo_target\n",
    "        return 0\n",
    "    \n",
    "    if 'how many' in q:\n",
    "        return min(numbers) if numbers else 1\n",
    "    if 'sum' in q or 'total' in q:\n",
    "        small = [n for n in numbers if n < 1000]\n",
    "        return sum(small) % 100000 if small else 0\n",
    "    \n",
    "    return numbers[0] % 100000 if numbers else 0\n",
    "\n",
    "print(\"✓ Panic mode loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f79edf2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:33:23.132793Z",
     "iopub.status.busy": "2025-12-06T04:33:23.132378Z",
     "iopub.status.idle": "2025-12-06T04:33:23.142072Z",
     "shell.execute_reply": "2025-12-06T04:33:23.141638Z"
    },
    "papermill": {
     "duration": 0.016131,
     "end_time": "2025-12-06T04:33:23.142723",
     "exception": false,
     "start_time": "2025-12-06T04:33:23.126592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Solver loaded\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# MAIN SOLVER with Time Budget Decay + Adaptive Temperature\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def predict_for_question(question: str) -> int:\n",
    "    global PROBLEM_COUNT, SOLVED_COUNT\n",
    "    PROBLEM_COUNT += 1\n",
    "    \n",
    "    time_remaining = cutoff_time - time.time()\n",
    "    if time_remaining < PANIC_TIME:\n",
    "        profile = classify_problem(question)\n",
    "        print(f\"⚠️ PANIC ({time_remaining:.0f}s)\")\n",
    "        return panic_guess(question, profile)\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════════════════\n",
    "    # PHASE 0: CLASSIFY PROBLEM\n",
    "    # ═══════════════════════════════════════════════════════════════════════════\n",
    "    profile = classify_problem(question)\n",
    "    print(f\"Q: {question[:60]}...\")\n",
    "    print(f\"  Type: {profile.primary_type.value} (conf={profile.confidence:.2f}) | Mod: {profile.modulo_target} | Enum: {profile.enumeration_hint}\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════════════════\n",
    "    # PHASE 0.5: TRY ORACLE ENUMERATION\n",
    "    # ═══════════════════════════════════════════════════════════════════════════\n",
    "    if profile.enumeration_hint and profile.enumeration_hint < ORACLE_THRESHOLD:\n",
    "        oracle_ans = try_oracle_enumeration(question, profile)\n",
    "        if oracle_ans is not None:\n",
    "            print(f\"  ✓ Oracle: {oracle_ans}\")\n",
    "            SOLVED_COUNT += 1\n",
    "            return oracle_ans\n",
    "    \n",
    "    # Get strategy weights for this problem type\n",
    "    weights = STRATEGY_WEIGHTS.get(profile.primary_type, STRATEGY_WEIGHTS[ProblemType.MIXED])\n",
    "    \n",
    "    # Build weighted strategy list\n",
    "    strategies = []\n",
    "    for strat, weight in weights.items():\n",
    "        strategies.extend([strat] * weight)\n",
    "    \n",
    "    all_candidates: List[AnswerCandidate] = []\n",
    "    current_temp = TEMP_START\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════════════════\n",
    "    # PHASE 1: INITIAL SCOUTING\n",
    "    # ═══════════════════════════════════════════════════════════════════════════\n",
    "    n_phase1 = min(len(strategies), 6)\n",
    "    print(f\"  Phase 1: PoT x{n_phase1} (temp={current_temp:.2f})\")\n",
    "    \n",
    "    msgs_p1, strats_p1 = [], []\n",
    "    for i in range(n_phase1):\n",
    "        strat = strategies[i % len(strategies)]\n",
    "        m, s = create_prompt(question, strat, profile, temp_idx=i)\n",
    "        msgs_p1.append(m)\n",
    "        strats_p1.append(s)\n",
    "    \n",
    "    outputs_p1 = batch_generate(msgs_p1, 'pot', temp=current_temp)\n",
    "    candidates_p1 = process_pot(outputs_p1, strats_p1)\n",
    "    all_candidates.extend(candidates_p1)\n",
    "    \n",
    "    answers_p1 = [c.value for c in candidates_p1]\n",
    "    print(f\"    → {len(answers_p1)} answers: {answers_p1}\")\n",
    "    \n",
    "    # EARLY STOP CHECK\n",
    "    if len(answers_p1) >= 2:\n",
    "        counts = Counter(answers_p1)\n",
    "        best, count = counts.most_common(1)[0]\n",
    "        if count / len(answers_p1) >= CONSENSUS_THRESHOLD:\n",
    "            print(f\"  ✓ Consensus: {best}\")\n",
    "            SOLVED_COUNT += 1\n",
    "            return int(best)\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════════════════\n",
    "    # PHASE 2: ADAPTIVE EXPANSION (Temperature Annealing)\n",
    "    # ═══════════════════════════════════════════════════════════════════════════\n",
    "    if time.time() > cutoff_time - 120:\n",
    "        if all_candidates:\n",
    "            return select_answer_with_kolmogorov(all_candidates, profile.modulo_target)\n",
    "        return panic_guess(question, profile)\n",
    "    \n",
    "    # Anneal temperature\n",
    "    current_temp = max(TEMP_MIN, current_temp * TEMP_DECAY)\n",
    "    \n",
    "    # Get most common wrong answer for ratcheting\n",
    "    prev_wrong = None\n",
    "    if answers_p1:\n",
    "        counts = Counter(answers_p1)\n",
    "        if len(counts) > 1:\n",
    "            prev_wrong = counts.most_common(1)[0][0]\n",
    "    \n",
    "    if len(candidates_p1) > 0:\n",
    "        # PoT working → expand with different strategies\n",
    "        n_phase2 = 4\n",
    "        print(f\"  Phase 2: PoT expand x{n_phase2} (temp={current_temp:.2f})\")\n",
    "        \n",
    "        msgs_p2, strats_p2 = [], []\n",
    "        for i in range(n_phase2):\n",
    "            strat = strategies[(i + n_phase1) % len(strategies)]\n",
    "            m, s = create_prompt(question, strat, profile, prev_wrong=prev_wrong, temp_idx=i+10)\n",
    "            msgs_p2.append(m)\n",
    "            strats_p2.append(s)\n",
    "        \n",
    "        outputs_p2 = batch_generate(msgs_p2, 'pot', temp=current_temp)\n",
    "        candidates_p2 = process_pot(outputs_p2, strats_p2)\n",
    "        all_candidates.extend(candidates_p2)\n",
    "        print(f\"    → {len(candidates_p2)} answers: {[c.value for c in candidates_p2]}\")\n",
    "    else:\n",
    "        # PoT failed → CoT fallback\n",
    "        print(f\"  Phase 2: CoT fallback x4\")\n",
    "        AAR_LOG['cot_fallback'] += 1\n",
    "        \n",
    "        msgs_p2 = [create_cot_prompt(question, profile)[0] for _ in range(4)]\n",
    "        outputs_p2 = batch_generate(msgs_p2, 'cot', temp=current_temp)\n",
    "        candidates_p2 = process_cot(outputs_p2)\n",
    "        all_candidates.extend(candidates_p2)\n",
    "        print(f\"    → {len(candidates_p2)} answers: {[c.value for c in candidates_p2]}\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════════════════\n",
    "    # FINAL: KOLMOGOROV-WEIGHTED BASIN SELECTION\n",
    "    # ═══════════════════════════════════════════════════════════════════════════\n",
    "    if not all_candidates:\n",
    "        print(\"  ✗ No answers\")\n",
    "        AAR_LOG['no_answers'] += 1\n",
    "        return panic_guess(question, profile)\n",
    "    \n",
    "    final = select_answer_with_kolmogorov(all_candidates, profile.modulo_target)\n",
    "    \n",
    "    # Log distribution\n",
    "    all_values = [c.value for c in all_candidates]\n",
    "    print(f\"  ★ Final: {final} | {Counter(all_values)}\")\n",
    "    \n",
    "    SOLVED_COUNT += 1\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return final\n",
    "\n",
    "print(\"✓ Solver loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aea5fbcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:33:23.154282Z",
     "iopub.status.busy": "2025-12-06T04:33:23.154111Z",
     "iopub.status.idle": "2025-12-06T04:33:23.157932Z",
     "shell.execute_reply": "2025-12-06T04:33:23.157494Z"
    },
    "papermill": {
     "duration": 0.010425,
     "end_time": "2025-12-06T04:33:23.158545",
     "exception": false,
     "start_time": "2025-12-06T04:33:23.148120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Interface loaded\n"
     ]
    }
   ],
   "source": [
    "# Kaggle Interface\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame, answer: Optional[pl.DataFrame] = None) -> pl.DataFrame:\n",
    "    id_val, question_str = id_.item(0), question.item(0)\n",
    "    time_left = (cutoff_time - time.time()) / 60\n",
    "    print(f\"\\n{'='*60}\\nID: {id_val} | Time: {time_left:.1f}m | Solved: {SOLVED_COUNT}/{PROBLEM_COUNT}\")\n",
    "    \n",
    "    prediction = predict_for_question(question_str)\n",
    "    \n",
    "    print(f\"ANSWER: {prediction}\")\n",
    "    print(f\"AAR: {dict(AAR_LOG)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return pl.DataFrame({'id': id_val, 'answer': prediction})\n",
    "\n",
    "print(\"✓ Interface loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f1e5cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T04:33:23.170053Z",
     "iopub.status.busy": "2025-12-06T04:33:23.169903Z",
     "iopub.status.idle": "2025-12-06T04:33:44.410708Z",
     "shell.execute_reply": "2025-12-06T04:33:44.410134Z"
    },
    "papermill": {
     "duration": 21.247532,
     "end_time": "2025-12-06T04:33:44.411580",
     "exception": false,
     "start_time": "2025-12-06T04:33:23.164048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Local test\n",
      "\n",
      "============================================================\n",
      "ID: 111bbb | Time: 276.3m | Solved: 0/0\n",
      "Q: What is $0\\times10$?...\n",
      "  Type: mixed (conf=0.00) | Mod: None | Enum: None\n",
      "  Phase 1: PoT x6 (temp=0.70)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1a85453a0a44d482dc7b7a9528498f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b0388f3dd3425a8fb706ed4bb45d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    → 6 answers: [0, 0, 0, 0, 0, 0]\n",
      "  ✓ Consensus: 0\n",
      "ANSWER: 0\n",
      "AAR: {}\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ID: 000aaa | Time: 276.2m | Solved: 1/1\n",
      "Q: What is $1-1$?...\n",
      "  Type: mixed (conf=0.00) | Mod: None | Enum: None\n",
      "  Phase 1: PoT x6 (temp=0.70)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8416ff254049728272b8c31a3590f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7c2bd9180649c9b2dc069ab38181e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    → 5 answers: [0, 0, 0, 0, 0]\n",
      "  ✓ Consensus: 0\n",
      "ANSWER: 0\n",
      "AAR: {}\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ID: 222ccc | Time: 276.1m | Solved: 2/2\n",
      "Q: Solve $4+x=4$ for $x$....\n",
      "  Type: alg (conf=0.20) | Mod: None | Enum: None\n",
      "  Phase 1: PoT x6 (temp=0.70)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adfa0c3f464466580338ab302d82c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0c1f4fcf8143c0b45a16fe2268ae4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    → 4 answers: [0, 0, 0, 0]\n",
      "  ✓ Consensus: 0\n",
      "ANSWER: 0\n",
      "AAR: {}\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FINAL AAR:\n",
      "Solved: 3/3\n"
     ]
    }
   ],
   "source": [
    "# Start Server\n",
    "\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    print(\"🚀 Competition mode\")\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    print(\"🧪 Local test\")\n",
    "    inference_server.run_local_gateway(('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',))\n",
    "    print(f\"\\n{'='*60}\\nFINAL AAR:\")\n",
    "    for k, v in sorted(AAR_LOG.items()): print(f\"  {k}: {v}\")\n",
    "    print(f\"Solved: {SOLVED_COUNT}/{PROBLEM_COUNT}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "sourceId": 263213121,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 422375,
     "modelInstanceId": 404469,
     "sourceId": 510361,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 422384,
     "modelInstanceId": 404485,
     "sourceId": 510391,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 334.556095,
   "end_time": "2025-12-06T04:33:47.034260",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-06T04:28:12.478165",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "093216819c0a46478ba056b8d1595e3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "099883d3c79242fe839b1d7637d2a590": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0b584e94b2094b518c0a55612abf6e7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0b6a57703847409c8c8cb45e26eb5ddc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b36bac7d69534443a066c063dbd41e9d",
       "max": 6.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_82e2dbdb0387469fa1665f081e9ad9e1",
       "tabbable": null,
       "tooltip": null,
       "value": 6.0
      }
     },
     "0c2368229df0482e8b502ae038d42c50": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "122968bdfa804b65954b2b8b635f2b02": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "14383779015d489193b6a84c77d7b1f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "14d6738adba645f6a2824f3f2c4a8f8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "198ee2f7c10f4865bfc4e6056113966c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "19ad1a9f48a54606aa3248efb41a0f18": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1af92f732c244ba2ae0990a689d115f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1c2a699713e648bbae21bfe769723aff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1cada57040ca40a4a5e0d9ea59920fb0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_dadc8b2f82c54c91b1fd947d5ecbd0b0",
       "max": 6.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_40b33059186e46908fecdf700eba0f54",
       "tabbable": null,
       "tooltip": null,
       "value": 6.0
      }
     },
     "1f3e696153e34602946f4b522462c389": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2534609ba2094fb18971f632fff205ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2c41a7205bc54a5384809659bb4eec02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2cbebb37b6f0471c8df740c9f007d2c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "36902521e0264165b78459e05eb3fe5b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40b33059186e46908fecdf700eba0f54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "43b0388f3dd3425a8fb706ed4bb45d55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_794e7602f3b5484c901cdd731594cca4",
        "IPY_MODEL_db9c9261072f4aa7a6afb78bb8998599",
        "IPY_MODEL_a97eb21ac45245f88f7459f459420fcd"
       ],
       "layout": "IPY_MODEL_8ae511ebd9bd4cf0b025fd3811bc2f29",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4adfa0c3f464466580338ab302d82c59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f6b4705541854c519f2605dfc8ed6016",
        "IPY_MODEL_550e1f54345648a0a6e044ac69acfd81",
        "IPY_MODEL_4d3671ca1f864647ab47b7d413a3acfb"
       ],
       "layout": "IPY_MODEL_0c2368229df0482e8b502ae038d42c50",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4c8416ff254049728272b8c31a3590f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bafb1246b8824b8d8628d13332c6a13d",
        "IPY_MODEL_7a8ddf49a0894957a5e4d93de2bf991d",
        "IPY_MODEL_ae1e51297d29469293a327259f798d4d"
       ],
       "layout": "IPY_MODEL_1f3e696153e34602946f4b522462c389",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4d3671ca1f864647ab47b7d413a3acfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fcb59066f482488eaddf821e89a345d9",
       "placeholder": "​",
       "style": "IPY_MODEL_0b584e94b2094b518c0a55612abf6e7c",
       "tabbable": null,
       "tooltip": null,
       "value": " 6/6 [00:00&lt;00:00, 598.56it/s]"
      }
     },
     "550e1f54345648a0a6e044ac69acfd81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_de3c2b29a7f44a279a95fb9af05f8f00",
       "max": 6.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7e7a6d79079c43c2bf82156dc27fad1d",
       "tabbable": null,
       "tooltip": null,
       "value": 6.0
      }
     },
     "574b7dec8867414e90d841182beb9873": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6cb5bbd41b76444eb5a21398c8aa578a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "71213d5c71814227bbc90b9544d7ab87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "794e7602f3b5484c901cdd731594cca4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e75202ed49b34805af60c52f0ff7223d",
       "placeholder": "​",
       "style": "IPY_MODEL_a42c6f4813554dabaaf85dc88c46b638",
       "tabbable": null,
       "tooltip": null,
       "value": "Processed prompts: 100%"
      }
     },
     "7a8ddf49a0894957a5e4d93de2bf991d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2534609ba2094fb18971f632fff205ff",
       "max": 6.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_71213d5c71814227bbc90b9544d7ab87",
       "tabbable": null,
       "tooltip": null,
       "value": 6.0
      }
     },
     "7e7a6d79079c43c2bf82156dc27fad1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "82df6b133d3249779024d84cd1c7e9d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "82e2dbdb0387469fa1665f081e9ad9e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "883670013dbb463d8cbc3e09c7c69509": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_36902521e0264165b78459e05eb3fe5b",
       "placeholder": "​",
       "style": "IPY_MODEL_1c2a699713e648bbae21bfe769723aff",
       "tabbable": null,
       "tooltip": null,
       "value": "Processed prompts: 100%"
      }
     },
     "8ae511ebd9bd4cf0b025fd3811bc2f29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "8c130cf236eb499ca7011c83d1307053": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_099883d3c79242fe839b1d7637d2a590",
       "placeholder": "​",
       "style": "IPY_MODEL_093216819c0a46478ba056b8d1595e3a",
       "tabbable": null,
       "tooltip": null,
       "value": " 6/6 [00:05&lt;00:00,  1.03it/s, est. speed input: 210.66 toks/s, output: 423.58 toks/s]"
      }
     },
     "9171422c038f47bb9057871e0ecc3244": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a8bd472e1acd4db1b5a215eb71c8124c",
       "max": 6.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9c1700cf8a9a43a4995615515e309c6c",
       "tabbable": null,
       "tooltip": null,
       "value": 6.0
      }
     },
     "9c1700cf8a9a43a4995615515e309c6c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a42c6f4813554dabaaf85dc88c46b638": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a4cc3401e43b4af6a5944bb560e68e4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a58449209a3147e4a67a1116346ce35e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "a8bd472e1acd4db1b5a215eb71c8124c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a97eb21ac45245f88f7459f459420fcd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b138dd6fc3f44440bfb68b1ed8ba6b9d",
       "placeholder": "​",
       "style": "IPY_MODEL_19ad1a9f48a54606aa3248efb41a0f18",
       "tabbable": null,
       "tooltip": null,
       "value": " 6/6 [00:03&lt;00:00,  1.82it/s, est. speed input: 302.84 toks/s, output: 473.15 toks/s]"
      }
     },
     "abf316f87a15420181a23ab718f82228": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ae1e51297d29469293a327259f798d4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_abf316f87a15420181a23ab718f82228",
       "placeholder": "​",
       "style": "IPY_MODEL_ba05e6fa28d34f2eb3e118106357e01d",
       "tabbable": null,
       "tooltip": null,
       "value": " 6/6 [00:00&lt;00:00, 713.03it/s]"
      }
     },
     "af7c2bd9180649c9b2dc069ab38181e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_df4256531f0343588d0925979a32397c",
        "IPY_MODEL_0b6a57703847409c8c8cb45e26eb5ddc",
        "IPY_MODEL_c35aed8384e44bd9b52625a7b88d8cb7"
       ],
       "layout": "IPY_MODEL_a58449209a3147e4a67a1116346ce35e",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b138dd6fc3f44440bfb68b1ed8ba6b9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b36bac7d69534443a066c063dbd41e9d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b58150536faa4bd8a3a201d1b0ee0ded": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_14383779015d489193b6a84c77d7b1f2",
       "placeholder": "​",
       "style": "IPY_MODEL_2c41a7205bc54a5384809659bb4eec02",
       "tabbable": null,
       "tooltip": null,
       "value": "Adding requests: 100%"
      }
     },
     "ba05e6fa28d34f2eb3e118106357e01d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bafb1246b8824b8d8628d13332c6a13d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1af92f732c244ba2ae0990a689d115f2",
       "placeholder": "​",
       "style": "IPY_MODEL_f80e870585cf4c97a024a7a776ca3510",
       "tabbable": null,
       "tooltip": null,
       "value": "Adding requests: 100%"
      }
     },
     "bc97d6d6c34347a5860eeac3871d1b25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_82df6b133d3249779024d84cd1c7e9d6",
       "placeholder": "​",
       "style": "IPY_MODEL_a4cc3401e43b4af6a5944bb560e68e4f",
       "tabbable": null,
       "tooltip": null,
       "value": " 6/6 [00:00&lt;00:00, 650.92it/s]"
      }
     },
     "c35aed8384e44bd9b52625a7b88d8cb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_122968bdfa804b65954b2b8b635f2b02",
       "placeholder": "​",
       "style": "IPY_MODEL_14d6738adba645f6a2824f3f2c4a8f8e",
       "tabbable": null,
       "tooltip": null,
       "value": " 6/6 [00:03&lt;00:00,  2.11it/s, est. speed input: 272.31 toks/s, output: 585.28 toks/s]"
      }
     },
     "c8ddc5f710a447bca952415ca4588965": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c9efc9e90f6d4b61af8e801da1ea0bc9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dadc8b2f82c54c91b1fd947d5ecbd0b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "db9c9261072f4aa7a6afb78bb8998599": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c9efc9e90f6d4b61af8e801da1ea0bc9",
       "max": 6.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dc54e67bd389458d92a973ff1c997b15",
       "tabbable": null,
       "tooltip": null,
       "value": 6.0
      }
     },
     "dc54e67bd389458d92a973ff1c997b15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "de1a85453a0a44d482dc7b7a9528498f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b58150536faa4bd8a3a201d1b0ee0ded",
        "IPY_MODEL_9171422c038f47bb9057871e0ecc3244",
        "IPY_MODEL_bc97d6d6c34347a5860eeac3871d1b25"
       ],
       "layout": "IPY_MODEL_198ee2f7c10f4865bfc4e6056113966c",
       "tabbable": null,
       "tooltip": null
      }
     },
     "de3c2b29a7f44a279a95fb9af05f8f00": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df4256531f0343588d0925979a32397c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2cbebb37b6f0471c8df740c9f007d2c5",
       "placeholder": "​",
       "style": "IPY_MODEL_c8ddc5f710a447bca952415ca4588965",
       "tabbable": null,
       "tooltip": null,
       "value": "Processed prompts: 100%"
      }
     },
     "e75202ed49b34805af60c52f0ff7223d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f6b4705541854c519f2605dfc8ed6016": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fed7531f682e422eba6059ed9932027c",
       "placeholder": "​",
       "style": "IPY_MODEL_574b7dec8867414e90d841182beb9873",
       "tabbable": null,
       "tooltip": null,
       "value": "Adding requests: 100%"
      }
     },
     "f80e870585cf4c97a024a7a776ca3510": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fcb59066f482488eaddf821e89a345d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd0c1f4fcf8143c0b45a16fe2268ae4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_883670013dbb463d8cbc3e09c7c69509",
        "IPY_MODEL_1cada57040ca40a4a5e0d9ea59920fb0",
        "IPY_MODEL_8c130cf236eb499ca7011c83d1307053"
       ],
       "layout": "IPY_MODEL_6cb5bbd41b76444eb5a21398c8aa578a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "fed7531f682e422eba6059ed9932027c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
