{
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11.0"},
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {"sourceId": 118448, "sourceType": "competition"},
    {"sourceType": "datasetVersion", "datasetSlug": "qwen-72b-math-nf4"}
   ],
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import os, re, gc, time, math, random, warnings\nfrom typing import Optional, List, Tuple\nfrom collections import Counter\nwarnings.filterwarnings('ignore')\nprint(\"Cell 1 OK\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport torch\nnp.random.seed(42)\nrandom.seed(42)\ntorch.manual_seed(42)\nprint(f\"torch {torch.__version__} | CUDA {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"{torch.cuda.get_device_name(0)} | {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "try:\n    import bitsandbytes as bnb\n    BNB_OK = True\n    print(f\"bitsandbytes {bnb.__version__}\")\nexcept:\n    BNB_OK = False\n    print(\"bitsandbytes: NO\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "QWEN_PATH = \"/kaggle/input/qwen-72b-math-nf4\"\nTEST_PATH = \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\"\n\nMEM_CEIL = 70.0\nSAMPLES = 6\nMAX_TOK = 1024\nTEMPS = [0.3, 0.5, 0.6, 0.7, 0.8, 0.9]\n\ndef gpu_mem(): return torch.cuda.memory_allocated()/1024**3 if torch.cuda.is_available() else 0\ndef mem_ok(): return gpu_mem() < MEM_CEIL\ndef force_gc():\n    gc.collect()\n    if torch.cuda.is_available(): torch.cuda.empty_cache(); torch.cuda.synchronize()\n\nprint(f\"Qwen: {os.path.exists(QWEN_PATH)}\")\nprint(f\"Test: {os.path.exists(TEST_PATH)}\")\nif os.path.exists(QWEN_PATH):\n    files = os.listdir(QWEN_PATH)\n    sz = sum(os.path.getsize(os.path.join(QWEN_PATH,f)) for f in files if os.path.isfile(os.path.join(QWEN_PATH,f)))\n    print(f\"Qwen: {len(files)} files, {sz/1024**3:.1f}GB\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def try_symbolic(p: str) -> Optional[int]:\n    try:\n        t, o = p.lower(), p\n        m = re.search(r'(\\d+)\\^(\\d+)\\s*mod\\s*(\\d+)', o)\n        if m: return pow(int(m.group(1)), int(m.group(2)), int(m.group(3)))\n        m = re.search(r'(\\d+)\\s*[\\^\\*]{1,2}\\s*(\\d+).*?(?:mod|modulo|divided by)\\s*(\\d+)', t)\n        if m: return pow(int(m.group(1)), int(m.group(2)), int(m.group(3)))\n        m = re.search(r'(\\d+)\\s*(?:mod|modulo)\\s*(\\d+)', t)\n        if m: return int(m.group(1)) % int(m.group(2))\n        m = re.search(r'remainder.*?(\\d+).*?divided.*?(\\d+)', t)\n        if m: return int(m.group(1)) % int(m.group(2))\n        m = re.search(r'(\\d+)\\s*[Ã—\\*]\\s*(\\d+)', o)\n        if m:\n            r = int(m.group(1)) * int(m.group(2))\n            if 0 <= r < 100000: return r\n        m = re.search(r'(\\d+)\\s*times\\s*(\\d+)', t)\n        if m:\n            r = int(m.group(1)) * int(m.group(2))\n            if 0 <= r < 100000: return r\n        m = re.search(r'sum.*?divisors.*?(\\d+)', t)\n        if m:\n            n = int(m.group(1))\n            if n < 10000: return sum(i for i in range(1,n+1) if n%i==0)\n        m = re.search(r'(?:gcd|greatest common divisor).*?(\\d+).*?(\\d+)', t)\n        if m: return math.gcd(int(m.group(1)), int(m.group(2)))\n    except: pass\n    return None\n\nassert try_symbolic(\"2^10 mod 7\") == 2\nprint(\"symbolic OK\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def clean(t):\n    if not t: return \"\"\n    t = re.sub(r'Study\\.com.*', '', t, flags=re.DOTALL|re.I)\n    t = re.sub(r'Become a.*?member.*', '', t, flags=re.I)\n    return t.strip()\n\ndef extract(t) -> Optional[int]:\n    t = clean(t)\n    if not t: return None\n    for p in [r'\\\\boxed\\{(\\d+)\\}', r'boxed\\{(\\d+)\\}', r'\\*\\*(\\d+)\\*\\*\\s*$', r'[Aa]nswer[^0-9]*(\\d+)', r'=\\s*(\\d+)\\s*$']:\n        m = re.findall(p, t, re.M|re.I)\n        if m:\n            try:\n                v = int(m[-1])\n                if 0 <= v < 100000: return v\n            except: pass\n    for s in reversed(re.findall(r'\\b(\\d+)\\b', t[-300:])):\n        try:\n            v = int(s)\n            if 1 < v < 100000: return v\n        except: pass\n    return None\n\nprint(\"extract OK\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def cic(samples: List[int]) -> Tuple[int, float]:\n    if not samples: return 0, 0.0\n    if len(samples) == 1: return samples[0], 0.5\n    c = Counter(samples)\n    top = c.most_common()\n    best, cnt = top[0]\n    conf = min(0.95, cnt / len(samples))\n    if len(top) > 1 and top[0][1] == top[1][1]:\n        best = int(np.median([x for x,_ in top[:2]]))\n    return max(0, min(99999, int(best))), conf\n\nprint(\"cic OK\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class QwenModel:\n    def __init__(self):\n        self.model = None\n        self.tok = None\n        self.loaded = False\n        \n    def load(self):\n        if self.loaded: return True\n        if not os.path.exists(QWEN_PATH):\n            print(\"Qwen: NOT FOUND\")\n            return False\n        try:\n            from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n            print(\"Loading Qwen...\")\n            \n            # Try direct first\n            try:\n                self.model = AutoModelForCausalLM.from_pretrained(\n                    QWEN_PATH, device_map=\"auto\", trust_remote_code=True, low_cpu_mem_usage=True\n                )\n                print(f\"  Direct load: {gpu_mem():.1f}GB\")\n            except Exception as e:\n                print(f\"  Direct failed: {e}\")\n                if BNB_OK:\n                    print(\"  Trying explicit NF4...\")\n                    cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n                                            bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True)\n                    self.model = AutoModelForCausalLM.from_pretrained(\n                        QWEN_PATH, quantization_config=cfg, device_map=\"auto\", trust_remote_code=True, low_cpu_mem_usage=True\n                    )\n                    print(f\"  NF4 load: {gpu_mem():.1f}GB\")\n                else:\n                    print(\"  Trying FP16...\")\n                    self.model = AutoModelForCausalLM.from_pretrained(\n                        QWEN_PATH, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True, low_cpu_mem_usage=True\n                    )\n                    print(f\"  FP16 load: {gpu_mem():.1f}GB\")\n            \n            self.tok = AutoTokenizer.from_pretrained(QWEN_PATH, trust_remote_code=True)\n            if self.tok.pad_token is None: self.tok.pad_token = self.tok.eos_token\n            self.loaded = True\n            print(f\"Qwen ready: {gpu_mem():.1f}GB\")\n            return True\n        except Exception as e:\n            print(f\"Qwen FAILED: {e}\")\n            import traceback; traceback.print_exc()\n            return False\n    \n    def gen(self, prompt: str, temp: float) -> str:\n        if not self.loaded: return \"\"\n        try:\n            inp = self.tok(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.model.device)\n            with torch.no_grad():\n                out = self.model.generate(**inp, max_new_tokens=MAX_TOK, temperature=temp, do_sample=True, top_p=0.9, pad_token_id=self.tok.pad_token_id)\n            return self.tok.decode(out[0][inp[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n        except Exception as e:\n            print(f\"  gen err: {e}\")\n            return \"\"\n\nprint(\"QwenModel OK\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "PROMPT = \"\"\"Solve step by step. Be precise.\n\nPROBLEM_HERE\n\nFinal answer as \\\\boxed{{INTEGER}} (0-99999).\"\"\"\n\nclass Engine:\n    def __init__(self):\n        self.qwen = QwenModel()\n        self.times = []\n        \n    def solve(self, problem: str, pid: str) -> int:\n        t0 = time.time()\n        print(f\"\\n[{pid}] {problem[:60]}...\")\n        \n        # Symbolic\n        sym = try_symbolic(problem)\n        if sym is not None:\n            print(f\"  SYM: {sym}\")\n            self.times.append(time.time()-t0)\n            return sym\n        \n        # LLM\n        if not self.qwen.load():\n            print(\"  MODEL FAILED - returning 0\")\n            return 0\n        \n        prompt = PROMPT.replace(\"PROBLEM_HERE\", problem)\n        answers = []\n        for temp in TEMPS[:SAMPLES]:\n            resp = self.qwen.gen(prompt, temp)\n            ans = extract(resp)\n            if ans is not None:\n                answers.append(ans)\n                print(f\"    T{temp}: {ans}\")\n            else:\n                print(f\"    T{temp}: -\")\n        \n        answer, conf = cic(answers)\n        elapsed = time.time() - t0\n        self.times.append(elapsed)\n        print(f\"  >>> {answer} | conf={conf:.2f} | {elapsed:.1f}s\")\n        return answer\n\nprint(\"Engine OK\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def run_comp():\n    print(\"=\"*50)\n    print(\"AIMO3 V12.9 QWEN-ONLY\")\n    print(\"=\"*50)\n    eng = Engine()\n    def predict(pid, prob):\n        try: return eng.solve(prob, pid)\n        except Exception as e:\n            print(f\"ERR: {e}\")\n            return 0\n    from kaggle_evaluation import aimo_2_inference_server as srv\n    server = srv.AIMO2InferenceServer(predict)\n    if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"): server.serve()\n    else: server.run_local_gateway((TEST_PATH,))\n\ndef run_local():\n    print(\"=\"*50)\n    print(\"AIMO3 V12.9 LOCAL\")\n    print(\"=\"*50)\n    eng = Engine()\n    tests = [\n        (\"t1\", \"2^10 mod 7\", 2),\n        (\"t2\", \"17 * 23\", 391),\n        (\"t3\", \"sum of divisors of 28\", 56),\n        (\"t4\", \"If f(x)=x^2-3x+5, what is f(4)?\", 9),\n    ]\n    ok = 0\n    for pid, p, exp in tests:\n        r = eng.solve(p, pid)\n        if r == exp: ok += 1\n        print(f\"  {pid}: {r} {'OK' if r==exp else f'WRONG({exp})'}\")\n    print(f\"\\n{ok}/{len(tests)}\")\n    if eng.times: print(f\"Avg: {np.mean(eng.times):.1f}s\")\n\nprint(\"runners OK\")",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(f\"GPU: {torch.cuda.get_device_properties(0).total_memory/1024**3:.1f}GB\" if torch.cuda.is_available() else \"No GPU\")\ntry:\n    from kaggle_evaluation import aimo_2_inference_server\n    print(\"kaggle_eval: YES\")\n    HAS = True\nexcept:\n    print(\"kaggle_eval: NO\")\n    HAS = False\n\nif os.path.exists(TEST_PATH) and HAS: run_comp()\nelse: run_local()",
   "metadata": {"trusted": true},
   "outputs": [],
   "execution_count": null
  }
 ]
}
