{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.0"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":14113316,"sourceType":"datasetVersion","datasetId":8990006},{"sourceId":166368,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":141565,"modelId":164048}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# AIMO3 Submission - Qwen2.5-72B-Instruct-AWQ\n# Quantized model that fits on H100 80GB\n# Uses vLLM with AWQ quantization\n\nimport subprocess\nimport sys\nimport os\nimport time\nimport gc\nfrom pathlib import Path\n\n# Free up memory\nfor pkg in ['tensorflow', 'matplotlib', 'keras', 'sklearn', 'scikit-learn']:\n    try:\n        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', pkg], \n                      capture_output=True, timeout=60)\n    except: pass\ngc.collect()\n\n# GPU info\ngpu_info = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], \n                          capture_output=True, text=True)\nprint(f\"GPU: {gpu_info.stdout.strip()}\")\nprint(f\"Budget: 280min\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install vLLM wheels - skip packages that conflict with Kaggle\nWHEEL_DIR = Path('/kaggle/input/aimo3-vllm-wheels')\n\n# Packages to skip - Kaggle provides these and mixing versions breaks things\nSKIP_PACKAGES = {\n    # CRITICAL - PyTorch ecosystem must stay intact\n    'torch', 'xformers', 'triton',  # Cannot mix PyTorch versions\n    'transformers', 'tokenizers', 'numpy',\n    # RISKY - may cause import errors  \n    'nvidia', 'accelerate', 'huggingface', 'safetensors', 'pillow', 'protobuf',\n    'pyyaml', 'regex', 'packaging', 'tqdm', 'fsspec', 'certifi', 'charset',\n    'idna', 'urllib3', 'jinja2', 'markupsafe', 'sympy', 'mpmath', 'networkx',\n    'six', 'python-dateutil', 'attrs', 'referencing', 'jsonschema', 'rpds',\n    # FLASHINFER - not in our wheels anyway\n    'flashinfer',\n}\n\ndef should_skip(wheel_name):\n    name_lower = wheel_name.lower()\n    for skip in SKIP_PACKAGES:\n        if skip.lower().replace('-', '_') in name_lower.replace('-', '_'):\n            return True\n    return False\n\nif WHEEL_DIR.exists():\n    wheels = list(WHEEL_DIR.glob('*.whl'))\n    print(f\"\\n{WHEEL_DIR}:\")\n    print(f\"  Found {len(wheels)} wheels\")\n    for w in wheels[:10]:\n        print(f\"  {w.name}\")\n    if len(wheels) > 10:\n        print(f\"  ... and {len(wheels)-10} more\")\n    \n    print(f\"\\n--- Installing wheels (skipping Kaggle-provided packages) ---\")\n    installed, skipped = 0, 0\n    for wheel in sorted(wheels):\n        if should_skip(wheel.name):\n            skipped += 1\n            continue\n        result = subprocess.run(\n            [sys.executable, '-m', 'pip', 'install', '--no-deps', '--quiet', str(wheel)],\n            capture_output=True, text=True\n        )\n        if result.returncode == 0:\n            installed += 1\n    print(f\"Installed {installed} wheels (skipped {skipped} conflicting)\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import vLLM and verify\nfrom vllm import LLM, SamplingParams\nimport torch\nprint(f\"\\nAll imports ready\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model path - Qwen2.5-72B-Instruct-AWQ\nMODEL_PATH = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n\n# Verify model exists\nmodel_path = Path(MODEL_PATH)\nif model_path.exists():\n    files = list(model_path.glob('*'))\n    print(f\"Found Qwen2.5-72B-AWQ: {MODEL_PATH}\")\n    safetensors = [f for f in files if f.suffix == '.safetensors']\n    print(f\"  Safetensor shards: {len(safetensors)}\")\n    total_size = sum(f.stat().st_size for f in safetensors) / 1e9\n    print(f\"  Total size: {total_size:.1f} GB\")\nelse:\n    raise FileNotFoundError(f\"Model not found at {MODEL_PATH}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model with AWQ quantization\nprint(\"Loading Qwen2.5-32B-Instruct-AWQ...\")\nprint(\"  Using AWQ INT4 quantization (~36GB VRAM)\")\n\nllm = LLM(\n    MODEL_PATH,\n    quantization=\"awq_marlin\",\n    dtype=\"float16\",\n    trust_remote_code=True,\n    gpu_memory_utilization=0.92,\n    max_model_len=4096,\n    max_num_seqs=16,\n    enforce_eager=True,  # More stable\n    enable_prefix_caching=True,\n    seed=42,\n)\n\ntokenizer = llm.get_tokenizer()\nprint(f\"Model loaded successfully!\")\nprint(f\"Max context: 4096 tokens\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test problems\nimport kaggle_evaluation.aimo_2_inference_server\n\n# Get sample problems for local testing\ntry:\n    import pandas as pd\n    sample_df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/sample_submission.csv')\n    test_df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv')\n    print(f\"Test set: {len(test_df)} problems\")\nexcept:\n    print(\"Running in submission mode - problems provided by server\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Math problem solving configuration\nSYSTEM_PROMPT = \"\"\"You are an expert mathematical problem solver specializing in competition mathematics.\n\nWhen solving problems:\n1. Read the problem carefully and identify what is being asked\n2. Break down the problem into manageable steps\n3. Show your work clearly with mathematical reasoning\n4. Double-check your calculations\n5. State your final answer clearly\n\nFor numerical answers, provide ONLY the integer value as your final answer.\nIf the answer should be a remainder or modular result, compute it explicitly.\"\"\"\n\ndef format_prompt(problem: str) -> str:\n    \"\"\"Format problem for Qwen chat template.\"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": f\"Solve this problem step by step:\\n\\n{problem}\\n\\nShow your complete solution, then state your final numerical answer.\"}\n    ]\n    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# Sampling parameters for math\nsampling_params = SamplingParams(\n    temperature=0.6,\n    top_p=0.9,\n    max_tokens=2048,\n    stop=[\"<|endoftext|>\", \"<|im_end|>\"],\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef extract_answer(response: str) -> int:\n    \"\"\"Extract numerical answer from model response.\"\"\"\n    # Look for common answer patterns\n    patterns = [\n        r'(?:final answer|answer is|therefore|thus|hence)[:\\s]*(?:\\$)?\\s*(\\d+)',\n        r'(?:=|equals?)\\s*(\\d+)\\s*$',\n        r'\\\\boxed\\{(\\d+)\\}',\n        r'\\*\\*(\\d+)\\*\\*\\s*$',\n    ]\n    \n    response_lower = response.lower()\n    \n    for pattern in patterns:\n        matches = re.findall(pattern, response_lower, re.IGNORECASE | re.MULTILINE)\n        if matches:\n            try:\n                return int(matches[-1])\n            except ValueError:\n                continue\n    \n    # Fallback: find last number in response\n    numbers = re.findall(r'\\b(\\d+)\\b', response)\n    if numbers:\n        return int(numbers[-1])\n    \n    return 0  # Default if no number found\n\ndef solve_problem(problem: str, num_samples: int = 3) -> int:\n    \"\"\"Solve a math problem using majority voting.\"\"\"\n    prompt = format_prompt(problem)\n    \n    # Generate multiple solutions\n    outputs = llm.generate([prompt] * num_samples, sampling_params)\n    \n    answers = []\n    for output in outputs:\n        response = output.outputs[0].text\n        answer = extract_answer(response)\n        answers.append(answer)\n    \n    # Majority vote\n    from collections import Counter\n    if answers:\n        most_common = Counter(answers).most_common(1)[0][0]\n        return most_common\n    return 0","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test with a sample problem\ntest_problem = \"\"\"Find the sum of all positive integers n such that n^2 + n + 1 divides n^4 + n^2 + 1.\"\"\"\n\nprint(\"Testing with sample problem...\")\nprint(f\"Problem: {test_problem[:100]}...\")\n\nprompt = format_prompt(test_problem)\noutput = llm.generate([prompt], sampling_params)[0]\nresponse = output.outputs[0].text\n\nprint(f\"\\nResponse preview: {response[:500]}...\")\nprint(f\"\\nExtracted answer: {extract_answer(response)}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main prediction function for Kaggle submission\ndef predict(id_: str, question: str) -> int:\n    \"\"\"Main prediction function called by Kaggle evaluation server.\"\"\"\n    try:\n        answer = solve_problem(question, num_samples=3)\n        print(f\"Problem {id_}: answer = {answer}\")\n        return answer\n    except Exception as e:\n        print(f\"Error on problem {id_}: {e}\")\n        return 0\n\nprint(\"Prediction function ready!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Start inference server for Kaggle submission\nprint(\"Starting Kaggle inference server...\")\nkaggle_evaluation.aimo_2_inference_server.serve(predict)","metadata":{},"outputs":[],"execution_count":null}]}